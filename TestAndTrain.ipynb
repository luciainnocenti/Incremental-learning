{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "082109dc5d3841c2b0c3bfa9eea97b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d690c6f4a19f4a899d29bdaf9bd8383b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7eb5727919864978847c6237aceceb88",
              "IPY_MODEL_ec592b7e51804f81926f115a7d4d13e6"
            ]
          }
        },
        "d690c6f4a19f4a899d29bdaf9bd8383b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7eb5727919864978847c6237aceceb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_20cc8379918d46aaa64cfbe3f0436efc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2803f04bd156452d83ee48e205603170"
          }
        },
        "ec592b7e51804f81926f115a7d4d13e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_65c52f26c2fb4ce18047ec409e3055dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:06&lt;00:00, 26955017.09it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ba3c72099114de8a8b69a23155b636c"
          }
        },
        "20cc8379918d46aaa64cfbe3f0436efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2803f04bd156452d83ee48e205603170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65c52f26c2fb4ce18047ec409e3055dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ba3c72099114de8a8b69a23155b636c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/Lucia/TestAndTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbtGDBU3QJaq",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf0TmOM3NdFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I0pKIVIM2KC",
        "colab_type": "code",
        "outputId": "887856ee-4919-467c-d9ac-e7fdbf754cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone -b Lucia https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 181, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/181)\u001b[K\rremote: Counting objects:   1% (2/181)\u001b[K\rremote: Counting objects:   2% (4/181)\u001b[K\rremote: Counting objects:   3% (6/181)\u001b[K\rremote: Counting objects:   4% (8/181)\u001b[K\rremote: Counting objects:   5% (10/181)\u001b[K\rremote: Counting objects:   6% (11/181)\u001b[K\rremote: Counting objects:   7% (13/181)\u001b[K\rremote: Counting objects:   8% (15/181)\u001b[K\rremote: Counting objects:   9% (17/181)\u001b[K\rremote: Counting objects:  10% (19/181)\u001b[K\rremote: Counting objects:  11% (20/181)\u001b[K\rremote: Counting objects:  12% (22/181)\u001b[K\rremote: Counting objects:  13% (24/181)\u001b[K\rremote: Counting objects:  14% (26/181)\u001b[K\rremote: Counting objects:  15% (28/181)\u001b[K\rremote: Counting objects:  16% (29/181)\u001b[K\rremote: Counting objects:  17% (31/181)\u001b[K\rremote: Counting objects:  18% (33/181)\u001b[K\rremote: Counting objects:  19% (35/181)\u001b[K\rremote: Counting objects:  20% (37/181)\u001b[K\rremote: Counting objects:  21% (39/181)\u001b[K\rremote: Counting objects:  22% (40/181)\u001b[K\rremote: Counting objects:  23% (42/181)\u001b[K\rremote: Counting objects:  24% (44/181)\u001b[K\rremote: Counting objects:  25% (46/181)\u001b[K\rremote: Counting objects:  26% (48/181)\u001b[K\rremote: Counting objects:  27% (49/181)\u001b[K\rremote: Counting objects:  28% (51/181)\u001b[K\rremote: Counting objects:  29% (53/181)\u001b[K\rremote: Counting objects:  30% (55/181)\u001b[K\rremote: Counting objects:  31% (57/181)\u001b[K\rremote: Counting objects:  32% (58/181)\u001b[K\rremote: Counting objects:  33% (60/181)\u001b[K\rremote: Counting objects:  34% (62/181)\u001b[K\rremote: Counting objects:  35% (64/181)\u001b[K\rremote: Counting objects:  36% (66/181)\u001b[K\rremote: Counting objects:  37% (67/181)\u001b[K\rremote: Counting objects:  38% (69/181)\u001b[K\rremote: Counting objects:  39% (71/181)\u001b[K\rremote: Counting objects:  40% (73/181)\u001b[K\rremote: Counting objects:  41% (75/181)\u001b[K\rremote: Counting objects:  42% (77/181)\u001b[K\rremote: Counting objects:  43% (78/181)\u001b[K\rremote: Counting objects:  44% (80/181)\u001b[K\rremote: Counting objects:  45% (82/181)\u001b[K\rremote: Counting objects:  46% (84/181)\u001b[K\rremote: Counting objects:  47% (86/181)\u001b[K\rremote: Counting objects:  48% (87/181)\u001b[K\rremote: Counting objects:  49% (89/181)\u001b[K\rremote: Counting objects:  50% (91/181)\u001b[K\rremote: Counting objects:  51% (93/181)\u001b[K\rremote: Counting objects:  52% (95/181)\u001b[K\rremote: Counting objects:  53% (96/181)\u001b[K\rremote: Counting objects:  54% (98/181)\u001b[K\rremote: Counting objects:  55% (100/181)\u001b[K\rremote: Counting objects:  56% (102/181)\u001b[K\rremote: Counting objects:  57% (104/181)\u001b[K\rremote: Counting objects:  58% (105/181)\u001b[K\rremote: Counting objects:  59% (107/181)\u001b[K\rremote: Counting objects:  60% (109/181)\u001b[K\rremote: Counting objects:  61% (111/181)\u001b[K\rremote: Counting objects:  62% (113/181)\u001b[K\rremote: Counting objects:  63% (115/181)\u001b[K\rremote: Counting objects:  64% (116/181)\u001b[K\rremote: Counting objects:  65% (118/181)\u001b[K\rremote: Counting objects:  66% (120/181)\u001b[K\rremote: Counting objects:  67% (122/181)\u001b[K\rremote: Counting objects:  68% (124/181)\u001b[K\rremote: Counting objects:  69% (125/181)\u001b[K\rremote: Counting objects:  70% (127/181)\u001b[K\rremote: Counting objects:  71% (129/181)\u001b[K\rremote: Counting objects:  72% (131/181)\u001b[K\rremote: Counting objects:  73% (133/181)\u001b[K\rremote: Counting objects:  74% (134/181)\u001b[K\rremote: Counting objects:  75% (136/181)\u001b[K\rremote: Counting objects:  76% (138/181)\u001b[K\rremote: Counting objects:  77% (140/181)\u001b[K\rremote: Counting objects:  78% (142/181)\u001b[K\rremote: Counting objects:  79% (143/181)\u001b[K\rremote: Counting objects:  80% (145/181)\u001b[K\rremote: Counting objects:  81% (147/181)\u001b[K\rremote: Counting objects:  82% (149/181)\u001b[K\rremote: Counting objects:  83% (151/181)\u001b[K\rremote: Counting objects:  84% (153/181)\u001b[K\rremote: Counting objects:  85% (154/181)\u001b[K\rremote: Counting objects:  86% (156/181)\u001b[K\rremote: Counting objects:  87% (158/181)\u001b[K\rremote: Counting objects:  88% (160/181)\u001b[K\rremote: Counting objects:  89% (162/181)\u001b[K\rremote: Counting objects:  90% (163/181)\u001b[K\rremote: Counting objects:  91% (165/181)\u001b[K\rremote: Counting objects:  92% (167/181)\u001b[K\rremote: Counting objects:  93% (169/181)\u001b[K\rremote: Counting objects:  94% (171/181)\u001b[K\rremote: Counting objects:  95% (172/181)\u001b[K\rremote: Counting objects:  96% (174/181)\u001b[K\rremote: Counting objects:  97% (176/181)\u001b[K\rremote: Counting objects:  98% (178/181)\u001b[K\rremote: Counting objects:  99% (180/181)\u001b[K\rremote: Counting objects: 100% (181/181)\u001b[K\rremote: Counting objects: 100% (181/181), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/173)\u001b[K\rremote: Compressing objects:   1% (2/173)\u001b[K\rremote: Compressing objects:   2% (4/173)\u001b[K\rremote: Compressing objects:   3% (6/173)\u001b[K\rremote: Compressing objects:   4% (7/173)\u001b[K\rremote: Compressing objects:   5% (9/173)\u001b[K\rremote: Compressing objects:   6% (11/173)\u001b[K\rremote: Compressing objects:   7% (13/173)\u001b[K\rremote: Compressing objects:   8% (14/173)\u001b[K\rremote: Compressing objects:   9% (16/173)\u001b[K\rremote: Compressing objects:  10% (18/173)\u001b[K\rremote: Compressing objects:  11% (20/173)\u001b[K\rremote: Compressing objects:  12% (21/173)\u001b[K\rremote: Compressing objects:  13% (23/173)\u001b[K\rremote: Compressing objects:  14% (25/173)\u001b[K\rremote: Compressing objects:  15% (26/173)\u001b[K\rremote: Compressing objects:  16% (28/173)\u001b[K\rremote: Compressing objects:  17% (30/173)\u001b[K\rremote: Compressing objects:  18% (32/173)\u001b[K\rremote: Compressing objects:  19% (33/173)\u001b[K\rremote: Compressing objects:  20% (35/173)\u001b[K\rremote: Compressing objects:  21% (37/173)\u001b[K\rremote: Compressing objects:  22% (39/173)\u001b[K\rremote: Compressing objects:  23% (40/173)\u001b[K\rremote: Compressing objects:  24% (42/173)\u001b[K\rremote: Compressing objects:  25% (44/173)\u001b[K\rremote: Compressing objects:  26% (45/173)\u001b[K\rremote: Compressing objects:  27% (47/173)\u001b[K\rremote: Compressing objects:  28% (49/173)\u001b[K\rremote: Compressing objects:  29% (51/173)\u001b[K\rremote: Compressing objects:  30% (52/173)\u001b[K\rremote: Compressing objects:  31% (54/173)\u001b[K\rremote: Compressing objects:  32% (56/173)\u001b[K\rremote: Compressing objects:  33% (58/173)\u001b[K\rremote: Compressing objects:  34% (59/173)\u001b[K\rremote: Compressing objects:  35% (61/173)\u001b[K\rremote: Compressing objects:  36% (63/173)\u001b[K\rremote: Compressing objects:  37% (65/173)\u001b[K\rremote: Compressing objects:  38% (66/173)\u001b[K\rremote: Compressing objects:  39% (68/173)\u001b[K\rremote: Compressing objects:  40% (70/173)\u001b[K\rremote: Compressing objects:  41% (71/173)\u001b[K\rremote: Compressing objects:  42% (73/173)\u001b[K\rremote: Compressing objects:  43% (75/173)\u001b[K\rremote: Compressing objects:  44% (77/173)\u001b[K\rremote: Compressing objects:  45% (78/173)\u001b[K\rremote: Compressing objects:  46% (80/173)\u001b[K\rremote: Compressing objects:  47% (82/173)\u001b[K\rremote: Compressing objects:  48% (84/173)\u001b[K\rremote: Compressing objects:  49% (85/173)\u001b[K\rremote: Compressing objects:  50% (87/173)\u001b[K\rremote: Compressing objects:  51% (89/173)\u001b[K\rremote: Compressing objects:  52% (90/173)\u001b[K\rremote: Compressing objects:  53% (92/173)\u001b[K\rremote: Compressing objects:  54% (94/173)\u001b[K\rremote: Compressing objects:  55% (96/173)\u001b[K\rremote: Compressing objects:  56% (97/173)\u001b[K\rremote: Compressing objects:  57% (99/173)\u001b[K\rremote: Compressing objects:  58% (101/173)\u001b[K\rremote: Compressing objects:  59% (103/173)\u001b[K\rremote: Compressing objects:  60% (104/173)\u001b[K\rremote: Compressing objects:  61% (106/173)\u001b[K\rremote: Compressing objects:  62% (108/173)\u001b[K\rremote: Compressing objects:  63% (109/173)\u001b[K\rremote: Compressing objects:  64% (111/173)\u001b[K\rremote: Compressing objects:  65% (113/173)\u001b[K\rremote: Compressing objects:  66% (115/173)\u001b[K\rremote: Compressing objects:  67% (116/173)\u001b[K\rremote: Compressing objects:  68% (118/173)\u001b[K\rremote: Compressing objects:  69% (120/173)\u001b[K\rremote: Compressing objects:  70% (122/173)\u001b[K\rremote: Compressing objects:  71% (123/173)\u001b[K\rremote: Compressing objects:  72% (125/173)\u001b[K\rremote: Compressing objects:  73% (127/173)\u001b[K\rremote: Compressing objects:  74% (129/173)\u001b[K\rremote: Compressing objects:  75% (130/173)\u001b[K\rremote: Compressing objects:  76% (132/173)\u001b[K\rremote: Compressing objects:  77% (134/173)\u001b[K\rremote: Compressing objects:  78% (135/173)\u001b[K\rremote: Compressing objects:  79% (137/173)\u001b[K\rremote: Compressing objects:  80% (139/173)\u001b[K\rremote: Compressing objects:  81% (141/173)\u001b[K\rremote: Compressing objects:  82% (142/173)\u001b[K\rremote: Compressing objects:  83% (144/173)\u001b[K\rremote: Compressing objects:  84% (146/173)\u001b[K\rremote: Compressing objects:  85% (148/173)\u001b[K\rremote: Compressing objects:  86% (149/173)\u001b[K\rremote: Compressing objects:  87% (151/173)\u001b[K\rremote: Compressing objects:  88% (153/173)\u001b[K\rremote: Compressing objects:  89% (154/173)\u001b[K\rremote: Compressing objects:  90% (156/173)\u001b[K\rremote: Compressing objects:  91% (158/173)\u001b[K\rremote: Compressing objects:  92% (160/173)\u001b[K\rremote: Compressing objects:  93% (161/173)\u001b[K\rremote: Compressing objects:  94% (163/173)\u001b[K\rremote: Compressing objects:  95% (165/173)\u001b[K\rremote: Compressing objects:  96% (167/173)\u001b[K\rremote: Compressing objects:  97% (168/173)\u001b[K\rremote: Compressing objects:  98% (170/173)\u001b[K\rremote: Compressing objects:  99% (172/173)\u001b[K\rremote: Compressing objects: 100% (173/173)\u001b[K\rremote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "Receiving objects:   0% (1/318)   \rReceiving objects:   1% (4/318)   \rReceiving objects:   2% (7/318)   \rReceiving objects:   3% (10/318)   \rReceiving objects:   4% (13/318)   \rReceiving objects:   5% (16/318)   \rReceiving objects:   6% (20/318)   \rReceiving objects:   7% (23/318)   \rReceiving objects:   8% (26/318)   \rReceiving objects:   9% (29/318)   \rReceiving objects:  10% (32/318)   \rReceiving objects:  11% (35/318)   \rReceiving objects:  12% (39/318)   \rReceiving objects:  13% (42/318)   \rReceiving objects:  14% (45/318)   \rReceiving objects:  15% (48/318)   \rReceiving objects:  16% (51/318)   \rReceiving objects:  17% (55/318)   \rReceiving objects:  18% (58/318)   \rReceiving objects:  19% (61/318)   \rReceiving objects:  20% (64/318)   \rReceiving objects:  21% (67/318)   \rReceiving objects:  22% (70/318)   \rReceiving objects:  23% (74/318)   \rReceiving objects:  24% (77/318)   \rReceiving objects:  25% (80/318)   \rReceiving objects:  26% (83/318)   \rReceiving objects:  27% (86/318)   \rReceiving objects:  28% (90/318)   \rReceiving objects:  29% (93/318)   \rReceiving objects:  30% (96/318)   \rReceiving objects:  31% (99/318)   \rReceiving objects:  32% (102/318)   \rReceiving objects:  33% (105/318)   \rReceiving objects:  34% (109/318)   \rReceiving objects:  35% (112/318)   \rReceiving objects:  36% (115/318)   \rReceiving objects:  37% (118/318)   \rReceiving objects:  38% (121/318)   \rReceiving objects:  39% (125/318)   \rReceiving objects:  40% (128/318)   \rReceiving objects:  41% (131/318)   \rReceiving objects:  42% (134/318)   \rReceiving objects:  43% (137/318)   \rReceiving objects:  44% (140/318)   \rReceiving objects:  45% (144/318)   \rReceiving objects:  46% (147/318)   \rReceiving objects:  47% (150/318)   \rReceiving objects:  48% (153/318)   \rReceiving objects:  49% (156/318)   \rReceiving objects:  50% (159/318)   \rReceiving objects:  51% (163/318)   \rReceiving objects:  52% (166/318)   \rReceiving objects:  53% (169/318)   \rReceiving objects:  54% (172/318)   \rReceiving objects:  55% (175/318)   \rReceiving objects:  56% (179/318)   \rReceiving objects:  57% (182/318)   \rReceiving objects:  58% (185/318)   \rReceiving objects:  59% (188/318)   \rReceiving objects:  60% (191/318)   \rReceiving objects:  61% (194/318)   \rReceiving objects:  62% (198/318)   \rReceiving objects:  63% (201/318)   \rReceiving objects:  64% (204/318)   \rReceiving objects:  65% (207/318)   \rReceiving objects:  66% (210/318)   \rReceiving objects:  67% (214/318)   \rReceiving objects:  68% (217/318)   \rReceiving objects:  69% (220/318)   \rReceiving objects:  70% (223/318)   \rReceiving objects:  71% (226/318)   \rReceiving objects:  72% (229/318)   \rReceiving objects:  73% (233/318)   \rReceiving objects:  74% (236/318)   \rReceiving objects:  75% (239/318)   \rReceiving objects:  76% (242/318)   \rReceiving objects:  77% (245/318)   \rReceiving objects:  78% (249/318)   \rReceiving objects:  79% (252/318)   \rReceiving objects:  80% (255/318)   \rReceiving objects:  81% (258/318)   \rReceiving objects:  82% (261/318)   \rReceiving objects:  83% (264/318)   \rReceiving objects:  84% (268/318)   \rReceiving objects:  85% (271/318)   \rremote: Total 318 (delta 113), reused 18 (delta 8), pack-reused 137\u001b[K\n",
            "Receiving objects:  86% (274/318)   \rReceiving objects:  87% (277/318)   \rReceiving objects:  88% (280/318)   \rReceiving objects:  89% (284/318)   \rReceiving objects:  90% (287/318)   \rReceiving objects:  91% (290/318)   \rReceiving objects:  92% (293/318)   \rReceiving objects:  93% (296/318)   \rReceiving objects:  94% (299/318)   \rReceiving objects:  95% (303/318)   \rReceiving objects:  96% (306/318)   \rReceiving objects:  97% (309/318)   \rReceiving objects:  98% (312/318)   \rReceiving objects:  99% (315/318)   \rReceiving objects: 100% (318/318)   \rReceiving objects: 100% (318/318), 229.22 KiB | 3.70 MiB/s, done.\n",
            "Resolving deltas:   0% (0/189)   \rResolving deltas:   1% (2/189)   \rResolving deltas:   2% (4/189)   \rResolving deltas:  19% (36/189)   \rResolving deltas:  20% (38/189)   \rResolving deltas:  21% (40/189)   \rResolving deltas:  27% (52/189)   \rResolving deltas:  28% (53/189)   \rResolving deltas:  30% (58/189)   \rResolving deltas:  33% (64/189)   \rResolving deltas:  34% (65/189)   \rResolving deltas:  36% (69/189)   \rResolving deltas:  37% (70/189)   \rResolving deltas:  62% (119/189)   \rResolving deltas:  68% (130/189)   \rResolving deltas:  74% (141/189)   \rResolving deltas:  82% (156/189)   \rResolving deltas:  87% (166/189)   \rResolving deltas: 100% (189/189)   \rResolving deltas: 100% (189/189), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLaS2laafBaG",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liUP5Kc1DMbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from DatasetCIFAR.data_set import Dataset \n",
        "from DatasetCIFAR import ResNet\n",
        "from DatasetCIFAR import utils\n",
        "from DatasetCIFAR import params\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_vlqOL7ehLC",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWttFW3ljoMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resNet = ResNet.resnet32(num_classes=100)\n",
        "resNet = resNet.to(params.DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmohsyVWFpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet_transformer = transforms.Compose([transforms.Resize(32), \n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_cyhIzFej5-",
        "colab_type": "text"
      },
      "source": [
        "# Define DataSets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcBNohmiYBtP",
        "colab_type": "code",
        "outputId": "089890b4-49c5-44db-e36c-1ce82823b30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "082109dc5d3841c2b0c3bfa9eea97b5e",
            "d690c6f4a19f4a899d29bdaf9bd8383b",
            "7eb5727919864978847c6237aceceb88",
            "ec592b7e51804f81926f115a7d4d13e6",
            "20cc8379918d46aaa64cfbe3f0436efc",
            "2803f04bd156452d83ee48e205603170",
            "65c52f26c2fb4ce18047ec409e3055dc",
            "7ba3c72099114de8a8b69a23155b636c"
          ]
        }
      },
      "source": [
        "trainDS = Dataset(train=True, transform = resnet_transformer)\n",
        "testDS = Dataset(train=False, transform = resnet_transformer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "082109dc5d3841c2b0c3bfa9eea97b5e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAT2KQEersx",
        "colab_type": "text"
      },
      "source": [
        "# Useful plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1l7flYj4NJh",
        "colab_type": "text"
      },
      "source": [
        "The function plotEpoch plots, at the end of each task, how accuracy and loss change during the training phase. It show\n",
        "\n",
        "*   Validation and Training Accuracy\n",
        "*   Validation and Training Loss\n",
        "\n",
        "The function plotTask, for each task, how the accuracy on the validation set change when adding new tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr58kkHiIzZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTask(pars_tasks):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  x_tasks =  np.linspace(10, 100, 10)\n",
        "\n",
        "  plt.plot(x_tasks, pars_tasks, label=['Accuracy', 'Loss'])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.title('Accuracy over tasks')\n",
        "  plt.legend(['Accuracy', 'Loss'])\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iApKvCs942aS",
        "colab_type": "text"
      },
      "source": [
        "# Train and evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJse4JU7d9ck",
        "colab_type": "code",
        "outputId": "9e9a7209-e619-4f9b-de23-1f7ac3dc5a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pars_tasks = []\n",
        "test_indexes = []\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "  pars_tasks.insert(task, 0)\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "\n",
        "  train_indexes = trainDS.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes + testDS.__getIndexesGroups__(task)\n",
        "\n",
        "  train_dataset = Subset(trainDS, train_indexes)\n",
        "  test_dataset = Subset(testDS, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE)\n",
        "  test_loader = DataLoader( test_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE )\n",
        "\n",
        "  if(task == 0):\n",
        "    torch.save(resNet, 'resNet_task{0}.pt'.format(task))\n",
        "  \n",
        "  utils.trainfunction(task, train_loader)\n",
        "  param = utils.evaluationTest(task, test_loader) #evaluate test set at step task\n",
        "  pars_tasks[int(task/10)] = param #pars_task[i] = (accuracy, loss) at i-th task\t\n",
        "\n",
        "#plotTask(pars_tasks)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "task = 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "At step  0  and at epoch =  0  the loss is =  0.3767191469669342  and accuracy is =  0.2096\n",
            "At step  0  and at epoch =  1  the loss is =  0.32787877321243286  and accuracy is =  0.3024\n",
            "At step  0  and at epoch =  2  the loss is =  0.2467537671327591  and accuracy is =  0.3896\n",
            "At step  0  and at epoch =  3  the loss is =  0.1633584201335907  and accuracy is =  0.4548\n",
            "At step  0  and at epoch =  4  the loss is =  0.12617488205432892  and accuracy is =  0.4932\n",
            "At step  0  and at epoch =  5  the loss is =  0.0878923162817955  and accuracy is =  0.5466\n",
            "At step  0  and at epoch =  6  the loss is =  0.0683269277215004  and accuracy is =  0.5782\n",
            "At step  0  and at epoch =  7  the loss is =  0.05473507568240166  and accuracy is =  0.6262\n",
            "At step  0  and at epoch =  8  the loss is =  0.059016693383455276  and accuracy is =  0.662\n",
            "At step  0  and at epoch =  9  the loss is =  0.03789397329092026  and accuracy is =  0.687\n",
            "At step  0  and at epoch =  10  the loss is =  0.04552793502807617  and accuracy is =  0.7184\n",
            "At step  0  and at epoch =  11  the loss is =  0.02845231257379055  and accuracy is =  0.7502\n",
            "At step  0  and at epoch =  12  the loss is =  0.019568368792533875  and accuracy is =  0.8\n",
            "At step  0  and at epoch =  13  the loss is =  0.010375677607953548  and accuracy is =  0.807\n",
            "At step  0  and at epoch =  14  the loss is =  0.010900229215621948  and accuracy is =  0.835\n",
            "At step  0  and at epoch =  15  the loss is =  0.013332277536392212  and accuracy is =  0.8734\n",
            "At step  0  and at epoch =  16  the loss is =  0.009968690574169159  and accuracy is =  0.8948\n",
            "At step  0  and at epoch =  17  the loss is =  0.005687345284968615  and accuracy is =  0.8996\n",
            "At step  0  and at epoch =  18  the loss is =  0.0262441486120224  and accuracy is =  0.9176\n",
            "At step  0  and at epoch =  19  the loss is =  0.018984287977218628  and accuracy is =  0.904\n",
            "At step  0  and at epoch =  20  the loss is =  0.0036084663588553667  and accuracy is =  0.9868\n",
            "At step  0  and at epoch =  21  the loss is =  0.002489929087460041  and accuracy is =  0.9964\n",
            "At step  0  and at epoch =  22  the loss is =  0.001985170878469944  and accuracy is =  0.9986\n",
            "At step  0  and at epoch =  23  the loss is =  0.0016756168333813548  and accuracy is =  0.9992\n",
            "At step  0  and at epoch =  24  the loss is =  0.0014496444491669536  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  25  the loss is =  0.0012822415446862578  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  26  the loss is =  0.0011506142327561975  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  27  the loss is =  0.0010448010871186852  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  28  the loss is =  0.0009568949462845922  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  29  the loss is =  0.0009098026785068214  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  30  the loss is =  0.0008961778366938233  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  31  the loss is =  0.0008827431011013687  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  32  the loss is =  0.0008698046440258622  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  33  the loss is =  0.0008572737569920719  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  34  the loss is =  0.0008450261666439474  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  35  the loss is =  0.0008330380660481751  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  36  the loss is =  0.0008214030531235039  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  37  the loss is =  0.0008100590785034001  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  38  the loss is =  0.0007990814629010856  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  39  the loss is =  0.0007925963145680726  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  40  the loss is =  0.000790490594226867  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  41  the loss is =  0.0007883966318331659  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  42  the loss is =  0.0007863041246309876  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  43  the loss is =  0.0007842369377613068  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  44  the loss is =  0.0007821949548088014  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  45  the loss is =  0.0007801543106324971  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  46  the loss is =  0.0007781330496072769  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  47  the loss is =  0.0007761192391626537  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  48  the loss is =  0.000774115789681673  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  49  the loss is =  0.0007728933705948293  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  50  the loss is =  0.000772498082369566  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  51  the loss is =  0.0007720923167653382  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  52  the loss is =  0.0007716940017417073  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  53  the loss is =  0.0007713001105003059  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  54  the loss is =  0.0007709003402851522  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  55  the loss is =  0.0007704989984631538  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  56  the loss is =  0.0007701096474193037  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  57  the loss is =  0.0007697157561779022  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  58  the loss is =  0.0007693145307712257  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  59  the loss is =  0.0007689250633120537  and accuracy is =  1.0\n",
            "Validation Loss: 0.2429603636264801 Validation Accuracy : 0.685\n",
            "task = 10 \n",
            "At step  10  and at epoch =  0  the loss is =  0.22875487804412842  and accuracy is =  0.2872\n",
            "At step  10  and at epoch =  1  the loss is =  0.26677799224853516  and accuracy is =  0.397\n",
            "At step  10  and at epoch =  2  the loss is =  0.20454320311546326  and accuracy is =  0.4676\n",
            "At step  10  and at epoch =  3  the loss is =  0.1559675633907318  and accuracy is =  0.5288\n",
            "At step  10  and at epoch =  4  the loss is =  0.12858203053474426  and accuracy is =  0.5954\n",
            "At step  10  and at epoch =  5  the loss is =  0.11109715700149536  and accuracy is =  0.663\n",
            "At step  10  and at epoch =  6  the loss is =  0.11042355000972748  and accuracy is =  0.7214\n",
            "At step  10  and at epoch =  7  the loss is =  0.08958664536476135  and accuracy is =  0.776\n",
            "At step  10  and at epoch =  8  the loss is =  0.06866353750228882  and accuracy is =  0.8364\n",
            "At step  10  and at epoch =  9  the loss is =  0.06830738484859467  and accuracy is =  0.8658\n",
            "At step  10  and at epoch =  10  the loss is =  0.06931102275848389  and accuracy is =  0.888\n",
            "At step  10  and at epoch =  11  the loss is =  0.06558890640735626  and accuracy is =  0.9286\n",
            "At step  10  and at epoch =  12  the loss is =  0.06066486984491348  and accuracy is =  0.9646\n",
            "At step  10  and at epoch =  13  the loss is =  0.061795949935913086  and accuracy is =  0.977\n",
            "At step  10  and at epoch =  14  the loss is =  0.0530489906668663  and accuracy is =  0.9882\n",
            "At step  10  and at epoch =  15  the loss is =  0.048004936426877975  and accuracy is =  0.9936\n",
            "At step  10  and at epoch =  16  the loss is =  0.04824964702129364  and accuracy is =  0.9972\n",
            "At step  10  and at epoch =  17  the loss is =  0.05294116213917732  and accuracy is =  0.998\n",
            "At step  10  and at epoch =  18  the loss is =  0.0495777353644371  and accuracy is =  0.999\n",
            "At step  10  and at epoch =  19  the loss is =  0.06159798055887222  and accuracy is =  0.9988\n",
            "At step  10  and at epoch =  20  the loss is =  0.048838283866643906  and accuracy is =  0.9998\n",
            "At step  10  and at epoch =  21  the loss is =  0.04545144736766815  and accuracy is =  0.9998\n",
            "At step  10  and at epoch =  22  the loss is =  0.044511277228593826  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  23  the loss is =  0.044294171035289764  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  24  the loss is =  0.04420046880841255  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  25  the loss is =  0.04413215443491936  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  26  the loss is =  0.044073399156332016  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  27  the loss is =  0.044021256268024445  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  28  the loss is =  0.04397687315940857  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  29  the loss is =  0.04390528053045273  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  30  the loss is =  0.04389876872301102  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  31  the loss is =  0.043892599642276764  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  32  the loss is =  0.04388638213276863  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  33  the loss is =  0.04388006776571274  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  34  the loss is =  0.04387374967336655  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  35  the loss is =  0.043867453932762146  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  36  the loss is =  0.043861180543899536  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  37  the loss is =  0.043855052441358566  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  38  the loss is =  0.04384893923997879  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  39  the loss is =  0.04383723810315132  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  40  the loss is =  0.043835557997226715  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  41  the loss is =  0.04383406415581703  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  42  the loss is =  0.04383264482021332  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  43  the loss is =  0.04383133351802826  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  44  the loss is =  0.04383007064461708  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  45  the loss is =  0.043828874826431274  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  46  the loss is =  0.04382769390940666  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  47  the loss is =  0.04382654279470444  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  48  the loss is =  0.043825432658195496  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  49  the loss is =  0.04382336139678955  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  50  the loss is =  0.04382311552762985  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  51  the loss is =  0.04382285848259926  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  52  the loss is =  0.043822623789310455  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  53  the loss is =  0.04382237792015076  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  54  the loss is =  0.04382213205099106  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  55  the loss is =  0.043821901082992554  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  56  the loss is =  0.04382166266441345  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  57  the loss is =  0.04382142797112465  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  58  the loss is =  0.04382118210196495  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  59  the loss is =  0.04382095858454704  and accuracy is =  1.0\n",
            "Validation Loss: 0.22823882102966309 Validation Accuracy : 0.4\n",
            "task = 20 \n",
            "At step  20  and at epoch =  0  the loss is =  0.18517325818538666  and accuracy is =  0.3786\n",
            "At step  20  and at epoch =  1  the loss is =  0.16032788157463074  and accuracy is =  0.4998\n",
            "At step  20  and at epoch =  2  the loss is =  0.1417158842086792  and accuracy is =  0.5644\n",
            "At step  20  and at epoch =  3  the loss is =  0.11298854649066925  and accuracy is =  0.627\n",
            "At step  20  and at epoch =  4  the loss is =  0.10380521416664124  and accuracy is =  0.6788\n",
            "At step  20  and at epoch =  5  the loss is =  0.11182667315006256  and accuracy is =  0.727\n",
            "At step  20  and at epoch =  6  the loss is =  0.09110235422849655  and accuracy is =  0.774\n",
            "At step  20  and at epoch =  7  the loss is =  0.08113577961921692  and accuracy is =  0.8214\n",
            "At step  20  and at epoch =  8  the loss is =  0.07492661476135254  and accuracy is =  0.8654\n",
            "At step  20  and at epoch =  9  the loss is =  0.07596646994352341  and accuracy is =  0.887\n",
            "At step  20  and at epoch =  10  the loss is =  0.07339112460613251  and accuracy is =  0.9254\n",
            "At step  20  and at epoch =  11  the loss is =  0.07463227957487106  and accuracy is =  0.9502\n",
            "At step  20  and at epoch =  12  the loss is =  0.08207782357931137  and accuracy is =  0.9654\n",
            "At step  20  and at epoch =  13  the loss is =  0.07324805110692978  and accuracy is =  0.9726\n",
            "At step  20  and at epoch =  14  the loss is =  0.06875806301832199  and accuracy is =  0.983\n",
            "At step  20  and at epoch =  15  the loss is =  0.06728412210941315  and accuracy is =  0.99\n",
            "At step  20  and at epoch =  16  the loss is =  0.0648600310087204  and accuracy is =  0.9948\n",
            "At step  20  and at epoch =  17  the loss is =  0.06230541691184044  and accuracy is =  0.9952\n",
            "At step  20  and at epoch =  18  the loss is =  0.06069212406873703  and accuracy is =  0.9976\n",
            "At step  20  and at epoch =  19  the loss is =  0.0664922371506691  and accuracy is =  0.9978\n",
            "At step  20  and at epoch =  20  the loss is =  0.05954579636454582  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  21  the loss is =  0.05811899155378342  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  22  the loss is =  0.057647690176963806  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  23  the loss is =  0.05744972452521324  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  24  the loss is =  0.05733443424105644  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  25  the loss is =  0.05724555253982544  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  26  the loss is =  0.05716884881258011  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  27  the loss is =  0.0570979006588459  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  28  the loss is =  0.05703011900186539  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  29  the loss is =  0.05690431594848633  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  30  the loss is =  0.056896012276411057  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  31  the loss is =  0.056884780526161194  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  32  the loss is =  0.05687311664223671  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  33  the loss is =  0.056861307471990585  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  34  the loss is =  0.05684966221451759  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  35  the loss is =  0.0568380281329155  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  36  the loss is =  0.05682650953531265  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  37  the loss is =  0.05681486427783966  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  38  the loss is =  0.05680336058139801  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  39  the loss is =  0.056777726858854294  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  40  the loss is =  0.05677655339241028  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  41  the loss is =  0.05677486211061478  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  42  the loss is =  0.05677282437682152  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  43  the loss is =  0.056770581752061844  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  44  the loss is =  0.05676829069852829  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  45  the loss is =  0.05676596239209175  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  46  the loss is =  0.056763604283332825  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  47  the loss is =  0.056761257350444794  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  48  the loss is =  0.05675887688994408  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  49  the loss is =  0.056753598153591156  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  50  the loss is =  0.056753214448690414  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  51  the loss is =  0.05675283074378967  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  52  the loss is =  0.056752439588308334  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  53  the loss is =  0.056752026081085205  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  54  the loss is =  0.056751593947410583  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  55  the loss is =  0.056751176714897156  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  56  the loss is =  0.05675075203180313  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  57  the loss is =  0.05675031244754791  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  58  the loss is =  0.05674987658858299  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  59  the loss is =  0.056749433279037476  and accuracy is =  0.9998\n",
            "Validation Loss: 0.16238300502300262 Validation Accuracy : 0.353\n",
            "task = 30 \n",
            "At step  30  and at epoch =  0  the loss is =  0.1325034499168396  and accuracy is =  0.3384\n",
            "At step  30  and at epoch =  1  the loss is =  0.17189966142177582  and accuracy is =  0.4604\n",
            "At step  30  and at epoch =  2  the loss is =  0.1700979471206665  and accuracy is =  0.528\n",
            "At step  30  and at epoch =  3  the loss is =  0.15054413676261902  and accuracy is =  0.5672\n",
            "At step  30  and at epoch =  4  the loss is =  0.13335180282592773  and accuracy is =  0.6204\n",
            "At step  30  and at epoch =  5  the loss is =  0.11178132891654968  and accuracy is =  0.659\n",
            "At step  30  and at epoch =  6  the loss is =  0.10603652894496918  and accuracy is =  0.7034\n",
            "At step  30  and at epoch =  7  the loss is =  0.09415080398321152  and accuracy is =  0.746\n",
            "At step  30  and at epoch =  8  the loss is =  0.09174197912216187  and accuracy is =  0.7892\n",
            "At step  30  and at epoch =  9  the loss is =  0.09324464201927185  and accuracy is =  0.8222\n",
            "At step  30  and at epoch =  10  the loss is =  0.0967891663312912  and accuracy is =  0.8458\n",
            "At step  30  and at epoch =  11  the loss is =  0.09808209538459778  and accuracy is =  0.877\n",
            "At step  30  and at epoch =  12  the loss is =  0.08477050811052322  and accuracy is =  0.9066\n",
            "At step  30  and at epoch =  13  the loss is =  0.08060631901025772  and accuracy is =  0.9274\n",
            "At step  30  and at epoch =  14  the loss is =  0.07606340944766998  and accuracy is =  0.9488\n",
            "At step  30  and at epoch =  15  the loss is =  0.07457002997398376  and accuracy is =  0.963\n",
            "At step  30  and at epoch =  16  the loss is =  0.07306776195764542  and accuracy is =  0.9746\n",
            "At step  30  and at epoch =  17  the loss is =  0.07287426292896271  and accuracy is =  0.9852\n",
            "At step  30  and at epoch =  18  the loss is =  0.07428032159805298  and accuracy is =  0.9874\n",
            "At step  30  and at epoch =  19  the loss is =  0.0821012556552887  and accuracy is =  0.9934\n",
            "At step  30  and at epoch =  20  the loss is =  0.07170235365629196  and accuracy is =  0.9972\n",
            "At step  30  and at epoch =  21  the loss is =  0.06966036558151245  and accuracy is =  0.9982\n",
            "At step  30  and at epoch =  22  the loss is =  0.06912483274936676  and accuracy is =  0.9982\n",
            "At step  30  and at epoch =  23  the loss is =  0.06890910863876343  and accuracy is =  0.9984\n",
            "At step  30  and at epoch =  24  the loss is =  0.06877453625202179  and accuracy is =  0.9986\n",
            "At step  30  and at epoch =  25  the loss is =  0.06866855919361115  and accuracy is =  0.999\n",
            "At step  30  and at epoch =  26  the loss is =  0.0685754045844078  and accuracy is =  0.9992\n",
            "At step  30  and at epoch =  27  the loss is =  0.06848996877670288  and accuracy is =  0.9992\n",
            "At step  30  and at epoch =  28  the loss is =  0.06841068714857101  and accuracy is =  0.9992\n",
            "At step  30  and at epoch =  29  the loss is =  0.06831594556570053  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  30  the loss is =  0.06830531358718872  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  31  the loss is =  0.06829150766134262  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  32  the loss is =  0.06827749311923981  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  33  the loss is =  0.06826351583003998  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  34  the loss is =  0.0682496428489685  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  35  the loss is =  0.06823593378067017  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  36  the loss is =  0.06822234392166138  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  37  the loss is =  0.06820901483297348  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  38  the loss is =  0.06819576025009155  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  39  the loss is =  0.06817472726106644  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  40  the loss is =  0.06817356497049332  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  41  the loss is =  0.06817188858985901  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  42  the loss is =  0.06816984713077545  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  43  the loss is =  0.06816764175891876  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  44  the loss is =  0.06816529482603073  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  45  the loss is =  0.06816286593675613  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  46  the loss is =  0.06816042214632034  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  47  the loss is =  0.06815794855356216  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  48  the loss is =  0.06815548241138458  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  49  the loss is =  0.06815099716186523  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  50  the loss is =  0.06815056502819061  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  51  the loss is =  0.0681501179933548  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  52  the loss is =  0.06814967095851898  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  53  the loss is =  0.06814923137426376  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  54  the loss is =  0.06814876198768616  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  55  the loss is =  0.06814830005168915  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  56  the loss is =  0.06814785301685333  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  57  the loss is =  0.06814736872911453  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  58  the loss is =  0.06814689934253693  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  59  the loss is =  0.06814642995595932  and accuracy is =  0.9996\n",
            "Validation Loss: 0.19303862750530243 Validation Accuracy : 0.28\n",
            "task = 40 \n",
            "At step  40  and at epoch =  0  the loss is =  0.12181880325078964  and accuracy is =  0.3478\n",
            "At step  40  and at epoch =  1  the loss is =  0.16664668917655945  and accuracy is =  0.509\n",
            "At step  40  and at epoch =  2  the loss is =  0.19744020700454712  and accuracy is =  0.567\n",
            "At step  40  and at epoch =  3  the loss is =  0.14995244145393372  and accuracy is =  0.5992\n",
            "At step  40  and at epoch =  4  the loss is =  0.11823698878288269  and accuracy is =  0.645\n",
            "At step  40  and at epoch =  5  the loss is =  0.10740061849355698  and accuracy is =  0.6774\n",
            "At step  40  and at epoch =  6  the loss is =  0.10316649079322815  and accuracy is =  0.717\n",
            "At step  40  and at epoch =  7  the loss is =  0.09992727637290955  and accuracy is =  0.7474\n",
            "At step  40  and at epoch =  8  the loss is =  0.10042119771242142  and accuracy is =  0.7762\n",
            "At step  40  and at epoch =  9  the loss is =  0.09700211882591248  and accuracy is =  0.8016\n",
            "At step  40  and at epoch =  10  the loss is =  0.0965728610754013  and accuracy is =  0.829\n",
            "At step  40  and at epoch =  11  the loss is =  0.09584230184555054  and accuracy is =  0.8528\n",
            "At step  40  and at epoch =  12  the loss is =  0.09339521825313568  and accuracy is =  0.8756\n",
            "At step  40  and at epoch =  13  the loss is =  0.08945109695196152  and accuracy is =  0.8912\n",
            "At step  40  and at epoch =  14  the loss is =  0.08959334343671799  and accuracy is =  0.9166\n",
            "At step  40  and at epoch =  15  the loss is =  0.09009932726621628  and accuracy is =  0.9338\n",
            "At step  40  and at epoch =  16  the loss is =  0.08808967471122742  and accuracy is =  0.9452\n",
            "At step  40  and at epoch =  17  the loss is =  0.08702385425567627  and accuracy is =  0.9578\n",
            "At step  40  and at epoch =  18  the loss is =  0.08634437620639801  and accuracy is =  0.9662\n",
            "At step  40  and at epoch =  19  the loss is =  0.09579741209745407  and accuracy is =  0.9782\n",
            "At step  40  and at epoch =  20  the loss is =  0.0849180594086647  and accuracy is =  0.9852\n",
            "At step  40  and at epoch =  21  the loss is =  0.08319627493619919  and accuracy is =  0.9878\n",
            "At step  40  and at epoch =  22  the loss is =  0.08268625289201736  and accuracy is =  0.9902\n",
            "At step  40  and at epoch =  23  the loss is =  0.08247577399015427  and accuracy is =  0.9906\n",
            "At step  40  and at epoch =  24  the loss is =  0.08235099911689758  and accuracy is =  0.991\n",
            "At step  40  and at epoch =  25  the loss is =  0.08225738257169724  and accuracy is =  0.992\n",
            "At step  40  and at epoch =  26  the loss is =  0.08217737823724747  and accuracy is =  0.9926\n",
            "At step  40  and at epoch =  27  the loss is =  0.08210466057062149  and accuracy is =  0.993\n",
            "At step  40  and at epoch =  28  the loss is =  0.08203563094139099  and accuracy is =  0.9936\n",
            "At step  40  and at epoch =  29  the loss is =  0.08189446479082108  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  30  the loss is =  0.08189251273870468  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  31  the loss is =  0.08188626170158386  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  32  the loss is =  0.08187838643789291  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  33  the loss is =  0.08186963200569153  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  34  the loss is =  0.08186036348342896  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  35  the loss is =  0.08185072243213654  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  36  the loss is =  0.08184085786342621  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  37  the loss is =  0.08183066546916962  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  38  the loss is =  0.08182033896446228  and accuracy is =  0.9946\n",
            "At step  40  and at epoch =  39  the loss is =  0.08179191499948502  and accuracy is =  0.9948\n",
            "At step  40  and at epoch =  40  the loss is =  0.08179028332233429  and accuracy is =  0.9948\n",
            "At step  40  and at epoch =  41  the loss is =  0.08178860694169998  and accuracy is =  0.9948\n",
            "At step  40  and at epoch =  42  the loss is =  0.08178684860467911  and accuracy is =  0.9948\n",
            "At step  40  and at epoch =  43  the loss is =  0.08178506791591644  and accuracy is =  0.9948\n",
            "At step  40  and at epoch =  44  the loss is =  0.0817832425236702  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  45  the loss is =  0.08178139477968216  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  46  the loss is =  0.08177953213453293  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  47  the loss is =  0.0817776545882225  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  48  the loss is =  0.08177576959133148  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  49  the loss is =  0.08177023380994797  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  50  the loss is =  0.08176989108324051  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  51  the loss is =  0.08176954090595245  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  52  the loss is =  0.0817691758275032  and accuracy is =  0.9952\n",
            "At step  40  and at epoch =  53  the loss is =  0.08176881819963455  and accuracy is =  0.9952\n",
            "At step  40  and at epoch =  54  the loss is =  0.0817684680223465  and accuracy is =  0.9952\n",
            "At step  40  and at epoch =  55  the loss is =  0.08176808804273605  and accuracy is =  0.9952\n",
            "At step  40  and at epoch =  56  the loss is =  0.0817677229642868  and accuracy is =  0.9952\n",
            "At step  40  and at epoch =  57  the loss is =  0.08176735043525696  and accuracy is =  0.9952\n",
            "At step  40  and at epoch =  58  the loss is =  0.0817670002579689  and accuracy is =  0.9952\n",
            "At step  40  and at epoch =  59  the loss is =  0.08176661282777786  and accuracy is =  0.9952\n",
            "Validation Loss: 0.14173445105552673 Validation Accuracy : 0.2404\n",
            "task = 50 \n",
            "At step  50  and at epoch =  0  the loss is =  0.132627472281456  and accuracy is =  0.3298\n",
            "At step  50  and at epoch =  1  the loss is =  0.1537642627954483  and accuracy is =  0.4706\n",
            "At step  50  and at epoch =  2  the loss is =  0.24847160279750824  and accuracy is =  0.5138\n",
            "At step  50  and at epoch =  3  the loss is =  0.1732681542634964  and accuracy is =  0.555\n",
            "At step  50  and at epoch =  4  the loss is =  0.13522924482822418  and accuracy is =  0.5894\n",
            "At step  50  and at epoch =  5  the loss is =  0.1305404007434845  and accuracy is =  0.6218\n",
            "At step  50  and at epoch =  6  the loss is =  0.12270653247833252  and accuracy is =  0.6506\n",
            "At step  50  and at epoch =  7  the loss is =  0.11754108965396881  and accuracy is =  0.6844\n",
            "At step  50  and at epoch =  8  the loss is =  0.11612718552350998  and accuracy is =  0.7172\n",
            "At step  50  and at epoch =  9  the loss is =  0.11615464091300964  and accuracy is =  0.7456\n",
            "At step  50  and at epoch =  10  the loss is =  0.11808544397354126  and accuracy is =  0.769\n",
            "At step  50  and at epoch =  11  the loss is =  0.11768225580453873  and accuracy is =  0.7928\n",
            "At step  50  and at epoch =  12  the loss is =  0.11794836819171906  and accuracy is =  0.814\n",
            "At step  50  and at epoch =  13  the loss is =  0.11144321411848068  and accuracy is =  0.8416\n",
            "At step  50  and at epoch =  14  the loss is =  0.10756538808345795  and accuracy is =  0.867\n",
            "At step  50  and at epoch =  15  the loss is =  0.10400958359241486  and accuracy is =  0.8926\n",
            "At step  50  and at epoch =  16  the loss is =  0.10242016613483429  and accuracy is =  0.9134\n",
            "At step  50  and at epoch =  17  the loss is =  0.10224981606006622  and accuracy is =  0.9326\n",
            "At step  50  and at epoch =  18  the loss is =  0.10327954590320587  and accuracy is =  0.9418\n",
            "At step  50  and at epoch =  19  the loss is =  0.1132667139172554  and accuracy is =  0.9644\n",
            "At step  50  and at epoch =  20  the loss is =  0.09798181802034378  and accuracy is =  0.9718\n",
            "At step  50  and at epoch =  21  the loss is =  0.09626421332359314  and accuracy is =  0.9742\n",
            "At step  50  and at epoch =  22  the loss is =  0.09574048221111298  and accuracy is =  0.9768\n",
            "At step  50  and at epoch =  23  the loss is =  0.09548713266849518  and accuracy is =  0.9786\n",
            "At step  50  and at epoch =  24  the loss is =  0.09531457722187042  and accuracy is =  0.9804\n",
            "At step  50  and at epoch =  25  the loss is =  0.09517199546098709  and accuracy is =  0.9818\n",
            "At step  50  and at epoch =  26  the loss is =  0.09504327923059464  and accuracy is =  0.983\n",
            "At step  50  and at epoch =  27  the loss is =  0.09492231905460358  and accuracy is =  0.9838\n",
            "At step  50  and at epoch =  28  the loss is =  0.09480765461921692  and accuracy is =  0.985\n",
            "At step  50  and at epoch =  29  the loss is =  0.09457710385322571  and accuracy is =  0.9872\n",
            "At step  50  and at epoch =  30  the loss is =  0.09456052631139755  and accuracy is =  0.9872\n",
            "At step  50  and at epoch =  31  the loss is =  0.09453994035720825  and accuracy is =  0.9872\n",
            "At step  50  and at epoch =  32  the loss is =  0.09451887011528015  and accuracy is =  0.9874\n",
            "At step  50  and at epoch =  33  the loss is =  0.09449797123670578  and accuracy is =  0.9876\n",
            "At step  50  and at epoch =  34  the loss is =  0.09447739273309708  and accuracy is =  0.9876\n",
            "At step  50  and at epoch =  35  the loss is =  0.09445711970329285  and accuracy is =  0.9876\n",
            "At step  50  and at epoch =  36  the loss is =  0.09443698823451996  and accuracy is =  0.9876\n",
            "At step  50  and at epoch =  37  the loss is =  0.09441706538200378  and accuracy is =  0.9878\n",
            "At step  50  and at epoch =  38  the loss is =  0.09439732134342194  and accuracy is =  0.988\n",
            "At step  50  and at epoch =  39  the loss is =  0.09435344487428665  and accuracy is =  0.9886\n",
            "At step  50  and at epoch =  40  the loss is =  0.09435081481933594  and accuracy is =  0.9884\n",
            "At step  50  and at epoch =  41  the loss is =  0.09434774518013  and accuracy is =  0.9884\n",
            "At step  50  and at epoch =  42  the loss is =  0.09434439241886139  and accuracy is =  0.9884\n",
            "At step  50  and at epoch =  43  the loss is =  0.09434085339307785  and accuracy is =  0.9884\n",
            "At step  50  and at epoch =  44  the loss is =  0.09433718770742416  and accuracy is =  0.9884\n",
            "At step  50  and at epoch =  45  the loss is =  0.09433344751596451  and accuracy is =  0.9884\n",
            "At step  50  and at epoch =  46  the loss is =  0.09432967007160187  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  47  the loss is =  0.09432587027549744  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  48  the loss is =  0.09432203322649002  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  49  the loss is =  0.09431321173906326  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  50  the loss is =  0.09431250393390656  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  51  the loss is =  0.09431181848049164  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  52  the loss is =  0.09431111067533493  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  53  the loss is =  0.09431040287017822  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  54  the loss is =  0.09430970996618271  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  55  the loss is =  0.094309002161026  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  56  the loss is =  0.0943082645535469  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  57  the loss is =  0.0943075567483902  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  58  the loss is =  0.0943068116903305  and accuracy is =  0.9888\n",
            "At step  50  and at epoch =  59  the loss is =  0.0943060889840126  and accuracy is =  0.9888\n",
            "Validation Loss: 0.1458730548620224 Validation Accuracy : 0.21033333333333334\n",
            "task = 60 \n",
            "At step  60  and at epoch =  0  the loss is =  0.13394573330879211  and accuracy is =  0.3578\n",
            "At step  60  and at epoch =  1  the loss is =  0.14616256952285767  and accuracy is =  0.5342\n",
            "At step  60  and at epoch =  2  the loss is =  0.2182350754737854  and accuracy is =  0.5784\n",
            "At step  60  and at epoch =  3  the loss is =  0.1681501567363739  and accuracy is =  0.5976\n",
            "At step  60  and at epoch =  4  the loss is =  0.145661398768425  and accuracy is =  0.6232\n",
            "At step  60  and at epoch =  5  the loss is =  0.14310888946056366  and accuracy is =  0.6516\n",
            "At step  60  and at epoch =  6  the loss is =  0.14836077392101288  and accuracy is =  0.673\n",
            "At step  60  and at epoch =  7  the loss is =  0.13783591985702515  and accuracy is =  0.6852\n",
            "At step  60  and at epoch =  8  the loss is =  0.12636421620845795  and accuracy is =  0.7118\n",
            "At step  60  and at epoch =  9  the loss is =  0.12363042682409286  and accuracy is =  0.7318\n",
            "At step  60  and at epoch =  10  the loss is =  0.11934389919042587  and accuracy is =  0.7536\n",
            "At step  60  and at epoch =  11  the loss is =  0.11774470657110214  and accuracy is =  0.775\n",
            "At step  60  and at epoch =  12  the loss is =  0.11581449210643768  and accuracy is =  0.7916\n",
            "At step  60  and at epoch =  13  the loss is =  0.11501380056142807  and accuracy is =  0.8098\n",
            "At step  60  and at epoch =  14  the loss is =  0.11410553008317947  and accuracy is =  0.825\n",
            "At step  60  and at epoch =  15  the loss is =  0.11426641792058945  and accuracy is =  0.8414\n",
            "At step  60  and at epoch =  16  the loss is =  0.11361813545227051  and accuracy is =  0.855\n",
            "At step  60  and at epoch =  17  the loss is =  0.11391645669937134  and accuracy is =  0.8734\n",
            "At step  60  and at epoch =  18  the loss is =  0.11441867798566818  and accuracy is =  0.8894\n",
            "At step  60  and at epoch =  19  the loss is =  0.12266300618648529  and accuracy is =  0.9084\n",
            "At step  60  and at epoch =  20  the loss is =  0.11179859936237335  and accuracy is =  0.9194\n",
            "At step  60  and at epoch =  21  the loss is =  0.11011219769716263  and accuracy is =  0.9246\n",
            "At step  60  and at epoch =  22  the loss is =  0.1096981018781662  and accuracy is =  0.9276\n",
            "At step  60  and at epoch =  23  the loss is =  0.10951579362154007  and accuracy is =  0.9306\n",
            "At step  60  and at epoch =  24  the loss is =  0.10939333587884903  and accuracy is =  0.9344\n",
            "At step  60  and at epoch =  25  the loss is =  0.10929366201162338  and accuracy is =  0.9372\n",
            "At step  60  and at epoch =  26  the loss is =  0.10920443385839462  and accuracy is =  0.939\n",
            "At step  60  and at epoch =  27  the loss is =  0.1091204583644867  and accuracy is =  0.9416\n",
            "At step  60  and at epoch =  28  the loss is =  0.1090400293469429  and accuracy is =  0.9446\n",
            "At step  60  and at epoch =  29  the loss is =  0.10893336683511734  and accuracy is =  0.9484\n",
            "At step  60  and at epoch =  30  the loss is =  0.10892955213785172  and accuracy is =  0.949\n",
            "At step  60  and at epoch =  31  the loss is =  0.10892099142074585  and accuracy is =  0.9494\n",
            "At step  60  and at epoch =  32  the loss is =  0.10891018807888031  and accuracy is =  0.9496\n",
            "At step  60  and at epoch =  33  the loss is =  0.10889816284179688  and accuracy is =  0.95\n",
            "At step  60  and at epoch =  34  the loss is =  0.10888534039258957  and accuracy is =  0.9506\n",
            "At step  60  and at epoch =  35  the loss is =  0.10887216776609421  and accuracy is =  0.9508\n",
            "At step  60  and at epoch =  36  the loss is =  0.10885865986347198  and accuracy is =  0.951\n",
            "At step  60  and at epoch =  37  the loss is =  0.10884499549865723  and accuracy is =  0.9512\n",
            "At step  60  and at epoch =  38  the loss is =  0.10883119702339172  and accuracy is =  0.9516\n",
            "At step  60  and at epoch =  39  the loss is =  0.10880803316831589  and accuracy is =  0.9528\n",
            "At step  60  and at epoch =  40  the loss is =  0.10880626738071442  and accuracy is =  0.9528\n",
            "At step  60  and at epoch =  41  the loss is =  0.10880432277917862  and accuracy is =  0.953\n",
            "At step  60  and at epoch =  42  the loss is =  0.10880220681428909  and accuracy is =  0.953\n",
            "At step  60  and at epoch =  43  the loss is =  0.1088000163435936  and accuracy is =  0.953\n",
            "At step  60  and at epoch =  44  the loss is =  0.10879769176244736  and accuracy is =  0.953\n",
            "At step  60  and at epoch =  45  the loss is =  0.10879532247781754  and accuracy is =  0.953\n",
            "At step  60  and at epoch =  46  the loss is =  0.10879290848970413  and accuracy is =  0.953\n",
            "At step  60  and at epoch =  47  the loss is =  0.10879046469926834  and accuracy is =  0.9532\n",
            "At step  60  and at epoch =  48  the loss is =  0.10878796130418777  and accuracy is =  0.9532\n",
            "At step  60  and at epoch =  49  the loss is =  0.10878327488899231  and accuracy is =  0.9534\n",
            "At step  60  and at epoch =  50  the loss is =  0.1087828129529953  and accuracy is =  0.9534\n",
            "At step  60  and at epoch =  51  the loss is =  0.10878235101699829  and accuracy is =  0.9534\n",
            "At step  60  and at epoch =  52  the loss is =  0.10878189653158188  and accuracy is =  0.9534\n",
            "At step  60  and at epoch =  53  the loss is =  0.10878142714500427  and accuracy is =  0.9534\n",
            "At step  60  and at epoch =  54  the loss is =  0.10878097265958786  and accuracy is =  0.9534\n",
            "At step  60  and at epoch =  55  the loss is =  0.10878050327301025  and accuracy is =  0.9534\n",
            "At step  60  and at epoch =  56  the loss is =  0.10878003388643265  and accuracy is =  0.9534\n",
            "At step  60  and at epoch =  57  the loss is =  0.10877956449985504  and accuracy is =  0.9536\n",
            "At step  60  and at epoch =  58  the loss is =  0.10877907276153564  and accuracy is =  0.9536\n",
            "At step  60  and at epoch =  59  the loss is =  0.10877860337495804  and accuracy is =  0.9536\n",
            "Validation Loss: 0.15740926563739777 Validation Accuracy : 0.20357142857142857\n",
            "task = 70 \n",
            "At step  70  and at epoch =  0  the loss is =  0.12455130368471146  and accuracy is =  0.4146\n",
            "At step  70  and at epoch =  1  the loss is =  0.13788804411888123  and accuracy is =  0.5512\n",
            "At step  70  and at epoch =  2  the loss is =  0.21700648963451385  and accuracy is =  0.5872\n",
            "At step  70  and at epoch =  3  the loss is =  0.15698757767677307  and accuracy is =  0.6096\n",
            "At step  70  and at epoch =  4  the loss is =  0.13667643070220947  and accuracy is =  0.637\n",
            "At step  70  and at epoch =  5  the loss is =  0.1308993697166443  and accuracy is =  0.6528\n",
            "At step  70  and at epoch =  6  the loss is =  0.125117227435112  and accuracy is =  0.6742\n",
            "At step  70  and at epoch =  7  the loss is =  0.1162843182682991  and accuracy is =  0.6954\n",
            "At step  70  and at epoch =  8  the loss is =  0.1143975704908371  and accuracy is =  0.7144\n",
            "At step  70  and at epoch =  9  the loss is =  0.11195935308933258  and accuracy is =  0.726\n",
            "At step  70  and at epoch =  10  the loss is =  0.11078145354986191  and accuracy is =  0.7454\n",
            "At step  70  and at epoch =  11  the loss is =  0.10958864539861679  and accuracy is =  0.7598\n",
            "At step  70  and at epoch =  12  the loss is =  0.10895396769046783  and accuracy is =  0.7816\n",
            "At step  70  and at epoch =  13  the loss is =  0.10885251313447952  and accuracy is =  0.7986\n",
            "At step  70  and at epoch =  14  the loss is =  0.10928497463464737  and accuracy is =  0.814\n",
            "At step  70  and at epoch =  15  the loss is =  0.1104968935251236  and accuracy is =  0.8328\n",
            "At step  70  and at epoch =  16  the loss is =  0.11074912548065186  and accuracy is =  0.8452\n",
            "At step  70  and at epoch =  17  the loss is =  0.11022701114416122  and accuracy is =  0.8574\n",
            "At step  70  and at epoch =  18  the loss is =  0.1107933297753334  and accuracy is =  0.867\n",
            "At step  70  and at epoch =  19  the loss is =  0.11489400267601013  and accuracy is =  0.8956\n",
            "At step  70  and at epoch =  20  the loss is =  0.10554878413677216  and accuracy is =  0.9016\n",
            "At step  70  and at epoch =  21  the loss is =  0.10474904626607895  and accuracy is =  0.9058\n",
            "At step  70  and at epoch =  22  the loss is =  0.1045086681842804  and accuracy is =  0.909\n",
            "At step  70  and at epoch =  23  the loss is =  0.10437668114900589  and accuracy is =  0.9118\n",
            "At step  70  and at epoch =  24  the loss is =  0.10427624732255936  and accuracy is =  0.9156\n",
            "At step  70  and at epoch =  25  the loss is =  0.10418765991926193  and accuracy is =  0.9186\n",
            "At step  70  and at epoch =  26  the loss is =  0.1041054055094719  and accuracy is =  0.9218\n",
            "At step  70  and at epoch =  27  the loss is =  0.10402744263410568  and accuracy is =  0.9234\n",
            "At step  70  and at epoch =  28  the loss is =  0.10395310074090958  and accuracy is =  0.9256\n",
            "At step  70  and at epoch =  29  the loss is =  0.10380683839321136  and accuracy is =  0.9282\n",
            "At step  70  and at epoch =  30  the loss is =  0.10380343347787857  and accuracy is =  0.929\n",
            "At step  70  and at epoch =  31  the loss is =  0.10379558801651001  and accuracy is =  0.9302\n",
            "At step  70  and at epoch =  32  the loss is =  0.10378596931695938  and accuracy is =  0.9306\n",
            "At step  70  and at epoch =  33  the loss is =  0.10377534478902817  and accuracy is =  0.9308\n",
            "At step  70  and at epoch =  34  the loss is =  0.10376404225826263  and accuracy is =  0.9314\n",
            "At step  70  and at epoch =  35  the loss is =  0.10375232994556427  and accuracy is =  0.932\n",
            "At step  70  and at epoch =  36  the loss is =  0.10374031215906143  and accuracy is =  0.9324\n",
            "At step  70  and at epoch =  37  the loss is =  0.10372816026210785  and accuracy is =  0.9332\n",
            "At step  70  and at epoch =  38  the loss is =  0.10371588170528412  and accuracy is =  0.9336\n",
            "At step  70  and at epoch =  39  the loss is =  0.10368622094392776  and accuracy is =  0.9342\n",
            "At step  70  and at epoch =  40  the loss is =  0.1036846935749054  and accuracy is =  0.9342\n",
            "At step  70  and at epoch =  41  the loss is =  0.1036829799413681  and accuracy is =  0.9346\n",
            "At step  70  and at epoch =  42  the loss is =  0.10368114709854126  and accuracy is =  0.9346\n",
            "At step  70  and at epoch =  43  the loss is =  0.10367919504642487  and accuracy is =  0.9346\n",
            "At step  70  and at epoch =  44  the loss is =  0.1036771833896637  and accuracy is =  0.9346\n",
            "At step  70  and at epoch =  45  the loss is =  0.10367509722709656  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  46  the loss is =  0.10367295891046524  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  47  the loss is =  0.10367077589035034  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  48  the loss is =  0.10366857051849365  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  49  the loss is =  0.1036626398563385  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  50  the loss is =  0.10366223752498627  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  51  the loss is =  0.10366185009479523  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  52  the loss is =  0.1036614254117012  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  53  the loss is =  0.10366101562976837  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  54  the loss is =  0.10366061329841614  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  55  the loss is =  0.1036602035164833  and accuracy is =  0.9348\n",
            "At step  70  and at epoch =  56  the loss is =  0.10365978628396988  and accuracy is =  0.935\n",
            "At step  70  and at epoch =  57  the loss is =  0.10365936905145645  and accuracy is =  0.935\n",
            "At step  70  and at epoch =  58  the loss is =  0.10365894436836243  and accuracy is =  0.935\n",
            "At step  70  and at epoch =  59  the loss is =  0.1036585196852684  and accuracy is =  0.935\n",
            "Validation Loss: 0.15285001695156097 Validation Accuracy : 0.18625\n",
            "task = 80 \n",
            "At step  80  and at epoch =  0  the loss is =  0.12209726870059967  and accuracy is =  0.3072\n",
            "At step  80  and at epoch =  1  the loss is =  0.13415728509426117  and accuracy is =  0.464\n",
            "At step  80  and at epoch =  2  the loss is =  0.20661786198616028  and accuracy is =  0.5152\n",
            "At step  80  and at epoch =  3  the loss is =  0.15746721625328064  and accuracy is =  0.5474\n",
            "At step  80  and at epoch =  4  the loss is =  0.13054828345775604  and accuracy is =  0.5754\n",
            "At step  80  and at epoch =  5  the loss is =  0.12615558505058289  and accuracy is =  0.5996\n",
            "At step  80  and at epoch =  6  the loss is =  0.1214134693145752  and accuracy is =  0.6208\n",
            "At step  80  and at epoch =  7  the loss is =  0.11796511709690094  and accuracy is =  0.645\n",
            "At step  80  and at epoch =  8  the loss is =  0.11379680782556534  and accuracy is =  0.6596\n",
            "At step  80  and at epoch =  9  the loss is =  0.11535254865884781  and accuracy is =  0.6822\n",
            "At step  80  and at epoch =  10  the loss is =  0.11718206107616425  and accuracy is =  0.6964\n",
            "At step  80  and at epoch =  11  the loss is =  0.11325784027576447  and accuracy is =  0.7188\n",
            "At step  80  and at epoch =  12  the loss is =  0.10858742892742157  and accuracy is =  0.7318\n",
            "At step  80  and at epoch =  13  the loss is =  0.10725545883178711  and accuracy is =  0.7546\n",
            "At step  80  and at epoch =  14  the loss is =  0.1055973470211029  and accuracy is =  0.7698\n",
            "At step  80  and at epoch =  15  the loss is =  0.10549712926149368  and accuracy is =  0.7858\n",
            "At step  80  and at epoch =  16  the loss is =  0.1049615889787674  and accuracy is =  0.8018\n",
            "At step  80  and at epoch =  17  the loss is =  0.10606155544519424  and accuracy is =  0.819\n",
            "At step  80  and at epoch =  18  the loss is =  0.10661928355693817  and accuracy is =  0.828\n",
            "At step  80  and at epoch =  19  the loss is =  0.1145944595336914  and accuracy is =  0.8524\n",
            "At step  80  and at epoch =  20  the loss is =  0.10323818027973175  and accuracy is =  0.8604\n",
            "At step  80  and at epoch =  21  the loss is =  0.10129600763320923  and accuracy is =  0.865\n",
            "At step  80  and at epoch =  22  the loss is =  0.10068360716104507  and accuracy is =  0.8678\n",
            "At step  80  and at epoch =  23  the loss is =  0.10040825605392456  and accuracy is =  0.8712\n",
            "At step  80  and at epoch =  24  the loss is =  0.10024212300777435  and accuracy is =  0.8746\n",
            "At step  80  and at epoch =  25  the loss is =  0.10011383146047592  and accuracy is =  0.8774\n",
            "At step  80  and at epoch =  26  the loss is =  0.10000048577785492  and accuracy is =  0.8802\n",
            "At step  80  and at epoch =  27  the loss is =  0.0998954027891159  and accuracy is =  0.883\n",
            "At step  80  and at epoch =  28  the loss is =  0.09979388862848282  and accuracy is =  0.8858\n",
            "At step  80  and at epoch =  29  the loss is =  0.09950857609510422  and accuracy is =  0.8898\n",
            "At step  80  and at epoch =  30  the loss is =  0.0995112881064415  and accuracy is =  0.891\n",
            "At step  80  and at epoch =  31  the loss is =  0.09950573742389679  and accuracy is =  0.8914\n",
            "At step  80  and at epoch =  32  the loss is =  0.099496029317379  and accuracy is =  0.892\n",
            "At step  80  and at epoch =  33  the loss is =  0.09948385506868362  and accuracy is =  0.8928\n",
            "At step  80  and at epoch =  34  the loss is =  0.09947007149457932  and accuracy is =  0.893\n",
            "At step  80  and at epoch =  35  the loss is =  0.09945521503686905  and accuracy is =  0.8934\n",
            "At step  80  and at epoch =  36  the loss is =  0.09943971037864685  and accuracy is =  0.8942\n",
            "At step  80  and at epoch =  37  the loss is =  0.09942372143268585  and accuracy is =  0.8946\n",
            "At step  80  and at epoch =  38  the loss is =  0.09940735995769501  and accuracy is =  0.8948\n",
            "At step  80  and at epoch =  39  the loss is =  0.09934943169355392  and accuracy is =  0.8958\n",
            "At step  80  and at epoch =  40  the loss is =  0.09934726357460022  and accuracy is =  0.896\n",
            "At step  80  and at epoch =  41  the loss is =  0.0993448868393898  and accuracy is =  0.896\n",
            "At step  80  and at epoch =  42  the loss is =  0.09934233874082565  and accuracy is =  0.8962\n",
            "At step  80  and at epoch =  43  the loss is =  0.09933966398239136  and accuracy is =  0.8966\n",
            "At step  80  and at epoch =  44  the loss is =  0.0993368923664093  and accuracy is =  0.8964\n",
            "At step  80  and at epoch =  45  the loss is =  0.09933404624462128  and accuracy is =  0.8964\n",
            "At step  80  and at epoch =  46  the loss is =  0.09933115541934967  and accuracy is =  0.8966\n",
            "At step  80  and at epoch =  47  the loss is =  0.09932821244001389  and accuracy is =  0.8968\n",
            "At step  80  and at epoch =  48  the loss is =  0.09932522475719452  and accuracy is =  0.8968\n",
            "At step  80  and at epoch =  49  the loss is =  0.09931370615959167  and accuracy is =  0.8974\n",
            "At step  80  and at epoch =  50  the loss is =  0.0993131697177887  and accuracy is =  0.8976\n",
            "At step  80  and at epoch =  51  the loss is =  0.09931260347366333  and accuracy is =  0.8976\n",
            "At step  80  and at epoch =  52  the loss is =  0.09931202232837677  and accuracy is =  0.8976\n",
            "At step  80  and at epoch =  53  the loss is =  0.099311463534832  and accuracy is =  0.8974\n",
            "At step  80  and at epoch =  54  the loss is =  0.09931090474128723  and accuracy is =  0.8974\n",
            "At step  80  and at epoch =  55  the loss is =  0.09931031614542007  and accuracy is =  0.8974\n",
            "At step  80  and at epoch =  56  the loss is =  0.09930974990129471  and accuracy is =  0.8974\n",
            "At step  80  and at epoch =  57  the loss is =  0.09930917620658875  and accuracy is =  0.8974\n",
            "At step  80  and at epoch =  58  the loss is =  0.09930860251188278  and accuracy is =  0.8974\n",
            "At step  80  and at epoch =  59  the loss is =  0.09930800646543503  and accuracy is =  0.8974\n",
            "Validation Loss: 0.14081406593322754 Validation Accuracy : 0.1698888888888889\n",
            "task = 90 \n",
            "At step  90  and at epoch =  0  the loss is =  0.13968722522258759  and accuracy is =  0.383\n",
            "At step  90  and at epoch =  1  the loss is =  0.14610245823860168  and accuracy is =  0.574\n",
            "At step  90  and at epoch =  2  the loss is =  0.20944014191627502  and accuracy is =  0.621\n",
            "At step  90  and at epoch =  3  the loss is =  0.1747477948665619  and accuracy is =  0.652\n",
            "At step  90  and at epoch =  4  the loss is =  0.15032798051834106  and accuracy is =  0.6678\n",
            "At step  90  and at epoch =  5  the loss is =  0.14856146275997162  and accuracy is =  0.6862\n",
            "At step  90  and at epoch =  6  the loss is =  0.144174724817276  and accuracy is =  0.6978\n",
            "At step  90  and at epoch =  7  the loss is =  0.14184297621250153  and accuracy is =  0.7148\n",
            "At step  90  and at epoch =  8  the loss is =  0.1364147812128067  and accuracy is =  0.7284\n",
            "At step  90  and at epoch =  9  the loss is =  0.1340046226978302  and accuracy is =  0.7404\n",
            "At step  90  and at epoch =  10  the loss is =  0.1292106807231903  and accuracy is =  0.7572\n",
            "At step  90  and at epoch =  11  the loss is =  0.12747526168823242  and accuracy is =  0.7718\n",
            "At step  90  and at epoch =  12  the loss is =  0.12665309011936188  and accuracy is =  0.7868\n",
            "At step  90  and at epoch =  13  the loss is =  0.12593363225460052  and accuracy is =  0.7972\n",
            "At step  90  and at epoch =  14  the loss is =  0.12552331387996674  and accuracy is =  0.8074\n",
            "At step  90  and at epoch =  15  the loss is =  0.12498077005147934  and accuracy is =  0.8204\n",
            "At step  90  and at epoch =  16  the loss is =  0.12481465190649033  and accuracy is =  0.8284\n",
            "At step  90  and at epoch =  17  the loss is =  0.12380929291248322  and accuracy is =  0.8404\n",
            "At step  90  and at epoch =  18  the loss is =  0.12355880439281464  and accuracy is =  0.851\n",
            "At step  90  and at epoch =  19  the loss is =  0.12698736786842346  and accuracy is =  0.8658\n",
            "At step  90  and at epoch =  20  the loss is =  0.12038492411375046  and accuracy is =  0.8732\n",
            "At step  90  and at epoch =  21  the loss is =  0.1198471188545227  and accuracy is =  0.8768\n",
            "At step  90  and at epoch =  22  the loss is =  0.11969193071126938  and accuracy is =  0.8802\n",
            "At step  90  and at epoch =  23  the loss is =  0.11960355937480927  and accuracy is =  0.8816\n",
            "At step  90  and at epoch =  24  the loss is =  0.11953050643205643  and accuracy is =  0.8826\n",
            "At step  90  and at epoch =  25  the loss is =  0.11946181952953339  and accuracy is =  0.8842\n",
            "At step  90  and at epoch =  26  the loss is =  0.11939496546983719  and accuracy is =  0.8854\n",
            "At step  90  and at epoch =  27  the loss is =  0.11932889372110367  and accuracy is =  0.8884\n",
            "At step  90  and at epoch =  28  the loss is =  0.11926373094320297  and accuracy is =  0.8896\n",
            "At step  90  and at epoch =  29  the loss is =  0.11899729818105698  and accuracy is =  0.8938\n",
            "At step  90  and at epoch =  30  the loss is =  0.11899910122156143  and accuracy is =  0.8948\n",
            "At step  90  and at epoch =  31  the loss is =  0.11899681389331818  and accuracy is =  0.8946\n",
            "At step  90  and at epoch =  32  the loss is =  0.11899209022521973  and accuracy is =  0.895\n",
            "At step  90  and at epoch =  33  the loss is =  0.11898574233055115  and accuracy is =  0.8952\n",
            "At step  90  and at epoch =  34  the loss is =  0.11897815018892288  and accuracy is =  0.8956\n",
            "At step  90  and at epoch =  35  the loss is =  0.11896966397762299  and accuracy is =  0.896\n",
            "At step  90  and at epoch =  36  the loss is =  0.11896049231290817  and accuracy is =  0.8966\n",
            "At step  90  and at epoch =  37  the loss is =  0.11895081400871277  and accuracy is =  0.897\n",
            "At step  90  and at epoch =  38  the loss is =  0.11894071102142334  and accuracy is =  0.8974\n",
            "At step  90  and at epoch =  39  the loss is =  0.11888711899518967  and accuracy is =  0.8972\n",
            "At step  90  and at epoch =  40  the loss is =  0.11888541281223297  and accuracy is =  0.8976\n",
            "At step  90  and at epoch =  41  the loss is =  0.11888372153043747  and accuracy is =  0.8976\n",
            "At step  90  and at epoch =  42  the loss is =  0.11888205260038376  and accuracy is =  0.8978\n",
            "At step  90  and at epoch =  43  the loss is =  0.11888033896684647  and accuracy is =  0.898\n",
            "At step  90  and at epoch =  44  the loss is =  0.11887861043214798  and accuracy is =  0.8982\n",
            "At step  90  and at epoch =  45  the loss is =  0.1188768669962883  and accuracy is =  0.8984\n",
            "At step  90  and at epoch =  46  the loss is =  0.11887507885694504  and accuracy is =  0.8984\n",
            "At step  90  and at epoch =  47  the loss is =  0.11887326836585999  and accuracy is =  0.8986\n",
            "At step  90  and at epoch =  48  the loss is =  0.11887144297361374  and accuracy is =  0.8986\n",
            "At step  90  and at epoch =  49  the loss is =  0.11886091530323029  and accuracy is =  0.8994\n",
            "At step  90  and at epoch =  50  the loss is =  0.11886055022478104  and accuracy is =  0.8994\n",
            "At step  90  and at epoch =  51  the loss is =  0.11886018514633179  and accuracy is =  0.8994\n",
            "At step  90  and at epoch =  52  the loss is =  0.11885981261730194  and accuracy is =  0.8994\n",
            "At step  90  and at epoch =  53  the loss is =  0.1188594400882721  and accuracy is =  0.8994\n",
            "At step  90  and at epoch =  54  the loss is =  0.11885907500982285  and accuracy is =  0.8994\n",
            "At step  90  and at epoch =  55  the loss is =  0.118858702480793  and accuracy is =  0.8996\n",
            "At step  90  and at epoch =  56  the loss is =  0.11885834485292435  and accuracy is =  0.8998\n",
            "At step  90  and at epoch =  57  the loss is =  0.1188579723238945  and accuracy is =  0.8998\n",
            "At step  90  and at epoch =  58  the loss is =  0.11885760724544525  and accuracy is =  0.8998\n",
            "At step  90  and at epoch =  59  the loss is =  0.118857242166996  and accuracy is =  0.8998\n",
            "Validation Loss: 0.12681891024112701 Validation Accuracy : 0.1704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHm9_i7WM7c-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "661ce43e-53bf-447b-f234-4fd83a6d3943"
      },
      "source": [
        "plotTask(pars_tasks)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_e0iit-M-SU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}