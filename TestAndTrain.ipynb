{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d556b3d1cb24fb78607a9b2aff463d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9ddc8e75bf81497aaf65e74a97b9eac1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_140d50d17ff44039bb4fe0320a27af86",
              "IPY_MODEL_13ae692a645246c4822348b3e1d05268"
            ]
          }
        },
        "9ddc8e75bf81497aaf65e74a97b9eac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "140d50d17ff44039bb4fe0320a27af86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_36c9cc51c0fe48e1aef632be86b23f29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e049a2cf7b8a4570af3e209938d8326c"
          }
        },
        "13ae692a645246c4822348b3e1d05268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d446150645cf4a5586cdb9580304b0a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:08&lt;00:00, 19273251.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38e7e4e0e26b4ac8bf9292ee5297858e"
          }
        },
        "36c9cc51c0fe48e1aef632be86b23f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e049a2cf7b8a4570af3e209938d8326c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d446150645cf4a5586cdb9580304b0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38e7e4e0e26b4ac8bf9292ee5297858e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/Lucia/TestAndTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbtGDBU3QJaq",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf0TmOM3NdFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I0pKIVIM2KC",
        "colab_type": "code",
        "outputId": "91cd4e8b-5373-4470-ccfa-33e19a9b5003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone -b Lucia https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 130, done.\u001b[K\n",
            "remote: Counting objects: 100% (130/130), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 267 (delta 79), reused 18 (delta 8), pack-reused 137\u001b[K\n",
            "Receiving objects: 100% (267/267), 200.89 KiB | 10.04 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLaS2laafBaG",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liUP5Kc1DMbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from DatasetCIFAR.data_set import Dataset \n",
        "from DatasetCIFAR import ResNet\n",
        "from DatasetCIFAR import utils\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQK7sS-Hed0j",
        "colab_type": "text"
      },
      "source": [
        "# Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eote8TwxYkfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 100\n",
        "TASK_SIZE = 10\n",
        "############################################\n",
        "#NUM_EPOCHS = 70\n",
        "NUM_EPOCHS = 5\n",
        "############################################\n",
        "\n",
        "\n",
        "WEIGHT_DECAY = 0.00001\n",
        "LR = 2\n",
        "STEP_SIZE = [49,63]\n",
        "GAMMA = 1/5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_vlqOL7ehLC",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWttFW3ljoMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resNet = ResNet.resnet32(num_classes=100)\n",
        "resNet = resNet.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmohsyVWFpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet_transformer = transforms.Compose([transforms.Resize(32), \n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_cyhIzFej5-",
        "colab_type": "text"
      },
      "source": [
        "# Define DataSets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcBNohmiYBtP",
        "colab_type": "code",
        "outputId": "829a8e48-3832-482f-8b9a-510f76f38743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "2d556b3d1cb24fb78607a9b2aff463d6",
            "9ddc8e75bf81497aaf65e74a97b9eac1",
            "140d50d17ff44039bb4fe0320a27af86",
            "13ae692a645246c4822348b3e1d05268",
            "36c9cc51c0fe48e1aef632be86b23f29",
            "e049a2cf7b8a4570af3e209938d8326c",
            "d446150645cf4a5586cdb9580304b0a2",
            "38e7e4e0e26b4ac8bf9292ee5297858e"
          ]
        }
      },
      "source": [
        "trainDS = Dataset(train=True, transform = resnet_transformer)\n",
        "testDS = Dataset(train=False, transform = resnet_transformer)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d556b3d1cb24fb78607a9b2aff463d6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAT2KQEersx",
        "colab_type": "text"
      },
      "source": [
        "# Useful plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1l7flYj4NJh",
        "colab_type": "text"
      },
      "source": [
        "The function plotEpoch plots, at the end of each task, how accuracy and loss change during the training phase. It show\n",
        "\n",
        "*   Validation and Training Accuracy\n",
        "*   Validation and Training Loss\n",
        "\n",
        "The function plotTask, for each task, how the accuracy on the validation set change when adding new tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr58kkHiIzZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTask(pars_tasks):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  x_tasks =  np.linspace(10, 100, 10)\n",
        "\n",
        "  plt.plot(x_tasks, pars_tasks ,'b', label='Accuracy')\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.title('Accuracy over classes')\n",
        "  plt.legend(['Validation Accuracy'])\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iApKvCs942aS",
        "colab_type": "text"
      },
      "source": [
        "# Train and evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJse4JU7d9ck",
        "colab_type": "code",
        "outputId": "a6fd5e62-5cd7-4564-ba06-0785545816e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pars_tasks = []\n",
        "test_indexes = []\n",
        "\n",
        "for task in range(0, 100, TASK_SIZE):\n",
        "  pars_tasks.insert(task, 0)\n",
        "\n",
        "for task in range(0, 100, TASK_SIZE):\n",
        "\n",
        "  train_indexes = trainDS.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes + testDS.__getIndexesGroups__(task)\n",
        "\n",
        "  train_dataset = Subset(trainDS, train_indexes)\n",
        "  test_dataset = Subset(testDS, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n",
        "  test_loader = DataLoader( test_dataset, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE )\n",
        "\n",
        "  if(task == 0):\n",
        "    torch.save(resNet, 'resNet_task{0}.pt'.format(task))\n",
        "  \n",
        "  utils.trainfunction(task, train_loader)\n",
        "  param = utils.evaluationTest(task, test_loader) #evaluate test set at step task\n",
        "  pars_tasks[int(task/10)] = param #pars_task[i] = (accuracy, loss) at i-th task\t\n",
        "\n",
        "#plotTask(pars_tasks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "task = 0 \n",
            "[2.0]     [2.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "class loss = 0.6882435083389282 dist loss = 0.0\n",
            "class loss = 0.7614362835884094 dist loss = 0.0\n",
            "class loss = 0.5008403062820435 dist loss = 0.0\n",
            "class loss = 0.3830077350139618 dist loss = 0.0\n",
            "class loss = 0.37044405937194824 dist loss = 0.0\n",
            "class loss = 0.36728787422180176 dist loss = 0.0\n",
            "class loss = 0.33800315856933594 dist loss = 0.0\n",
            "class loss = 0.32464781403541565 dist loss = 0.0\n",
            "class loss = 0.32672005891799927 dist loss = 0.0\n",
            "class loss = 0.3296315371990204 dist loss = 0.0\n",
            "class loss = 0.3213450610637665 dist loss = 0.0\n",
            "class loss = 0.3168812692165375 dist loss = 0.0\n",
            "class loss = 0.3139455020427704 dist loss = 0.0\n",
            "class loss = 0.31402987241744995 dist loss = 0.0\n",
            "class loss = 0.31973519921302795 dist loss = 0.0\n",
            "class loss = 0.3144184648990631 dist loss = 0.0\n",
            "class loss = 0.30627238750457764 dist loss = 0.0\n",
            "class loss = 0.30653849244117737 dist loss = 0.0\n",
            "class loss = 0.3118932247161865 dist loss = 0.0\n",
            "class loss = 0.30624982714653015 dist loss = 0.0\n",
            "class loss = 0.31135135889053345 dist loss = 0.0\n",
            "class loss = 0.3114585876464844 dist loss = 0.0\n",
            "class loss = 0.3033979535102844 dist loss = 0.0\n",
            "class loss = 0.298210084438324 dist loss = 0.0\n",
            "class loss = 0.29741182923316956 dist loss = 0.0\n",
            "class loss = 0.3062582314014435 dist loss = 0.0\n",
            "class loss = 0.29804688692092896 dist loss = 0.0\n",
            "class loss = 0.2933962047100067 dist loss = 0.0\n",
            "class loss = 0.28736647963523865 dist loss = 0.0\n",
            "class loss = 0.28580719232559204 dist loss = 0.0\n",
            "class loss = 0.3046495020389557 dist loss = 0.0\n",
            "class loss = 0.2915287911891937 dist loss = 0.0\n",
            "class loss = 0.2875465452671051 dist loss = 0.0\n",
            "class loss = 0.3277241885662079 dist loss = 0.0\n",
            "class loss = 0.2821013331413269 dist loss = 0.0\n",
            "class loss = 0.2900901436805725 dist loss = 0.0\n",
            "class loss = 0.2925936281681061 dist loss = 0.0\n",
            "class loss = 0.28226619958877563 dist loss = 0.0\n",
            "class loss = 0.28278324007987976 dist loss = 0.0\n",
            "class loss = 0.3850822448730469 dist loss = 0.0\n",
            "At step  0  and at epoch =  0  the loss is =  0.3850822448730469  and accuracy is =  0.0016\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.3804749548435211 dist loss = 0.0\n",
            "class loss = 0.3469814360141754 dist loss = 0.0\n",
            "class loss = 0.30690011382102966 dist loss = 0.0\n",
            "class loss = 0.29036745429039 dist loss = 0.0\n",
            "class loss = 0.2883531153202057 dist loss = 0.0\n",
            "class loss = 0.2964654564857483 dist loss = 0.0\n",
            "class loss = 0.2852315604686737 dist loss = 0.0\n",
            "class loss = 0.29184168577194214 dist loss = 0.0\n",
            "class loss = 0.2844981253147125 dist loss = 0.0\n",
            "class loss = 0.2883475720882416 dist loss = 0.0\n",
            "class loss = 0.27006781101226807 dist loss = 0.0\n",
            "class loss = 0.29876548051834106 dist loss = 0.0\n",
            "class loss = 0.2828748822212219 dist loss = 0.0\n",
            "class loss = 0.27748140692710876 dist loss = 0.0\n",
            "class loss = 0.29956886172294617 dist loss = 0.0\n",
            "class loss = 0.2941230237483978 dist loss = 0.0\n",
            "class loss = 0.2739267945289612 dist loss = 0.0\n",
            "class loss = 0.2873693108558655 dist loss = 0.0\n",
            "class loss = 0.28142285346984863 dist loss = 0.0\n",
            "class loss = 0.2619417607784271 dist loss = 0.0\n",
            "class loss = 0.2734423279762268 dist loss = 0.0\n",
            "class loss = 0.2833286225795746 dist loss = 0.0\n",
            "class loss = 0.27146658301353455 dist loss = 0.0\n",
            "class loss = 0.27451515197753906 dist loss = 0.0\n",
            "class loss = 0.2667519748210907 dist loss = 0.0\n",
            "class loss = 0.29576897621154785 dist loss = 0.0\n",
            "class loss = 0.2961225211620331 dist loss = 0.0\n",
            "class loss = 0.27034687995910645 dist loss = 0.0\n",
            "class loss = 0.2647552192211151 dist loss = 0.0\n",
            "class loss = 0.2655525505542755 dist loss = 0.0\n",
            "class loss = 0.28610071539878845 dist loss = 0.0\n",
            "class loss = 0.27629438042640686 dist loss = 0.0\n",
            "class loss = 0.2713800072669983 dist loss = 0.0\n",
            "class loss = 0.28119421005249023 dist loss = 0.0\n",
            "class loss = 0.2547714412212372 dist loss = 0.0\n",
            "class loss = 0.2633776366710663 dist loss = 0.0\n",
            "class loss = 0.28764888644218445 dist loss = 0.0\n",
            "class loss = 0.2646597921848297 dist loss = 0.0\n",
            "class loss = 0.273042231798172 dist loss = 0.0\n",
            "class loss = 0.31132927536964417 dist loss = 0.0\n",
            "At step  0  and at epoch =  1  the loss is =  0.31132927536964417  and accuracy is =  0.0102\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.374621719121933 dist loss = 0.0\n",
            "class loss = 0.3232545852661133 dist loss = 0.0\n",
            "class loss = 0.29572898149490356 dist loss = 0.0\n",
            "class loss = 0.27504220604896545 dist loss = 0.0\n",
            "class loss = 0.2644326686859131 dist loss = 0.0\n",
            "class loss = 0.2789844572544098 dist loss = 0.0\n",
            "class loss = 0.2776903510093689 dist loss = 0.0\n",
            "class loss = 0.26630789041519165 dist loss = 0.0\n",
            "class loss = 0.2650812268257141 dist loss = 0.0\n",
            "class loss = 0.26570916175842285 dist loss = 0.0\n",
            "class loss = 0.2502429187297821 dist loss = 0.0\n",
            "class loss = 0.27899158000946045 dist loss = 0.0\n",
            "class loss = 0.25286588072776794 dist loss = 0.0\n",
            "class loss = 0.2512894570827484 dist loss = 0.0\n",
            "class loss = 0.274244487285614 dist loss = 0.0\n",
            "class loss = 0.2742745280265808 dist loss = 0.0\n",
            "class loss = 0.257197767496109 dist loss = 0.0\n",
            "class loss = 0.26673611998558044 dist loss = 0.0\n",
            "class loss = 0.2667030394077301 dist loss = 0.0\n",
            "class loss = 0.24887171387672424 dist loss = 0.0\n",
            "class loss = 0.26981672644615173 dist loss = 0.0\n",
            "class loss = 0.2670214772224426 dist loss = 0.0\n",
            "class loss = 0.2547118663787842 dist loss = 0.0\n",
            "class loss = 0.2555035948753357 dist loss = 0.0\n",
            "class loss = 0.25432977080345154 dist loss = 0.0\n",
            "class loss = 0.2937014698982239 dist loss = 0.0\n",
            "class loss = 0.2714582085609436 dist loss = 0.0\n",
            "class loss = 0.25255832076072693 dist loss = 0.0\n",
            "class loss = 0.25065574049949646 dist loss = 0.0\n",
            "class loss = 0.24351806938648224 dist loss = 0.0\n",
            "class loss = 0.2697971761226654 dist loss = 0.0\n",
            "class loss = 0.25556254386901855 dist loss = 0.0\n",
            "class loss = 0.24811790883541107 dist loss = 0.0\n",
            "class loss = 0.2556631565093994 dist loss = 0.0\n",
            "class loss = 0.24731671810150146 dist loss = 0.0\n",
            "class loss = 0.26130974292755127 dist loss = 0.0\n",
            "class loss = 0.27544066309928894 dist loss = 0.0\n",
            "class loss = 0.250073105096817 dist loss = 0.0\n",
            "class loss = 0.2730948030948639 dist loss = 0.0\n",
            "class loss = 0.26342231035232544 dist loss = 0.0\n",
            "At step  0  and at epoch =  2  the loss is =  0.26342231035232544  and accuracy is =  0.0246\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.3850843906402588 dist loss = 0.0\n",
            "class loss = 0.334444522857666 dist loss = 0.0\n",
            "class loss = 0.2989315390586853 dist loss = 0.0\n",
            "class loss = 0.2670328617095947 dist loss = 0.0\n",
            "class loss = 0.2564142644405365 dist loss = 0.0\n",
            "class loss = 0.2755250632762909 dist loss = 0.0\n",
            "class loss = 0.26357221603393555 dist loss = 0.0\n",
            "class loss = 0.2643165588378906 dist loss = 0.0\n",
            "class loss = 0.2644007205963135 dist loss = 0.0\n",
            "class loss = 0.2571692168712616 dist loss = 0.0\n",
            "class loss = 0.23514442145824432 dist loss = 0.0\n",
            "class loss = 0.26738688349723816 dist loss = 0.0\n",
            "class loss = 0.23960915207862854 dist loss = 0.0\n",
            "class loss = 0.2419995814561844 dist loss = 0.0\n",
            "class loss = 0.26070961356163025 dist loss = 0.0\n",
            "class loss = 0.2556574046611786 dist loss = 0.0\n",
            "class loss = 0.24360130727291107 dist loss = 0.0\n",
            "class loss = 0.24874818325042725 dist loss = 0.0\n",
            "class loss = 0.26407766342163086 dist loss = 0.0\n",
            "class loss = 0.23154346644878387 dist loss = 0.0\n",
            "class loss = 0.25640588998794556 dist loss = 0.0\n",
            "class loss = 0.2586911618709564 dist loss = 0.0\n",
            "class loss = 0.25099238753318787 dist loss = 0.0\n",
            "class loss = 0.24733226001262665 dist loss = 0.0\n",
            "class loss = 0.2279059737920761 dist loss = 0.0\n",
            "class loss = 0.2744959890842438 dist loss = 0.0\n",
            "class loss = 0.24756069481372833 dist loss = 0.0\n",
            "class loss = 0.23536019027233124 dist loss = 0.0\n",
            "class loss = 0.23506371676921844 dist loss = 0.0\n",
            "class loss = 0.22693634033203125 dist loss = 0.0\n",
            "class loss = 0.266544908285141 dist loss = 0.0\n",
            "class loss = 0.24031467735767365 dist loss = 0.0\n",
            "class loss = 0.22615313529968262 dist loss = 0.0\n",
            "class loss = 0.2392197847366333 dist loss = 0.0\n",
            "class loss = 0.2254706174135208 dist loss = 0.0\n",
            "class loss = 0.2281842678785324 dist loss = 0.0\n",
            "class loss = 0.27497297525405884 dist loss = 0.0\n",
            "class loss = 0.23879317939281464 dist loss = 0.0\n",
            "class loss = 0.2657068967819214 dist loss = 0.0\n",
            "class loss = 0.18645396828651428 dist loss = 0.0\n",
            "At step  0  and at epoch =  3  the loss is =  0.18645396828651428  and accuracy is =  0.0422\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.34577780961990356 dist loss = 0.0\n",
            "class loss = 0.29926401376724243 dist loss = 0.0\n",
            "class loss = 0.2692151963710785 dist loss = 0.0\n",
            "class loss = 0.2577340304851532 dist loss = 0.0\n",
            "class loss = 0.238682821393013 dist loss = 0.0\n",
            "class loss = 0.2669869661331177 dist loss = 0.0\n",
            "class loss = 0.24352958798408508 dist loss = 0.0\n",
            "class loss = 0.23955217003822327 dist loss = 0.0\n",
            "class loss = 0.25484633445739746 dist loss = 0.0\n",
            "class loss = 0.24706974625587463 dist loss = 0.0\n",
            "class loss = 0.2289312332868576 dist loss = 0.0\n",
            "class loss = 0.25251659750938416 dist loss = 0.0\n",
            "class loss = 0.22007539868354797 dist loss = 0.0\n",
            "class loss = 0.22553443908691406 dist loss = 0.0\n",
            "class loss = 0.2469598352909088 dist loss = 0.0\n",
            "class loss = 0.23704080283641815 dist loss = 0.0\n",
            "class loss = 0.23682112991809845 dist loss = 0.0\n",
            "class loss = 0.2387760728597641 dist loss = 0.0\n",
            "class loss = 0.24755458533763885 dist loss = 0.0\n",
            "class loss = 0.21236443519592285 dist loss = 0.0\n",
            "class loss = 0.2554037272930145 dist loss = 0.0\n",
            "class loss = 0.25283512473106384 dist loss = 0.0\n",
            "class loss = 0.23656527698040009 dist loss = 0.0\n",
            "class loss = 0.23238150775432587 dist loss = 0.0\n",
            "class loss = 0.2265324592590332 dist loss = 0.0\n",
            "class loss = 0.2485891431570053 dist loss = 0.0\n",
            "class loss = 0.22781868278980255 dist loss = 0.0\n",
            "class loss = 0.21510715782642365 dist loss = 0.0\n",
            "class loss = 0.2123652547597885 dist loss = 0.0\n",
            "class loss = 0.2171248495578766 dist loss = 0.0\n",
            "class loss = 0.2363845854997635 dist loss = 0.0\n",
            "class loss = 0.23056109249591827 dist loss = 0.0\n",
            "class loss = 0.21888089179992676 dist loss = 0.0\n",
            "class loss = 0.22533872723579407 dist loss = 0.0\n",
            "class loss = 0.21431730687618256 dist loss = 0.0\n",
            "class loss = 0.21120691299438477 dist loss = 0.0\n",
            "class loss = 0.23876245319843292 dist loss = 0.0\n",
            "class loss = 0.22603397071361542 dist loss = 0.0\n",
            "class loss = 0.23445084691047668 dist loss = 0.0\n",
            "class loss = 0.15576790273189545 dist loss = 0.0\n",
            "At step  0  and at epoch =  4  the loss is =  0.15576790273189545  and accuracy is =  0.0622\n",
            " running corrects = 1\n",
            " running corrects = 2\n",
            " running corrects = 4\n",
            " running corrects = 5\n",
            " running corrects = 7\n",
            " running corrects = 9\n",
            " running corrects = 11\n",
            " running corrects = 11\n",
            "Validation Loss: 0.9229353070259094 Validation Accuracy : 0.011\n",
            "task = 10 \n",
            "[2.0]     [2.0]\n",
            "class loss = 0.4050873816013336 dist loss = 0.12845182418823242\n",
            "class loss = 0.17576108872890472 dist loss = 0.1363215446472168\n",
            "class loss = 0.16486209630966187 dist loss = 0.13060438632965088\n",
            "class loss = 0.16295135021209717 dist loss = 0.1255146563053131\n",
            "class loss = 0.16591830551624298 dist loss = 0.1321135014295578\n",
            "class loss = 0.16328738629817963 dist loss = 0.129337340593338\n",
            "class loss = 0.15740858018398285 dist loss = 0.1412597894668579\n",
            "class loss = 0.1614750176668167 dist loss = 0.13542930781841278\n",
            "class loss = 0.1611112803220749 dist loss = 0.1284383088350296\n",
            "class loss = 0.16333642601966858 dist loss = 0.13505055010318756\n",
            "class loss = 0.15735797584056854 dist loss = 0.13794095814228058\n",
            "class loss = 0.15816719830036163 dist loss = 0.1418381929397583\n",
            "class loss = 0.15363958477973938 dist loss = 0.12712900340557098\n",
            "class loss = 0.15262949466705322 dist loss = 0.13387718796730042\n",
            "class loss = 0.15255048871040344 dist loss = 0.13895466923713684\n",
            "class loss = 0.14921119809150696 dist loss = 0.12825994193553925\n",
            "class loss = 0.15407812595367432 dist loss = 0.12923744320869446\n",
            "class loss = 0.1544952690601349 dist loss = 0.1257420778274536\n",
            "class loss = 0.15230900049209595 dist loss = 0.13435155153274536\n",
            "class loss = 0.14672353863716125 dist loss = 0.13004854321479797\n",
            "class loss = 0.148220956325531 dist loss = 0.138738751411438\n",
            "class loss = 0.1521693915128708 dist loss = 0.1390644758939743\n",
            "class loss = 0.1497648060321808 dist loss = 0.13085900247097015\n",
            "class loss = 0.15000692009925842 dist loss = 0.1321045309305191\n",
            "class loss = 0.14717744290828705 dist loss = 0.13564608991146088\n",
            "class loss = 0.1446193903684616 dist loss = 0.13600876927375793\n",
            "class loss = 0.14503656327724457 dist loss = 0.136796772480011\n",
            "class loss = 0.14681068062782288 dist loss = 0.13565874099731445\n",
            "class loss = 0.14509820938110352 dist loss = 0.13087455928325653\n",
            "class loss = 0.14794859290122986 dist loss = 0.1342080682516098\n",
            "class loss = 0.14159490168094635 dist loss = 0.13223259150981903\n",
            "class loss = 0.14829491078853607 dist loss = 0.13506817817687988\n",
            "class loss = 0.14235146343708038 dist loss = 0.1404692530632019\n",
            "class loss = 0.14150729775428772 dist loss = 0.13802388310432434\n",
            "class loss = 0.14475475251674652 dist loss = 0.13396184146404266\n",
            "class loss = 0.1404213160276413 dist loss = 0.13335928320884705\n",
            "class loss = 0.14533868432044983 dist loss = 0.1277662068605423\n",
            "class loss = 0.14321351051330566 dist loss = 0.1329534351825714\n",
            "class loss = 0.13855066895484924 dist loss = 0.13193129003047943\n",
            "class loss = 0.14358440041542053 dist loss = 0.12178289145231247\n",
            "At step  10  and at epoch =  0  the loss is =  0.2653672993183136  and accuracy is =  0.0014\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.1889760047197342 dist loss = 0.13628721237182617\n",
            "class loss = 0.14729565382003784 dist loss = 0.14198283851146698\n",
            "class loss = 0.1415081024169922 dist loss = 0.13277123868465424\n",
            "class loss = 0.14011891186237335 dist loss = 0.1271544098854065\n",
            "class loss = 0.14130841195583344 dist loss = 0.13337068259716034\n",
            "class loss = 0.13956372439861298 dist loss = 0.13116928935050964\n",
            "class loss = 0.13694284856319427 dist loss = 0.1435808390378952\n",
            "class loss = 0.14371345937252045 dist loss = 0.13896821439266205\n",
            "class loss = 0.14345338940620422 dist loss = 0.12977592647075653\n",
            "class loss = 0.14513052999973297 dist loss = 0.13658113777637482\n",
            "class loss = 0.14273399114608765 dist loss = 0.1397760510444641\n",
            "class loss = 0.14292870461940765 dist loss = 0.14362302422523499\n",
            "class loss = 0.13574735820293427 dist loss = 0.12903594970703125\n",
            "class loss = 0.13429604470729828 dist loss = 0.1362846940755844\n",
            "class loss = 0.1361827403306961 dist loss = 0.14199867844581604\n",
            "class loss = 0.12986962497234344 dist loss = 0.13050882518291473\n",
            "class loss = 0.13784174621105194 dist loss = 0.13089261949062347\n",
            "class loss = 0.14031168818473816 dist loss = 0.1281266063451767\n",
            "class loss = 0.136844664812088 dist loss = 0.13571996986865997\n",
            "class loss = 0.1313776671886444 dist loss = 0.13163557648658752\n",
            "class loss = 0.1344936639070511 dist loss = 0.14051704108715057\n",
            "class loss = 0.1391536295413971 dist loss = 0.14011439681053162\n",
            "class loss = 0.13777802884578705 dist loss = 0.13186819851398468\n",
            "class loss = 0.13636551797389984 dist loss = 0.13348305225372314\n",
            "class loss = 0.13532602787017822 dist loss = 0.1364990919828415\n",
            "class loss = 0.12802432477474213 dist loss = 0.13726627826690674\n",
            "class loss = 0.13032905757427216 dist loss = 0.13732372224330902\n",
            "class loss = 0.13322393596172333 dist loss = 0.13733318448066711\n",
            "class loss = 0.13443557918071747 dist loss = 0.13316906988620758\n",
            "class loss = 0.13699232041835785 dist loss = 0.13587714731693268\n",
            "class loss = 0.12920090556144714 dist loss = 0.13449466228485107\n",
            "class loss = 0.13677899539470673 dist loss = 0.1372263878583908\n",
            "class loss = 0.1277349293231964 dist loss = 0.14379693567752838\n",
            "class loss = 0.12903426587581635 dist loss = 0.14029733836650848\n",
            "class loss = 0.13685563206672668 dist loss = 0.13748787343502045\n",
            "class loss = 0.12689559161663055 dist loss = 0.13547150790691376\n",
            "class loss = 0.13521887362003326 dist loss = 0.1287926882505417\n",
            "class loss = 0.13140557706356049 dist loss = 0.13442058861255646\n",
            "class loss = 0.12533576786518097 dist loss = 0.1331661492586136\n",
            "class loss = 0.1134764775633812 dist loss = 0.1243038922548294\n",
            "At step  10  and at epoch =  1  the loss is =  0.23778036236763  and accuracy is =  0.0014\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.16771116852760315 dist loss = 0.13519711792469025\n",
            "class loss = 0.1379687637090683 dist loss = 0.14154478907585144\n",
            "class loss = 0.12983761727809906 dist loss = 0.13362851738929749\n",
            "class loss = 0.13250544667243958 dist loss = 0.1282261162996292\n",
            "class loss = 0.12786531448364258 dist loss = 0.1344650238752365\n",
            "class loss = 0.12920191884040833 dist loss = 0.13251133263111115\n",
            "class loss = 0.13091452419757843 dist loss = 0.14512041211128235\n",
            "class loss = 0.13459332287311554 dist loss = 0.1392325907945633\n",
            "class loss = 0.13423466682434082 dist loss = 0.13084058463573456\n",
            "class loss = 0.13540486991405487 dist loss = 0.13721603155136108\n",
            "class loss = 0.13450424373149872 dist loss = 0.14053094387054443\n",
            "class loss = 0.1330997496843338 dist loss = 0.1447162628173828\n",
            "class loss = 0.1276574581861496 dist loss = 0.12994061410427094\n",
            "class loss = 0.12438894808292389 dist loss = 0.13791970908641815\n",
            "class loss = 0.12858843803405762 dist loss = 0.14302659034729004\n",
            "class loss = 0.11947204172611237 dist loss = 0.13199806213378906\n",
            "class loss = 0.1276552528142929 dist loss = 0.1323530375957489\n",
            "class loss = 0.13229922950267792 dist loss = 0.12946496903896332\n",
            "class loss = 0.12850642204284668 dist loss = 0.13696599006652832\n",
            "class loss = 0.1227932721376419 dist loss = 0.1324910968542099\n",
            "class loss = 0.1258981078863144 dist loss = 0.14181214570999146\n",
            "class loss = 0.13112370669841766 dist loss = 0.14056845009326935\n",
            "class loss = 0.12842850387096405 dist loss = 0.1329905241727829\n",
            "class loss = 0.1279023140668869 dist loss = 0.1345595121383667\n",
            "class loss = 0.12676863372325897 dist loss = 0.13737896084785461\n",
            "class loss = 0.11777777969837189 dist loss = 0.13872425258159637\n",
            "class loss = 0.11945085972547531 dist loss = 0.13899675011634827\n",
            "class loss = 0.12312614917755127 dist loss = 0.13880641758441925\n",
            "class loss = 0.12702180445194244 dist loss = 0.13396230340003967\n",
            "class loss = 0.1281580626964569 dist loss = 0.13719794154167175\n",
            "class loss = 0.11908111721277237 dist loss = 0.1359311193227768\n",
            "class loss = 0.129359170794487 dist loss = 0.1386854648590088\n",
            "class loss = 0.12013497203588486 dist loss = 0.14661681652069092\n",
            "class loss = 0.12109172344207764 dist loss = 0.1414676457643509\n",
            "class loss = 0.13008911907672882 dist loss = 0.13800965249538422\n",
            "class loss = 0.11836166679859161 dist loss = 0.1359173059463501\n",
            "class loss = 0.12804855406284332 dist loss = 0.12991861999034882\n",
            "class loss = 0.12289046496152878 dist loss = 0.13504067063331604\n",
            "class loss = 0.11619763821363449 dist loss = 0.13439789414405823\n",
            "class loss = 0.09293649345636368 dist loss = 0.12264709919691086\n",
            "At step  10  and at epoch =  2  the loss is =  0.21558359265327454  and accuracy is =  0.0044\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.15166831016540527 dist loss = 0.13405947387218475\n",
            "class loss = 0.13230623304843903 dist loss = 0.14140819013118744\n",
            "class loss = 0.12434990704059601 dist loss = 0.13442642986774445\n",
            "class loss = 0.12783733010292053 dist loss = 0.12966610491275787\n",
            "class loss = 0.1197906881570816 dist loss = 0.1357705444097519\n",
            "class loss = 0.12164248526096344 dist loss = 0.13384948670864105\n",
            "class loss = 0.12482219934463501 dist loss = 0.14506445825099945\n",
            "class loss = 0.12607751786708832 dist loss = 0.1393846720457077\n",
            "class loss = 0.1277736872434616 dist loss = 0.13156549632549286\n",
            "class loss = 0.12992613017559052 dist loss = 0.1377241611480713\n",
            "class loss = 0.1285545825958252 dist loss = 0.14104612171649933\n",
            "class loss = 0.12578430771827698 dist loss = 0.14535172283649445\n",
            "class loss = 0.12205322831869125 dist loss = 0.13065846264362335\n",
            "class loss = 0.1178668960928917 dist loss = 0.13860046863555908\n",
            "class loss = 0.1221243143081665 dist loss = 0.14325693249702454\n",
            "class loss = 0.11300455778837204 dist loss = 0.13295909762382507\n",
            "class loss = 0.12075573205947876 dist loss = 0.13281749188899994\n",
            "class loss = 0.12564341723918915 dist loss = 0.13018174469470978\n",
            "class loss = 0.12185712903738022 dist loss = 0.13762734830379486\n",
            "class loss = 0.1167718768119812 dist loss = 0.1333421915769577\n",
            "class loss = 0.11919810622930527 dist loss = 0.14242970943450928\n",
            "class loss = 0.12439323961734772 dist loss = 0.14118719100952148\n",
            "class loss = 0.12082512676715851 dist loss = 0.13367529213428497\n",
            "class loss = 0.12067630141973495 dist loss = 0.1352066993713379\n",
            "class loss = 0.11915188282728195 dist loss = 0.13830602169036865\n",
            "class loss = 0.11156012862920761 dist loss = 0.13937361538410187\n",
            "class loss = 0.11174030601978302 dist loss = 0.13937842845916748\n",
            "class loss = 0.11459537595510483 dist loss = 0.1391228288412094\n",
            "class loss = 0.12031730264425278 dist loss = 0.13484352827072144\n",
            "class loss = 0.1216479167342186 dist loss = 0.13824903964996338\n",
            "class loss = 0.10991945117712021 dist loss = 0.1363762617111206\n",
            "class loss = 0.12416081875562668 dist loss = 0.1391998827457428\n",
            "class loss = 0.1132618710398674 dist loss = 0.1458437591791153\n",
            "class loss = 0.11286646127700806 dist loss = 0.14097538590431213\n",
            "class loss = 0.12507952749729156 dist loss = 0.13815830647945404\n",
            "class loss = 0.11317794770002365 dist loss = 0.1358247995376587\n",
            "class loss = 0.12093377113342285 dist loss = 0.13071131706237793\n",
            "class loss = 0.1166418120265007 dist loss = 0.13549190759658813\n",
            "class loss = 0.10865520685911179 dist loss = 0.13508884608745575\n",
            "class loss = 0.07502003759145737 dist loss = 0.12266121059656143\n",
            "At step  10  and at epoch =  3  the loss is =  0.1976812481880188  and accuracy is =  0.018\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.1388891041278839 dist loss = 0.134766086935997\n",
            "class loss = 0.128508523106575 dist loss = 0.14214295148849487\n",
            "class loss = 0.11959927529096603 dist loss = 0.13585303723812103\n",
            "class loss = 0.12275978177785873 dist loss = 0.13099925220012665\n",
            "class loss = 0.11511778831481934 dist loss = 0.13609063625335693\n",
            "class loss = 0.11639472097158432 dist loss = 0.13489459455013275\n",
            "class loss = 0.11923040449619293 dist loss = 0.14506284892559052\n",
            "class loss = 0.12022852897644043 dist loss = 0.13998210430145264\n",
            "class loss = 0.12162798643112183 dist loss = 0.13197527825832367\n",
            "class loss = 0.12554942071437836 dist loss = 0.13834671676158905\n",
            "class loss = 0.12314889580011368 dist loss = 0.14143340289592743\n",
            "class loss = 0.11987411975860596 dist loss = 0.14578834176063538\n",
            "class loss = 0.11671330034732819 dist loss = 0.1314365565776825\n",
            "class loss = 0.11222181469202042 dist loss = 0.13931138813495636\n",
            "class loss = 0.11708398163318634 dist loss = 0.14364086091518402\n",
            "class loss = 0.10775911808013916 dist loss = 0.13372652232646942\n",
            "class loss = 0.11478947848081589 dist loss = 0.13288870453834534\n",
            "class loss = 0.11964382231235504 dist loss = 0.1310528814792633\n",
            "class loss = 0.11763093620538712 dist loss = 0.13743209838867188\n",
            "class loss = 0.113296739757061 dist loss = 0.13386714458465576\n",
            "class loss = 0.11307170242071152 dist loss = 0.14252233505249023\n",
            "class loss = 0.1171736866235733 dist loss = 0.14149542152881622\n",
            "class loss = 0.11573579162359238 dist loss = 0.13390548527240753\n",
            "class loss = 0.11454601585865021 dist loss = 0.135562926530838\n",
            "class loss = 0.11283013969659805 dist loss = 0.13839831948280334\n",
            "class loss = 0.10616078227758408 dist loss = 0.1399565190076828\n",
            "class loss = 0.10564815998077393 dist loss = 0.1395174264907837\n",
            "class loss = 0.10825979709625244 dist loss = 0.13929374516010284\n",
            "class loss = 0.1144021674990654 dist loss = 0.13607878983020782\n",
            "class loss = 0.1175224557518959 dist loss = 0.13867098093032837\n",
            "class loss = 0.10393287241458893 dist loss = 0.13712793588638306\n",
            "class loss = 0.12028437107801437 dist loss = 0.139762744307518\n",
            "class loss = 0.10799937695264816 dist loss = 0.1456729620695114\n",
            "class loss = 0.10610959678888321 dist loss = 0.1410009115934372\n",
            "class loss = 0.11974616348743439 dist loss = 0.13778021931648254\n",
            "class loss = 0.10660950094461441 dist loss = 0.13620389997959137\n",
            "class loss = 0.11543560028076172 dist loss = 0.13107693195343018\n",
            "class loss = 0.11136753857135773 dist loss = 0.135812908411026\n",
            "class loss = 0.10206997394561768 dist loss = 0.13504640758037567\n",
            "class loss = 0.05866587907075882 dist loss = 0.12364959716796875\n",
            "At step  10  and at epoch =  4  the loss is =  0.18231546878814697  and accuracy is =  0.0332\n",
            " running corrects = 4\n",
            " running corrects = 13\n",
            " running corrects = 14\n",
            " running corrects = 20\n",
            " running corrects = 25\n",
            " running corrects = 27\n",
            " running corrects = 39\n",
            " running corrects = 46\n",
            " running corrects = 59\n",
            " running corrects = 79\n",
            " running corrects = 96\n",
            " running corrects = 108\n",
            " running corrects = 123\n",
            " running corrects = 139\n",
            " running corrects = 152\n",
            " running corrects = 166\n",
            "Validation Loss: 0.6317236423492432 Validation Accuracy : 0.083\n",
            "task = 20 \n",
            "[2.0]     [2.0]\n",
            "class loss = 0.26994505524635315 dist loss = 0.16646718978881836\n",
            "class loss = 0.10642699897289276 dist loss = 0.1721055805683136\n",
            "class loss = 0.10496710985898972 dist loss = 0.17140832543373108\n",
            "class loss = 0.10349179804325104 dist loss = 0.17464898526668549\n",
            "class loss = 0.10188030451536179 dist loss = 0.17953895032405853\n",
            "class loss = 0.1017269715666771 dist loss = 0.17378205060958862\n",
            "class loss = 0.09906452894210815 dist loss = 0.16982899606227875\n",
            "class loss = 0.09607703983783722 dist loss = 0.16661828756332397\n",
            "class loss = 0.09463298320770264 dist loss = 0.16871196031570435\n",
            "class loss = 0.09464101493358612 dist loss = 0.16558851301670074\n",
            "class loss = 0.09238477051258087 dist loss = 0.16950327157974243\n",
            "class loss = 0.09491058439016342 dist loss = 0.16616201400756836\n",
            "class loss = 0.09697829931974411 dist loss = 0.17069344222545624\n",
            "class loss = 0.09519698470830917 dist loss = 0.16701292991638184\n",
            "class loss = 0.09889205545186996 dist loss = 0.17250338196754456\n",
            "class loss = 0.09469956159591675 dist loss = 0.1745399832725525\n",
            "class loss = 0.09321589767932892 dist loss = 0.16804645955562592\n",
            "class loss = 0.09692593663930893 dist loss = 0.17630694806575775\n",
            "class loss = 0.0955185741186142 dist loss = 0.1759161651134491\n",
            "class loss = 0.09302976727485657 dist loss = 0.1771073341369629\n",
            "class loss = 0.09265199303627014 dist loss = 0.1791360229253769\n",
            "class loss = 0.09177305549383163 dist loss = 0.1780862659215927\n",
            "class loss = 0.08658420294523239 dist loss = 0.17116259038448334\n",
            "class loss = 0.09464836120605469 dist loss = 0.17795458436012268\n",
            "class loss = 0.09629081934690475 dist loss = 0.17404453456401825\n",
            "class loss = 0.09557780623435974 dist loss = 0.17299841344356537\n",
            "class loss = 0.0942690372467041 dist loss = 0.17185714840888977\n",
            "class loss = 0.08909516781568527 dist loss = 0.16808205842971802\n",
            "class loss = 0.09222692251205444 dist loss = 0.16799628734588623\n",
            "class loss = 0.09119497239589691 dist loss = 0.1719588041305542\n",
            "class loss = 0.08900682628154755 dist loss = 0.17769645154476166\n",
            "class loss = 0.09543676674365997 dist loss = 0.17085662484169006\n",
            "class loss = 0.08710392564535141 dist loss = 0.17216616868972778\n",
            "class loss = 0.09213294088840485 dist loss = 0.17469197511672974\n",
            "class loss = 0.09021206200122833 dist loss = 0.1702437400817871\n",
            "class loss = 0.0890415757894516 dist loss = 0.16866229474544525\n",
            "class loss = 0.09117737412452698 dist loss = 0.17165051400661469\n",
            "class loss = 0.08951675891876221 dist loss = 0.18205472826957703\n",
            "class loss = 0.0919346734881401 dist loss = 0.17178229987621307\n",
            "class loss = 0.08567982912063599 dist loss = 0.1601949781179428\n",
            "At step  20  and at epoch =  0  the loss is =  0.2458748072385788  and accuracy is =  0.0004\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.09699888527393341 dist loss = 0.19745825231075287\n",
            "class loss = 0.08808194100856781 dist loss = 0.18319933116436005\n",
            "class loss = 0.0972658023238182 dist loss = 0.1758091300725937\n",
            "class loss = 0.09050054848194122 dist loss = 0.17693129181861877\n",
            "class loss = 0.08903487026691437 dist loss = 0.1787755787372589\n",
            "class loss = 0.0867743045091629 dist loss = 0.1740606725215912\n",
            "class loss = 0.08774346113204956 dist loss = 0.16889896988868713\n",
            "class loss = 0.083920419216156 dist loss = 0.16738447546958923\n",
            "class loss = 0.08272816240787506 dist loss = 0.169220969080925\n",
            "class loss = 0.08645644783973694 dist loss = 0.16652236878871918\n",
            "class loss = 0.08210036158561707 dist loss = 0.17064134776592255\n",
            "class loss = 0.08560716360807419 dist loss = 0.16651785373687744\n",
            "class loss = 0.08763545751571655 dist loss = 0.17202743887901306\n",
            "class loss = 0.08637654036283493 dist loss = 0.1680935025215149\n",
            "class loss = 0.09127075970172882 dist loss = 0.17351356148719788\n",
            "class loss = 0.08818423002958298 dist loss = 0.175324946641922\n",
            "class loss = 0.08315867930650711 dist loss = 0.16828858852386475\n",
            "class loss = 0.08909276127815247 dist loss = 0.17587894201278687\n",
            "class loss = 0.08812761306762695 dist loss = 0.1745968461036682\n",
            "class loss = 0.08615308254957199 dist loss = 0.17487961053848267\n",
            "class loss = 0.08575430512428284 dist loss = 0.17546622455120087\n",
            "class loss = 0.08357878774404526 dist loss = 0.1761736273765564\n",
            "class loss = 0.07894417643547058 dist loss = 0.16810324788093567\n",
            "class loss = 0.08451580256223679 dist loss = 0.17735670506954193\n",
            "class loss = 0.08877736330032349 dist loss = 0.17070655524730682\n",
            "class loss = 0.08727100491523743 dist loss = 0.16892212629318237\n",
            "class loss = 0.08789458870887756 dist loss = 0.17104975879192352\n",
            "class loss = 0.0817938894033432 dist loss = 0.1688637137413025\n",
            "class loss = 0.0856284499168396 dist loss = 0.16934923827648163\n",
            "class loss = 0.08332838118076324 dist loss = 0.1727859377861023\n",
            "class loss = 0.08156139403581619 dist loss = 0.17818085849285126\n",
            "class loss = 0.09030979126691818 dist loss = 0.17163512110710144\n",
            "class loss = 0.08001871407032013 dist loss = 0.1734558641910553\n",
            "class loss = 0.08634953200817108 dist loss = 0.17550978064537048\n",
            "class loss = 0.08411329984664917 dist loss = 0.17149358987808228\n",
            "class loss = 0.08180131018161774 dist loss = 0.1692383885383606\n",
            "class loss = 0.08464699238538742 dist loss = 0.17151938378810883\n",
            "class loss = 0.08306276798248291 dist loss = 0.18187163770198822\n",
            "class loss = 0.08615562319755554 dist loss = 0.17294403910636902\n",
            "class loss = 0.06446336209774017 dist loss = 0.1599595546722412\n",
            "At step  20  and at epoch =  1  the loss is =  0.22442291676998138  and accuracy is =  0.0006\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.08717839419841766 dist loss = 0.1729227751493454\n",
            "class loss = 0.07980388402938843 dist loss = 0.1756490170955658\n",
            "class loss = 0.09085538983345032 dist loss = 0.17325490713119507\n",
            "class loss = 0.08616028726100922 dist loss = 0.1780637502670288\n",
            "class loss = 0.08382609486579895 dist loss = 0.18042060732841492\n",
            "class loss = 0.0815301388502121 dist loss = 0.17435352504253387\n",
            "class loss = 0.08261069655418396 dist loss = 0.17007598280906677\n",
            "class loss = 0.07792750000953674 dist loss = 0.16833847761154175\n",
            "class loss = 0.07677183300256729 dist loss = 0.1694972962141037\n",
            "class loss = 0.0820956602692604 dist loss = 0.16750118136405945\n",
            "class loss = 0.07774382084608078 dist loss = 0.17153923213481903\n",
            "class loss = 0.08028718829154968 dist loss = 0.1675245612859726\n",
            "class loss = 0.08240067958831787 dist loss = 0.17298030853271484\n",
            "class loss = 0.08153264224529266 dist loss = 0.16847847402095795\n",
            "class loss = 0.08670443296432495 dist loss = 0.1741459220647812\n",
            "class loss = 0.08414151519536972 dist loss = 0.17564231157302856\n",
            "class loss = 0.0769193023443222 dist loss = 0.16913174092769623\n",
            "class loss = 0.08444362133741379 dist loss = 0.1763129085302353\n",
            "class loss = 0.08334395289421082 dist loss = 0.17486999928951263\n",
            "class loss = 0.08207002282142639 dist loss = 0.17492817342281342\n",
            "class loss = 0.08038173615932465 dist loss = 0.1757684201002121\n",
            "class loss = 0.07909978926181793 dist loss = 0.17680510878562927\n",
            "class loss = 0.07301446795463562 dist loss = 0.16861191391944885\n",
            "class loss = 0.07908362150192261 dist loss = 0.1776382327079773\n",
            "class loss = 0.08440664410591125 dist loss = 0.17062626779079437\n",
            "class loss = 0.08147121965885162 dist loss = 0.16976690292358398\n",
            "class loss = 0.083001509308815 dist loss = 0.17156273126602173\n",
            "class loss = 0.0767655223608017 dist loss = 0.16959427297115326\n",
            "class loss = 0.0804225355386734 dist loss = 0.16983729600906372\n",
            "class loss = 0.07775138318538666 dist loss = 0.17288444936275482\n",
            "class loss = 0.0757615938782692 dist loss = 0.17900660634040833\n",
            "class loss = 0.08605451881885529 dist loss = 0.1724679172039032\n",
            "class loss = 0.07507675141096115 dist loss = 0.17439042031764984\n",
            "class loss = 0.08188879489898682 dist loss = 0.17653654515743256\n",
            "class loss = 0.08058556169271469 dist loss = 0.17164939641952515\n",
            "class loss = 0.07692710310220718 dist loss = 0.17016488313674927\n",
            "class loss = 0.08054710924625397 dist loss = 0.17220661044120789\n",
            "class loss = 0.07821402698755264 dist loss = 0.18304023146629333\n",
            "class loss = 0.08136148750782013 dist loss = 0.17329812049865723\n",
            "class loss = 0.0529789924621582 dist loss = 0.1598723828792572\n",
            "At step  20  and at epoch =  2  the loss is =  0.2128513753414154  and accuracy is =  0.0042\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.08201777935028076 dist loss = 0.1713244765996933\n",
            "class loss = 0.07450884580612183 dist loss = 0.17537640035152435\n",
            "class loss = 0.08628025650978088 dist loss = 0.1741981953382492\n",
            "class loss = 0.08246183395385742 dist loss = 0.1793728917837143\n",
            "class loss = 0.0792677104473114 dist loss = 0.18127751350402832\n",
            "class loss = 0.07730184495449066 dist loss = 0.17522959411144257\n",
            "class loss = 0.07930304110050201 dist loss = 0.1713009476661682\n",
            "class loss = 0.07403038442134857 dist loss = 0.16881898045539856\n",
            "class loss = 0.07229529321193695 dist loss = 0.1700873225927353\n",
            "class loss = 0.07836095988750458 dist loss = 0.1680326759815216\n",
            "class loss = 0.07433442771434784 dist loss = 0.17241863906383514\n",
            "class loss = 0.0761079341173172 dist loss = 0.16823965311050415\n",
            "class loss = 0.07861848175525665 dist loss = 0.1738699972629547\n",
            "class loss = 0.07738763839006424 dist loss = 0.16882000863552094\n",
            "class loss = 0.08275970071554184 dist loss = 0.17526856064796448\n",
            "class loss = 0.08147789537906647 dist loss = 0.17611974477767944\n",
            "class loss = 0.07258661836385727 dist loss = 0.16952572762966156\n",
            "class loss = 0.08070339262485504 dist loss = 0.1768375039100647\n",
            "class loss = 0.08034203946590424 dist loss = 0.17540691792964935\n",
            "class loss = 0.07978992909193039 dist loss = 0.17566202580928802\n",
            "class loss = 0.07646148651838303 dist loss = 0.1766408085823059\n",
            "class loss = 0.07602156698703766 dist loss = 0.17722803354263306\n",
            "class loss = 0.06884108483791351 dist loss = 0.1690293401479721\n",
            "class loss = 0.0746316909790039 dist loss = 0.17837423086166382\n",
            "class loss = 0.08112774044275284 dist loss = 0.17168942093849182\n",
            "class loss = 0.07712408900260925 dist loss = 0.17057564854621887\n",
            "class loss = 0.07910886406898499 dist loss = 0.17215512692928314\n",
            "class loss = 0.07326898723840714 dist loss = 0.16995111107826233\n",
            "class loss = 0.07619526982307434 dist loss = 0.17033296823501587\n",
            "class loss = 0.07321299612522125 dist loss = 0.17338603734970093\n",
            "class loss = 0.07130024582147598 dist loss = 0.17944377660751343\n",
            "class loss = 0.08221371471881866 dist loss = 0.17298296093940735\n",
            "class loss = 0.07091689109802246 dist loss = 0.17513445019721985\n",
            "class loss = 0.07859620451927185 dist loss = 0.17666122317314148\n",
            "class loss = 0.07712288200855255 dist loss = 0.17280080914497375\n",
            "class loss = 0.0728023573756218 dist loss = 0.17050275206565857\n",
            "class loss = 0.07641319185495377 dist loss = 0.17208342254161835\n",
            "class loss = 0.07404111325740814 dist loss = 0.18285688757896423\n",
            "class loss = 0.07755252718925476 dist loss = 0.1742134392261505\n",
            "class loss = 0.04354645311832428 dist loss = 0.16179251670837402\n",
            "At step  20  and at epoch =  3  the loss is =  0.2053389698266983  and accuracy is =  0.0154\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.07476872205734253 dist loss = 0.17451301217079163\n",
            "class loss = 0.06994540244340897 dist loss = 0.17628739774227142\n",
            "class loss = 0.08044271916151047 dist loss = 0.17332711815834045\n",
            "class loss = 0.07832217216491699 dist loss = 0.17773555219173431\n",
            "class loss = 0.07594700157642365 dist loss = 0.18048153817653656\n",
            "class loss = 0.07458034157752991 dist loss = 0.1754576414823532\n",
            "class loss = 0.0763019323348999 dist loss = 0.17244940996170044\n",
            "class loss = 0.07056976854801178 dist loss = 0.1695127934217453\n",
            "class loss = 0.0680265799164772 dist loss = 0.1710425466299057\n",
            "class loss = 0.07502257823944092 dist loss = 0.1681390255689621\n",
            "class loss = 0.07153503596782684 dist loss = 0.17331074178218842\n",
            "class loss = 0.07260101288557053 dist loss = 0.16913698613643646\n",
            "class loss = 0.07517121732234955 dist loss = 0.17420583963394165\n",
            "class loss = 0.07316216081380844 dist loss = 0.16955572366714478\n",
            "class loss = 0.07929010689258575 dist loss = 0.17622560262680054\n",
            "class loss = 0.07872562855482101 dist loss = 0.17672640085220337\n",
            "class loss = 0.06905952095985413 dist loss = 0.16984479129314423\n",
            "class loss = 0.07751099765300751 dist loss = 0.1773047298192978\n",
            "class loss = 0.0776938870549202 dist loss = 0.17595815658569336\n",
            "class loss = 0.07781285047531128 dist loss = 0.17637982964515686\n",
            "class loss = 0.07282225787639618 dist loss = 0.177164226770401\n",
            "class loss = 0.07278980314731598 dist loss = 0.17776337265968323\n",
            "class loss = 0.06541746109724045 dist loss = 0.16953927278518677\n",
            "class loss = 0.07055888324975967 dist loss = 0.1788257658481598\n",
            "class loss = 0.0771506205201149 dist loss = 0.17187657952308655\n",
            "class loss = 0.07391628623008728 dist loss = 0.17041316628456116\n",
            "class loss = 0.0773184671998024 dist loss = 0.1737123727798462\n",
            "class loss = 0.0705331340432167 dist loss = 0.17177078127861023\n",
            "class loss = 0.07315855473279953 dist loss = 0.17166930437088013\n",
            "class loss = 0.06960165500640869 dist loss = 0.1747424453496933\n",
            "class loss = 0.06818852573633194 dist loss = 0.1800023764371872\n",
            "class loss = 0.07830099016427994 dist loss = 0.1733427494764328\n",
            "class loss = 0.06778265535831451 dist loss = 0.17544423043727875\n",
            "class loss = 0.0754345953464508 dist loss = 0.17690980434417725\n",
            "class loss = 0.07408298552036285 dist loss = 0.1733938306570053\n",
            "class loss = 0.0690671056509018 dist loss = 0.17148064076900482\n",
            "class loss = 0.0732479989528656 dist loss = 0.17265135049819946\n",
            "class loss = 0.07011374831199646 dist loss = 0.18358758091926575\n",
            "class loss = 0.0739210844039917 dist loss = 0.17457422614097595\n",
            "class loss = 0.038775987923145294 dist loss = 0.1615717113018036\n",
            "At step  20  and at epoch =  4  the loss is =  0.2003476917743683  and accuracy is =  0.0346\n",
            " running corrects = 2\n",
            " running corrects = 7\n",
            " running corrects = 8\n",
            " running corrects = 12\n",
            " running corrects = 17\n",
            " running corrects = 20\n",
            " running corrects = 26\n",
            " running corrects = 32\n",
            " running corrects = 33\n",
            " running corrects = 37\n",
            " running corrects = 37\n",
            " running corrects = 40\n",
            " running corrects = 45\n",
            " running corrects = 46\n",
            " running corrects = 48\n",
            " running corrects = 50\n",
            " running corrects = 58\n",
            " running corrects = 67\n",
            " running corrects = 75\n",
            " running corrects = 82\n",
            " running corrects = 92\n",
            " running corrects = 110\n",
            " running corrects = 117\n",
            " running corrects = 126\n",
            "Validation Loss: 0.5637931823730469 Validation Accuracy : 0.042\n",
            "task = 30 \n",
            "[2.0]     [2.0]\n",
            "class loss = 0.18711549043655396 dist loss = 0.18007005751132965\n",
            "class loss = 0.08267106115818024 dist loss = 0.17712023854255676\n",
            "class loss = 0.07759072631597519 dist loss = 0.17651820182800293\n",
            "class loss = 0.07716617733240128 dist loss = 0.17874422669410706\n",
            "class loss = 0.07679618149995804 dist loss = 0.1763300895690918\n",
            "class loss = 0.07485391199588776 dist loss = 0.17900849878787994\n",
            "class loss = 0.07461941242218018 dist loss = 0.18398763239383698\n",
            "class loss = 0.07195965200662613 dist loss = 0.17978432774543762\n",
            "class loss = 0.0735538974404335 dist loss = 0.1785968393087387\n",
            "class loss = 0.07193148136138916 dist loss = 0.17646510899066925\n",
            "class loss = 0.07152646034955978 dist loss = 0.17938584089279175\n",
            "class loss = 0.07272865623235703 dist loss = 0.18153274059295654\n",
            "class loss = 0.07139735668897629 dist loss = 0.17837965488433838\n",
            "class loss = 0.07171260565519333 dist loss = 0.1818975806236267\n",
            "class loss = 0.07224898785352707 dist loss = 0.18346160650253296\n",
            "class loss = 0.06903420388698578 dist loss = 0.18167437613010406\n",
            "class loss = 0.06739182770252228 dist loss = 0.17658644914627075\n",
            "class loss = 0.07000448554754257 dist loss = 0.17767010629177094\n",
            "class loss = 0.06671810895204544 dist loss = 0.18026283383369446\n",
            "class loss = 0.06876161694526672 dist loss = 0.17670699954032898\n",
            "class loss = 0.0684855729341507 dist loss = 0.17893241345882416\n",
            "class loss = 0.0687018558382988 dist loss = 0.18158327043056488\n",
            "class loss = 0.06805730611085892 dist loss = 0.17777040600776672\n",
            "class loss = 0.06700875610113144 dist loss = 0.1833188384771347\n",
            "class loss = 0.06779078394174576 dist loss = 0.17973102629184723\n",
            "class loss = 0.065391406416893 dist loss = 0.17865262925624847\n",
            "class loss = 0.06589406728744507 dist loss = 0.18272365629673004\n",
            "class loss = 0.06774857640266418 dist loss = 0.1805742084980011\n",
            "class loss = 0.06578609347343445 dist loss = 0.17937949299812317\n",
            "class loss = 0.06374578922986984 dist loss = 0.1800159513950348\n",
            "class loss = 0.06630917638540268 dist loss = 0.18241649866104126\n",
            "class loss = 0.06313025206327438 dist loss = 0.17657847702503204\n",
            "class loss = 0.06388743221759796 dist loss = 0.17932306230068207\n",
            "class loss = 0.06516069918870926 dist loss = 0.18075837194919586\n",
            "class loss = 0.06527548283338547 dist loss = 0.18261978030204773\n",
            "class loss = 0.06443782895803452 dist loss = 0.18229496479034424\n",
            "class loss = 0.06435324996709824 dist loss = 0.1830182671546936\n",
            "class loss = 0.06235913187265396 dist loss = 0.1830057054758072\n",
            "class loss = 0.06629186123609543 dist loss = 0.17918600142002106\n",
            "class loss = 0.07246005535125732 dist loss = 0.17777101695537567\n",
            "At step  30  and at epoch =  0  the loss is =  0.2502310872077942  and accuracy is =  0.0\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.07175051420927048 dist loss = 0.18416014313697815\n",
            "class loss = 0.06248084455728531 dist loss = 0.17897436022758484\n",
            "class loss = 0.06281760334968567 dist loss = 0.1774444878101349\n",
            "class loss = 0.06453168392181396 dist loss = 0.1798664927482605\n",
            "class loss = 0.06179427728056908 dist loss = 0.17681333422660828\n",
            "class loss = 0.0614810474216938 dist loss = 0.1797848641872406\n",
            "class loss = 0.06292317062616348 dist loss = 0.1846170276403427\n",
            "class loss = 0.0610123872756958 dist loss = 0.18044611811637878\n",
            "class loss = 0.0643942728638649 dist loss = 0.1794765293598175\n",
            "class loss = 0.06216322258114815 dist loss = 0.17707815766334534\n",
            "class loss = 0.06187490373849869 dist loss = 0.1800825148820877\n",
            "class loss = 0.06428857892751694 dist loss = 0.18209999799728394\n",
            "class loss = 0.06369851529598236 dist loss = 0.17868724465370178\n",
            "class loss = 0.06339805573225021 dist loss = 0.1824105679988861\n",
            "class loss = 0.0657220259308815 dist loss = 0.18389937281608582\n",
            "class loss = 0.06078395992517471 dist loss = 0.18206344544887543\n",
            "class loss = 0.06011809781193733 dist loss = 0.17689692974090576\n",
            "class loss = 0.06409993022680283 dist loss = 0.17829954624176025\n",
            "class loss = 0.05941803380846977 dist loss = 0.18073609471321106\n",
            "class loss = 0.06252823024988174 dist loss = 0.17731618881225586\n",
            "class loss = 0.06295404583215714 dist loss = 0.17946362495422363\n",
            "class loss = 0.062081921845674515 dist loss = 0.1822032630443573\n",
            "class loss = 0.06271101534366608 dist loss = 0.17836886644363403\n",
            "class loss = 0.05937674269080162 dist loss = 0.18377505242824554\n",
            "class loss = 0.06212718412280083 dist loss = 0.18031135201454163\n",
            "class loss = 0.058930404484272 dist loss = 0.17923402786254883\n",
            "class loss = 0.06034361198544502 dist loss = 0.18298588693141937\n",
            "class loss = 0.06344794481992722 dist loss = 0.18083259463310242\n",
            "class loss = 0.06064191460609436 dist loss = 0.17977562546730042\n",
            "class loss = 0.05813143774867058 dist loss = 0.18062730133533478\n",
            "class loss = 0.061567313969135284 dist loss = 0.18295574188232422\n",
            "class loss = 0.058071721345186234 dist loss = 0.17735260725021362\n",
            "class loss = 0.05941402539610863 dist loss = 0.18008099496364594\n",
            "class loss = 0.06166638061404228 dist loss = 0.18228432536125183\n",
            "class loss = 0.06039464473724365 dist loss = 0.1844310760498047\n",
            "class loss = 0.059849973767995834 dist loss = 0.18294629454612732\n",
            "class loss = 0.06014754995703697 dist loss = 0.18352197110652924\n",
            "class loss = 0.05757967382669449 dist loss = 0.18339373171329498\n",
            "class loss = 0.061194002628326416 dist loss = 0.17957042157649994\n",
            "class loss = 0.06498751044273376 dist loss = 0.17784345149993896\n",
            "At step  30  and at epoch =  1  the loss is =  0.24283096194267273  and accuracy is =  0.0\n",
            "[2.0]     [2.0]\n",
            "class loss = 0.0690532848238945 dist loss = 0.18484961986541748\n",
            "class loss = 0.058306705206632614 dist loss = 0.18094180524349213\n",
            "class loss = 0.05901229381561279 dist loss = 0.17823418974876404\n",
            "class loss = 0.06126147508621216 dist loss = 0.1803334355354309\n",
            "class loss = 0.0578046552836895 dist loss = 0.1772293895483017\n",
            "class loss = 0.0565139539539814 dist loss = 0.18044409155845642\n",
            "class loss = 0.058599650859832764 dist loss = 0.1850583553314209\n",
            "class loss = 0.05825783684849739 dist loss = 0.18077176809310913\n",
            "class loss = 0.06089711934328079 dist loss = 0.17998160421848297\n",
            "class loss = 0.05824226140975952 dist loss = 0.17745956778526306\n",
            "class loss = 0.058589786291122437 dist loss = 0.1805288940668106\n",
            "class loss = 0.060778092592954636 dist loss = 0.18256208300590515\n",
            "class loss = 0.059997644275426865 dist loss = 0.17915576696395874\n",
            "class loss = 0.06086471304297447 dist loss = 0.182750403881073\n",
            "class loss = 0.062430690973997116 dist loss = 0.1844499409198761\n",
            "class loss = 0.05724257975816727 dist loss = 0.18235746026039124\n",
            "class loss = 0.05728887394070625 dist loss = 0.1773894876241684\n",
            "class loss = 0.06055372953414917 dist loss = 0.1789294183254242\n",
            "class loss = 0.0555979423224926 dist loss = 0.18114666640758514\n",
            "class loss = 0.059774335473775864 dist loss = 0.1778600662946701\n",
            "class loss = 0.06006012111902237 dist loss = 0.1799575835466385\n",
            "class loss = 0.05905964598059654 dist loss = 0.18260928988456726\n",
            "class loss = 0.06004545837640762 dist loss = 0.178909569978714\n",
            "class loss = 0.05535348877310753 dist loss = 0.18421748280525208\n",
            "class loss = 0.059173233807086945 dist loss = 0.18078206479549408\n",
            "class loss = 0.05563023313879967 dist loss = 0.1796571910381317\n",
            "class loss = 0.057512879371643066 dist loss = 0.18336981534957886\n",
            "class loss = 0.06098222732543945 dist loss = 0.1812911331653595\n",
            "class loss = 0.0578097365796566 dist loss = 0.1803033947944641\n",
            "class loss = 0.05468131974339485 dist loss = 0.18115386366844177\n",
            "class loss = 0.05834812670946121 dist loss = 0.18339700996875763\n",
            "class loss = 0.05542919784784317 dist loss = 0.1777312457561493\n",
            "class loss = 0.0564642958343029 dist loss = 0.18031926453113556\n",
            "class loss = 0.05877721309661865 dist loss = 0.18235358595848083\n",
            "class loss = 0.056922245770692825 dist loss = 0.18405041098594666\n",
            "class loss = 0.05683736130595207 dist loss = 0.1828411966562271\n",
            "class loss = 0.05728527531027794 dist loss = 0.18386338651180267\n",
            "class loss = 0.05472549423575401 dist loss = 0.18387827277183533\n",
            "class loss = 0.05798199772834778 dist loss = 0.18007537722587585\n",
            "class loss = 0.05655479431152344 dist loss = 0.17914462089538574\n",
            "At step  30  and at epoch =  2  the loss is =  0.23569941520690918  and accuracy is =  0.0014\n",
            "[2.0]     [2.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}