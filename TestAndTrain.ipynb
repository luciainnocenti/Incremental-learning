{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestAndTrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1B7tLQZpXMoibv67Nr5DmsnSXgElh3fOJ",
      "authorship_tag": "ABX9TyO3RukpOe5H8eTjn6XgYCfW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/master/TestAndTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZPlUN0gVLhm",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPcTKoHMVEcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoZhDDvKVIkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'\n",
        "\n",
        "\n",
        "from DatasetCIFAR.data_set import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLaS2laafBaG",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liUP5Kc1DMbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from data_set import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQK7sS-Hed0j",
        "colab_type": "text"
      },
      "source": [
        "# Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eote8TwxYkfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "BATCH_SIZE = 256\n",
        "NUM_WORKERS = 100\n",
        "TASK_SIZE = 10\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "LR = 1\n",
        "NUM_EPOCHS = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_vlqOL7ehLC",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2CakyTYYmVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resNet = models.resnet18(pretrained=False)\n",
        "resNet = resNet.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_cyhIzFej5-",
        "colab_type": "text"
      },
      "source": [
        "# Define Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcBNohmiYBtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    Dataset(train=True),\n",
        "    num_workers=NUM_WORKERS,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    Dataset(train=False),\n",
        "    num_workers=NUM_WORKERS,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg4WUpU6ey7G",
        "colab_type": "text"
      },
      "source": [
        "# Define training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR4q4blae3Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(resNet.parameters(), lr=LR)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAT2KQEersx",
        "colab_type": "text"
      },
      "source": [
        "# Train phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJse4JU7d9ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_indexes = []\n",
        "for task in range(0, 100, TASK_SIZE):\n",
        "  print(\"Task classes {} to {}\".format(task, task + TASK_SIZE))\n",
        "\n",
        "  train_indexes = train_loader.dataset.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes.append(test_loader.dataset.__getIndexesGroups__(task))\n",
        "\n",
        "  train_dataset = Subset(Dataset(train=True), train_indexes)\n",
        "  test_dataset = Subset(Dataset(train=False), test_indexes)\n",
        "  resNet.fc = nn.Linear(512, task)\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    for images, labels in train_loader:\n",
        "    # Bring data over the device of choice\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      print(labels)\n",
        "      optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      # Forward pass to the network\n",
        "      outputs = resNet(images)\n",
        "\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()  # backward pass: computes gradients\n",
        "      optimizer.step() # update weights based on accumulated gradients\n",
        "      print(\"At step \", str(task), \" and at epoch = \", epoch, \" the loss is = \", loss)\n",
        "  scheduler.step() "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}