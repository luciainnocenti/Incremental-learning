{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainAndTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31e7ead1ef37434bb369b7564a8cd416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e86e51a3ed7f4412aeaa5316ebd44ebe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_380701c9c70d4957b61805628ea35b81",
              "IPY_MODEL_796549246d18466e832c0e0753bffaad"
            ]
          }
        },
        "e86e51a3ed7f4412aeaa5316ebd44ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "380701c9c70d4957b61805628ea35b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c7ff0415ed464fa88963f7ec2d596d17",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dcb8a415f28a4cd2b3f9e1515a3ba8d4"
          }
        },
        "796549246d18466e832c0e0753bffaad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1fc4733165784ae3a360913fd0b85f7f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:06&lt;00:00, 26887614.21it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dfdfe4d884e64fb397ce5a79f86c4f1c"
          }
        },
        "c7ff0415ed464fa88963f7ec2d596d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dcb8a415f28a4cd2b3f9e1515a3ba8d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fc4733165784ae3a360913fd0b85f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dfdfe4d884e64fb397ce5a79f86c4f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/Lucia/TrainAndTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbtGDBU3QJaq",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf0TmOM3NdFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I0pKIVIM2KC",
        "colab_type": "code",
        "outputId": "3dfdd24b-9ba5-42ca-fd39-f9ec187ef218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone -b Lucia https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/106)\u001b[K\rremote: Counting objects:   1% (2/106)\u001b[K\rremote: Counting objects:   2% (3/106)\u001b[K\rremote: Counting objects:   3% (4/106)\u001b[K\rremote: Counting objects:   4% (5/106)\u001b[K\rremote: Counting objects:   5% (6/106)\u001b[K\rremote: Counting objects:   6% (7/106)\u001b[K\rremote: Counting objects:   7% (8/106)\u001b[K\rremote: Counting objects:   8% (9/106)\u001b[K\rremote: Counting objects:   9% (10/106)\u001b[K\rremote: Counting objects:  10% (11/106)\u001b[K\rremote: Counting objects:  11% (12/106)\u001b[K\rremote: Counting objects:  12% (13/106)\u001b[K\rremote: Counting objects:  13% (14/106)\u001b[K\rremote: Counting objects:  14% (15/106)\u001b[K\rremote: Counting objects:  15% (16/106)\u001b[K\rremote: Counting objects:  16% (17/106)\u001b[K\rremote: Counting objects:  17% (19/106)\u001b[K\rremote: Counting objects:  18% (20/106)\u001b[K\rremote: Counting objects:  19% (21/106)\u001b[K\rremote: Counting objects:  20% (22/106)\u001b[K\rremote: Counting objects:  21% (23/106)\u001b[K\rremote: Counting objects:  22% (24/106)\u001b[K\rremote: Counting objects:  23% (25/106)\u001b[K\rremote: Counting objects:  24% (26/106)\u001b[K\rremote: Counting objects:  25% (27/106)\u001b[K\rremote: Counting objects:  26% (28/106)\u001b[K\rremote: Counting objects:  27% (29/106)\u001b[K\rremote: Counting objects:  28% (30/106)\u001b[K\rremote: Counting objects:  29% (31/106)\u001b[K\rremote: Counting objects:  30% (32/106)\u001b[K\rremote: Counting objects:  31% (33/106)\u001b[K\rremote: Counting objects:  32% (34/106)\u001b[K\rremote: Counting objects:  33% (35/106)\u001b[K\rremote: Counting objects:  34% (37/106)\u001b[K\rremote: Counting objects:  35% (38/106)\u001b[K\rremote: Counting objects:  36% (39/106)\u001b[K\rremote: Counting objects:  37% (40/106)\u001b[K\rremote: Counting objects:  38% (41/106)\u001b[K\rremote: Counting objects:  39% (42/106)\u001b[K\rremote: Counting objects:  40% (43/106)\u001b[K\rremote: Counting objects:  41% (44/106)\u001b[K\rremote: Counting objects:  42% (45/106)\u001b[K\rremote: Counting objects:  43% (46/106)\u001b[K\rremote: Counting objects:  44% (47/106)\u001b[K\rremote: Counting objects:  45% (48/106)\u001b[K\rremote: Counting objects:  46% (49/106)\u001b[K\rremote: Counting objects:  47% (50/106)\u001b[K\rremote: Counting objects:  48% (51/106)\u001b[K\rremote: Counting objects:  49% (52/106)\u001b[K\rremote: Counting objects:  50% (53/106)\u001b[K\rremote: Counting objects:  51% (55/106)\u001b[K\rremote: Counting objects:  52% (56/106)\u001b[K\rremote: Counting objects:  53% (57/106)\u001b[K\rremote: Counting objects:  54% (58/106)\u001b[K\rremote: Counting objects:  55% (59/106)\u001b[K\rremote: Counting objects:  56% (60/106)\u001b[K\rremote: Counting objects:  57% (61/106)\u001b[K\rremote: Counting objects:  58% (62/106)\u001b[K\rremote: Counting objects:  59% (63/106)\u001b[K\rremote: Counting objects:  60% (64/106)\u001b[K\rremote: Counting objects:  61% (65/106)\u001b[K\rremote: Counting objects:  62% (66/106)\u001b[K\rremote: Counting objects:  63% (67/106)\u001b[K\rremote: Counting objects:  64% (68/106)\u001b[K\rremote: Counting objects:  65% (69/106)\u001b[K\rremote: Counting objects:  66% (70/106)\u001b[K\rremote: Counting objects:  67% (72/106)\u001b[K\rremote: Counting objects:  68% (73/106)\u001b[K\rremote: Counting objects:  69% (74/106)\u001b[K\rremote: Counting objects:  70% (75/106)\u001b[K\rremote: Counting objects:  71% (76/106)\u001b[K\rremote: Counting objects:  72% (77/106)\u001b[K\rremote: Counting objects:  73% (78/106)\u001b[K\rremote: Counting objects:  74% (79/106)\u001b[K\rremote: Counting objects:  75% (80/106)\u001b[K\rremote: Counting objects:  76% (81/106)\u001b[K\rremote: Counting objects:  77% (82/106)\u001b[K\rremote: Counting objects:  78% (83/106)\u001b[K\rremote: Counting objects:  79% (84/106)\u001b[K\rremote: Counting objects:  80% (85/106)\u001b[K\rremote: Counting objects:  81% (86/106)\u001b[K\rremote: Counting objects:  82% (87/106)\u001b[K\rremote: Counting objects:  83% (88/106)\u001b[K\rremote: Counting objects:  84% (90/106)\u001b[K\rremote: Counting objects:  85% (91/106)\u001b[K\rremote: Counting objects:  86% (92/106)\u001b[K\rremote: Counting objects:  87% (93/106)\u001b[K\rremote: Counting objects:  88% (94/106)\u001b[K\rremote: Counting objects:  89% (95/106)\u001b[K\rremote: Counting objects:  90% (96/106)\u001b[K\rremote: Counting objects:  91% (97/106)\u001b[K\rremote: Counting objects:  92% (98/106)\u001b[K\rremote: Counting objects:  93% (99/106)\u001b[K\rremote: Counting objects:  94% (100/106)\u001b[K\rremote: Counting objects:  95% (101/106)\u001b[K\rremote: Counting objects:  96% (102/106)\u001b[K\rremote: Counting objects:  97% (103/106)\u001b[K\rremote: Counting objects:  98% (104/106)\u001b[K\rremote: Counting objects:  99% (105/106)\u001b[K\rremote: Counting objects: 100% (106/106)\u001b[K\rremote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/106)\u001b[K\rremote: Compressing objects:   1% (2/106)\u001b[K\rremote: Compressing objects:   2% (3/106)\u001b[K\rremote: Compressing objects:   3% (4/106)\u001b[K\rremote: Compressing objects:   4% (5/106)\u001b[K\rremote: Compressing objects:   5% (6/106)\u001b[K\rremote: Compressing objects:   6% (7/106)\u001b[K\rremote: Compressing objects:   7% (8/106)\u001b[K\rremote: Compressing objects:   8% (9/106)\u001b[K\rremote: Compressing objects:   9% (10/106)\u001b[K\rremote: Compressing objects:  10% (11/106)\u001b[K\rremote: Compressing objects:  11% (12/106)\u001b[K\rremote: Compressing objects:  12% (13/106)\u001b[K\rremote: Compressing objects:  13% (14/106)\u001b[K\rremote: Compressing objects:  14% (15/106)\u001b[K\rremote: Compressing objects:  15% (16/106)\u001b[K\rremote: Compressing objects:  16% (17/106)\u001b[K\rremote: Compressing objects:  17% (19/106)\u001b[K\rremote: Compressing objects:  18% (20/106)\u001b[K\rremote: Compressing objects:  19% (21/106)\u001b[K\rremote: Compressing objects:  20% (22/106)\u001b[K\rremote: Compressing objects:  21% (23/106)\u001b[K\rremote: Compressing objects:  22% (24/106)\u001b[K\rremote: Compressing objects:  23% (25/106)\u001b[K\rremote: Compressing objects:  24% (26/106)\u001b[K\rremote: Compressing objects:  25% (27/106)\u001b[K\rremote: Compressing objects:  26% (28/106)\u001b[K\rremote: Compressing objects:  27% (29/106)\u001b[K\rremote: Compressing objects:  28% (30/106)\u001b[K\rremote: Compressing objects:  29% (31/106)\u001b[K\rremote: Compressing objects:  30% (32/106)\u001b[K\rremote: Compressing objects:  31% (33/106)\u001b[K\rremote: Compressing objects:  32% (34/106)\u001b[K\rremote: Compressing objects:  33% (35/106)\u001b[K\rremote: Compressing objects:  34% (37/106)\u001b[K\rremote: Compressing objects:  35% (38/106)\u001b[K\rremote: Compressing objects:  36% (39/106)\u001b[K\rremote: Compressing objects:  37% (40/106)\u001b[K\rremote: Compressing objects:  38% (41/106)\u001b[K\rremote: Compressing objects:  39% (42/106)\u001b[K\rremote: Compressing objects:  40% (43/106)\u001b[K\rremote: Compressing objects:  41% (44/106)\u001b[K\rremote: Compressing objects:  42% (45/106)\u001b[K\rremote: Compressing objects:  43% (46/106)\u001b[K\rremote: Compressing objects:  44% (47/106)\u001b[K\rremote: Compressing objects:  45% (48/106)\u001b[K\rremote: Compressing objects:  46% (49/106)\u001b[K\rremote: Compressing objects:  47% (50/106)\u001b[K\rremote: Compressing objects:  48% (51/106)\u001b[K\rremote: Compressing objects:  49% (52/106)\u001b[K\rremote: Compressing objects:  50% (53/106)\u001b[K\rremote: Compressing objects:  51% (55/106)\u001b[K\rremote: Compressing objects:  52% (56/106)\u001b[K\rremote: Compressing objects:  53% (57/106)\u001b[K\rremote: Compressing objects:  54% (58/106)\u001b[K\rremote: Compressing objects:  55% (59/106)\u001b[K\rremote: Compressing objects:  56% (60/106)\u001b[K\rremote: Compressing objects:  57% (61/106)\u001b[K\rremote: Compressing objects:  58% (62/106)\u001b[K\rremote: Compressing objects:  59% (63/106)\u001b[K\rremote: Compressing objects:  60% (64/106)\u001b[K\rremote: Compressing objects:  61% (65/106)\u001b[K\rremote: Compressing objects:  62% (66/106)\u001b[K\rremote: Compressing objects:  63% (67/106)\u001b[K\rremote: Compressing objects:  64% (68/106)\u001b[K\rremote: Compressing objects:  65% (69/106)\u001b[K\rremote: Compressing objects:  66% (70/106)\u001b[K\rremote: Compressing objects:  67% (72/106)\u001b[K\rremote: Compressing objects:  68% (73/106)\u001b[K\rremote: Compressing objects:  69% (74/106)\u001b[K\rremote: Compressing objects:  70% (75/106)\u001b[K\rremote: Compressing objects:  71% (76/106)\u001b[K\rremote: Compressing objects:  72% (77/106)\u001b[K\rremote: Compressing objects:  73% (78/106)\u001b[K\rremote: Compressing objects:  74% (79/106)\u001b[K\rremote: Compressing objects:  75% (80/106)\u001b[K\rremote: Compressing objects:  76% (81/106)\u001b[K\rremote: Compressing objects:  77% (82/106)\u001b[K\rremote: Compressing objects:  78% (83/106)\u001b[K\rremote: Compressing objects:  79% (84/106)\u001b[K\rremote: Compressing objects:  80% (85/106)\u001b[K\rremote: Compressing objects:  81% (86/106)\u001b[K\rremote: Compressing objects:  82% (87/106)\u001b[K\rremote: Compressing objects:  83% (88/106)\u001b[K\rremote: Compressing objects:  84% (90/106)\u001b[K\rremote: Compressing objects:  85% (91/106)\u001b[K\rremote: Compressing objects:  86% (92/106)\u001b[K\rremote: Compressing objects:  87% (93/106)\u001b[K\rremote: Compressing objects:  88% (94/106)\u001b[K\rremote: Compressing objects:  89% (95/106)\u001b[K\rremote: Compressing objects:  90% (96/106)\u001b[K\rremote: Compressing objects:  91% (97/106)\u001b[K\rremote: Compressing objects:  92% (98/106)\u001b[K\rremote: Compressing objects:  93% (99/106)\u001b[K\rremote: Compressing objects:  94% (100/106)\u001b[K\rremote: Compressing objects:  95% (101/106)\u001b[K\rremote: Compressing objects:  96% (102/106)\u001b[K\rremote: Compressing objects:  97% (103/106)\u001b[K\rremote: Compressing objects:  98% (104/106)\u001b[K\rremote: Compressing objects:  99% (105/106)\u001b[K\rremote: Compressing objects: 100% (106/106)\u001b[K\rremote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "Receiving objects:   0% (1/638)   \rReceiving objects:   1% (7/638)   \rReceiving objects:   2% (13/638)   \rReceiving objects:   3% (20/638)   \rReceiving objects:   4% (26/638)   \rReceiving objects:   5% (32/638)   \rReceiving objects:   6% (39/638)   \rReceiving objects:   7% (45/638)   \rReceiving objects:   8% (52/638)   \rReceiving objects:   9% (58/638)   \rReceiving objects:  10% (64/638)   \rReceiving objects:  11% (71/638)   \rReceiving objects:  12% (77/638)   \rReceiving objects:  13% (83/638)   \rReceiving objects:  14% (90/638)   \rReceiving objects:  15% (96/638)   \rReceiving objects:  16% (103/638)   \rReceiving objects:  17% (109/638)   \rReceiving objects:  18% (115/638)   \rReceiving objects:  19% (122/638)   \rReceiving objects:  20% (128/638)   \rReceiving objects:  21% (134/638)   \rReceiving objects:  22% (141/638)   \rReceiving objects:  23% (147/638)   \rReceiving objects:  24% (154/638)   \rReceiving objects:  25% (160/638)   \rReceiving objects:  26% (166/638)   \rReceiving objects:  27% (173/638)   \rReceiving objects:  28% (179/638)   \rReceiving objects:  29% (186/638)   \rReceiving objects:  30% (192/638)   \rReceiving objects:  31% (198/638)   \rReceiving objects:  32% (205/638)   \rReceiving objects:  33% (211/638)   \rReceiving objects:  34% (217/638)   \rReceiving objects:  35% (224/638)   \rReceiving objects:  36% (230/638)   \rReceiving objects:  37% (237/638)   \rReceiving objects:  38% (243/638)   \rReceiving objects:  39% (249/638)   \rReceiving objects:  40% (256/638)   \rReceiving objects:  41% (262/638)   \rReceiving objects:  42% (268/638)   \rReceiving objects:  43% (275/638)   \rReceiving objects:  44% (281/638)   \rReceiving objects:  45% (288/638)   \rReceiving objects:  46% (294/638)   \rReceiving objects:  47% (300/638)   \rReceiving objects:  48% (307/638)   \rReceiving objects:  49% (313/638)   \rReceiving objects:  50% (319/638)   \rReceiving objects:  51% (326/638)   \rReceiving objects:  52% (332/638)   \rReceiving objects:  53% (339/638)   \rReceiving objects:  54% (345/638)   \rReceiving objects:  55% (351/638)   \rReceiving objects:  56% (358/638)   \rReceiving objects:  57% (364/638)   \rReceiving objects:  58% (371/638)   \rReceiving objects:  59% (377/638)   \rReceiving objects:  60% (383/638)   \rReceiving objects:  61% (390/638)   \rReceiving objects:  62% (396/638)   \rReceiving objects:  63% (402/638)   \rReceiving objects:  64% (409/638)   \rReceiving objects:  65% (415/638)   \rReceiving objects:  66% (422/638)   \rReceiving objects:  67% (428/638)   \rReceiving objects:  68% (434/638)   \rReceiving objects:  69% (441/638)   \rReceiving objects:  70% (447/638)   \rReceiving objects:  71% (453/638)   \rReceiving objects:  72% (460/638)   \rReceiving objects:  73% (466/638)   \rReceiving objects:  74% (473/638)   \rReceiving objects:  75% (479/638)   \rReceiving objects:  76% (485/638)   \rReceiving objects:  77% (492/638)   \rReceiving objects:  78% (498/638)   \rReceiving objects:  79% (505/638)   \rReceiving objects:  80% (511/638)   \rremote: Total 638 (delta 66), reused 0 (delta 0), pack-reused 532\u001b[K\n",
            "Receiving objects:  81% (517/638)   \rReceiving objects:  82% (524/638)   \rReceiving objects:  83% (530/638)   \rReceiving objects:  84% (536/638)   \rReceiving objects:  85% (543/638)   \rReceiving objects:  86% (549/638)   \rReceiving objects:  87% (556/638)   \rReceiving objects:  88% (562/638)   \rReceiving objects:  89% (568/638)   \rReceiving objects:  90% (575/638)   \rReceiving objects:  91% (581/638)   \rReceiving objects:  92% (587/638)   \rReceiving objects:  93% (594/638)   \rReceiving objects:  94% (600/638)   \rReceiving objects:  95% (607/638)   \rReceiving objects:  96% (613/638)   \rReceiving objects:  97% (619/638)   \rReceiving objects:  98% (626/638)   \rReceiving objects:  99% (632/638)   \rReceiving objects: 100% (638/638)   \rReceiving objects: 100% (638/638), 406.19 KiB | 5.21 MiB/s, done.\n",
            "Resolving deltas:   0% (0/395)   \rResolving deltas:   1% (4/395)   \rResolving deltas:   5% (23/395)   \rResolving deltas:   8% (33/395)   \rResolving deltas:  12% (50/395)   \rResolving deltas:  14% (59/395)   \rResolving deltas:  38% (154/395)   \rResolving deltas:  40% (158/395)   \rResolving deltas:  64% (254/395)   \rResolving deltas:  65% (257/395)   \rResolving deltas:  74% (294/395)   \rResolving deltas:  78% (309/395)   \rResolving deltas:  80% (316/395)   \rResolving deltas:  81% (320/395)   \rResolving deltas:  89% (354/395)   \rResolving deltas:  91% (360/395)   \rResolving deltas:  94% (374/395)   \rResolving deltas:  98% (389/395)   \rResolving deltas: 100% (395/395)   \rResolving deltas: 100% (395/395), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLaS2laafBaG",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liUP5Kc1DMbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from DatasetCIFAR.data_set import Dataset \n",
        "from DatasetCIFAR import ResNet\n",
        "from DatasetCIFAR import utils\n",
        "from DatasetCIFAR import params\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "random.seed(params.SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_vlqOL7ehLC",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWttFW3ljoMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resNet = ResNet.resnet32(num_classes=100)\n",
        "resNet = resNet.to(params.DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmohsyVWFpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transformer = transforms.Compose([transforms.RandomCrop(size = 32, padding=4),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_cyhIzFej5-",
        "colab_type": "text"
      },
      "source": [
        "# Define DataSets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcBNohmiYBtP",
        "colab_type": "code",
        "outputId": "389689a9-d31a-40bf-b993-676474cac6df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "31e7ead1ef37434bb369b7564a8cd416",
            "e86e51a3ed7f4412aeaa5316ebd44ebe",
            "380701c9c70d4957b61805628ea35b81",
            "796549246d18466e832c0e0753bffaad",
            "c7ff0415ed464fa88963f7ec2d596d17",
            "dcb8a415f28a4cd2b3f9e1515a3ba8d4",
            "1fc4733165784ae3a360913fd0b85f7f",
            "dfdfe4d884e64fb397ce5a79f86c4f1c"
          ]
        }
      },
      "source": [
        "trainDS = Dataset(train=True, transform = train_transformer)\n",
        "testDS = Dataset(train=False, transform = test_transformer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31e7ead1ef37434bb369b7564a8cd416",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFxMUO_FQZRo",
        "colab_type": "code",
        "outputId": "eb2e5824-92df-41d5-cc5f-2408669d6e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "train_splits = trainDS.splits\n",
        "test_splits = testDS.splits\n",
        "print(train_splits)\n",
        "print(test_splits)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[94.0, 63.0, 74.0, 21.0, 35.0, 56.0, 91.0, 96.0, 87.0, 48.0], [68.0, 80.0, 22.0, 37.0, 60.0, 97.0, 51.0, 62.0, 92.0, 76.0], [75.0, 89.0, 23.0, 99.0, 39.0, 66.0, 54.0, 69.0, 84.0, 61.0], [85.0, 24.0, 98.0, 41.0, 73.0, 58.0, 78.0, 77.0, 70.0, 49.0], [65.0, 88.0, 36.0, 93.0, 45.0, 10.0, 90.0, 17.0, 32.0, 59.0], [83.0, 43.0, 53.0, 11.0, 86.0, 19.0, 38.0, 30.0, 40.0, 50.0], [57.0, 81.0, 12.0, 95.0, 25.0, 47.0, 34.0, 52.0, 44.0, 72.0], [46.0, 79.0, 20.0, 28.0, 5.0, 71.0, 8.0, 18.0, 33.0, 15.0], [55.0, 29.0, 64.0, 31.0, 67.0, 7.0, 13.0, 14.0, 42.0, 6.0], [82.0, 2.0, 27.0, 16.0, 26.0, 3.0, 4.0, 1.0, 9.0, 0.0]]\n",
            "[[94.0, 63.0, 74.0, 21.0, 35.0, 56.0, 91.0, 96.0, 87.0, 48.0], [68.0, 80.0, 22.0, 37.0, 60.0, 97.0, 51.0, 62.0, 92.0, 76.0], [75.0, 89.0, 23.0, 99.0, 39.0, 66.0, 54.0, 69.0, 84.0, 61.0], [85.0, 24.0, 98.0, 41.0, 73.0, 58.0, 78.0, 77.0, 70.0, 49.0], [65.0, 88.0, 36.0, 93.0, 45.0, 10.0, 90.0, 17.0, 32.0, 59.0], [83.0, 43.0, 53.0, 11.0, 86.0, 19.0, 38.0, 30.0, 40.0, 50.0], [57.0, 81.0, 12.0, 95.0, 25.0, 47.0, 34.0, 52.0, 44.0, 72.0], [46.0, 79.0, 20.0, 28.0, 5.0, 71.0, 8.0, 18.0, 33.0, 15.0], [55.0, 29.0, 64.0, 31.0, 67.0, 7.0, 13.0, 14.0, 42.0, 6.0], [82.0, 2.0, 27.0, 16.0, 26.0, 3.0, 4.0, 1.0, 9.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAT2KQEersx",
        "colab_type": "text"
      },
      "source": [
        "# Useful plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1l7flYj4NJh",
        "colab_type": "text"
      },
      "source": [
        "The function plotEpoch plots, at the end of each task, how accuracy and loss change during the training phase. It show\n",
        "\n",
        "*   Validation and Training Accuracy\n",
        "*   Validation and Training Loss\n",
        "\n",
        "The function plotTask, for each task, how the accuracy on the validation set change when adding new tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr58kkHiIzZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTask(pars_tasks):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  x_tasks =  np.linspace(10, 100, 10)\n",
        "\n",
        "  plt.plot(x_tasks, pars_tasks, label=['Accuracy', 'Loss'])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.title('Accuracy over tasks')\n",
        "  plt.legend(['Accuracy', 'Loss'])\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iApKvCs942aS",
        "colab_type": "text"
      },
      "source": [
        "# Train and evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJse4JU7d9ck",
        "colab_type": "code",
        "outputId": "de9eac31-f73d-49ad-bc26-c09147a71727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pars_tasks = []\n",
        "test_indexes = []\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "  pars_tasks.insert(task, 0)\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "\n",
        "  train_indexes = trainDS.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes + testDS.__getIndexesGroups__(task)\n",
        "\n",
        "  train_dataset = Subset(trainDS, train_indexes)\n",
        "  test_dataset = Subset(testDS, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE)\n",
        "  test_loader = DataLoader( test_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE )\n",
        "\n",
        "  if(task == 0):\n",
        "    torch.save(resNet, 'resNet_task{0}.pt'.format(task))\n",
        "  \n",
        "  \n",
        "\n",
        "  utils.trainfunction(task, train_loader, train_splits)\n",
        "  param = utils.evaluationTest(task, test_loader, test_splits)\n",
        "  pars_tasks[int(task/10)] = param #pars_task[i] = (accuracy, loss) at i-th task\t"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "task = 0 \n",
            "train col =  [94 63 74 21 35 56 91 96 87 48]\n",
            "train col =  [[94 63 74 21 35 56 91 96 87 48]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "At step  0  and at epoch =  0  the loss is =  0.03609519824385643  and accuracy is =  0.2056\n",
            "At step  0  and at epoch =  1  the loss is =  0.02680780366063118  and accuracy is =  0.4162\n",
            "At step  0  and at epoch =  2  the loss is =  0.01907293312251568  and accuracy is =  0.4996\n",
            "At step  0  and at epoch =  3  the loss is =  0.012124900706112385  and accuracy is =  0.581\n",
            "At step  0  and at epoch =  4  the loss is =  0.010085375048220158  and accuracy is =  0.6584\n",
            "At step  0  and at epoch =  5  the loss is =  0.004410715773701668  and accuracy is =  0.6976\n",
            "At step  0  and at epoch =  6  the loss is =  0.0028553269803524017  and accuracy is =  0.7402\n",
            "At step  0  and at epoch =  7  the loss is =  0.003499081125482917  and accuracy is =  0.7734\n",
            "At step  0  and at epoch =  8  the loss is =  0.0032141385599970818  and accuracy is =  0.7746\n",
            "At step  0  and at epoch =  9  the loss is =  0.0018210397101938725  and accuracy is =  0.796\n",
            "At step  0  and at epoch =  10  the loss is =  0.0012096695136278868  and accuracy is =  0.83\n",
            "At step  0  and at epoch =  11  the loss is =  0.0031817930284887552  and accuracy is =  0.836\n",
            "At step  0  and at epoch =  12  the loss is =  0.0019267911557108164  and accuracy is =  0.8336\n",
            "At step  0  and at epoch =  13  the loss is =  0.0010738298296928406  and accuracy is =  0.888\n",
            "At step  0  and at epoch =  14  the loss is =  0.0009695847402326763  and accuracy is =  0.9126\n",
            "At step  0  and at epoch =  15  the loss is =  0.0006848697084933519  and accuracy is =  0.9258\n",
            "At step  0  and at epoch =  16  the loss is =  0.0007704693707637489  and accuracy is =  0.9288\n",
            "At step  0  and at epoch =  17  the loss is =  0.004355736076831818  and accuracy is =  0.9308\n",
            "At step  0  and at epoch =  18  the loss is =  0.003251518588513136  and accuracy is =  0.9306\n",
            "At step  0  and at epoch =  19  the loss is =  0.0005535873933695257  and accuracy is =  0.9214\n",
            "At step  0  and at epoch =  20  the loss is =  0.0003459776926320046  and accuracy is =  0.9462\n",
            "At step  0  and at epoch =  21  the loss is =  0.0004814017447642982  and accuracy is =  0.968\n",
            "At step  0  and at epoch =  22  the loss is =  0.00020072174083907157  and accuracy is =  0.972\n",
            "At step  0  and at epoch =  23  the loss is =  0.00022409766097553074  and accuracy is =  0.973\n",
            "At step  0  and at epoch =  24  the loss is =  0.0011613046517595649  and accuracy is =  0.9718\n",
            "At step  0  and at epoch =  25  the loss is =  0.009110193699598312  and accuracy is =  0.9796\n",
            "At step  0  and at epoch =  26  the loss is =  0.0059602390974760056  and accuracy is =  0.966\n",
            "At step  0  and at epoch =  27  the loss is =  0.0065015219151973724  and accuracy is =  0.9492\n",
            "At step  0  and at epoch =  28  the loss is =  0.0008649345254525542  and accuracy is =  0.9388\n",
            "At step  0  and at epoch =  29  the loss is =  0.00015114693087525666  and accuracy is =  0.9788\n",
            "At step  0  and at epoch =  30  the loss is =  0.00013384022167883813  and accuracy is =  0.9918\n",
            "At step  0  and at epoch =  31  the loss is =  9.836218669079244e-05  and accuracy is =  0.9972\n",
            "At step  0  and at epoch =  32  the loss is =  8.249011443695053e-05  and accuracy is =  0.998\n",
            "At step  0  and at epoch =  33  the loss is =  8.561040158383548e-05  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  34  the loss is =  7.336382986977696e-05  and accuracy is =  0.9994\n",
            "At step  0  and at epoch =  35  the loss is =  6.721529643982649e-05  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  36  the loss is =  6.74275288474746e-05  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  37  the loss is =  6.617343024117872e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  38  the loss is =  6.585881055798382e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  39  the loss is =  6.582672358490527e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  40  the loss is =  6.614416633965448e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  41  the loss is =  6.669951108051464e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  42  the loss is =  6.74507682560943e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  43  the loss is =  6.829684571130201e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  44  the loss is =  6.924146873643622e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  45  the loss is =  7.021721103228629e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  46  the loss is =  7.123686373233795e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  47  the loss is =  7.227648166008294e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  48  the loss is =  7.224240835057572e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  49  the loss is =  7.239441038109362e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  50  the loss is =  7.25574282114394e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  51  the loss is =  7.273279334185645e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  52  the loss is =  7.291530346265063e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  53  the loss is =  7.309973443625495e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  54  the loss is =  7.328714855248109e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  55  the loss is =  7.347826613113284e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  56  the loss is =  7.36711808713153e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  57  the loss is =  7.38692979211919e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  58  the loss is =  7.406861550407484e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  59  the loss is =  7.426955562550575e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  60  the loss is =  7.447332609444857e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  61  the loss is =  7.467262912541628e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  62  the loss is =  7.467355317203328e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  63  the loss is =  7.471062417607754e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  64  the loss is =  7.474530139006674e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  65  the loss is =  7.478504994651303e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  66  the loss is =  7.482180808437988e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  67  the loss is =  7.485932292183861e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  68  the loss is =  7.489549170713872e-05  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  69  the loss is =  7.493404700653628e-05  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "104\n",
            "Validation Loss: 0.014445848762989044 Validation Accuracy : 0.793\n",
            "task = 10 \n",
            "train col =  [68 80 22 37 60 97 51 62 92 76]\n",
            "train col =  [[68 80 22 37 60 97 51 62 92 76]]\n",
            "At step  10  and at epoch =  0  the loss is =  0.04008498042821884  and accuracy is =  0.323\n",
            "At step  10  and at epoch =  1  the loss is =  0.02599959820508957  and accuracy is =  0.5722\n",
            "At step  10  and at epoch =  2  the loss is =  0.0229790061712265  and accuracy is =  0.6582\n",
            "At step  10  and at epoch =  3  the loss is =  0.020171210169792175  and accuracy is =  0.7264\n",
            "At step  10  and at epoch =  4  the loss is =  0.021286677569150925  and accuracy is =  0.7776\n",
            "At step  10  and at epoch =  5  the loss is =  0.017927173525094986  and accuracy is =  0.834\n",
            "At step  10  and at epoch =  6  the loss is =  0.018599506467580795  and accuracy is =  0.8488\n",
            "At step  10  and at epoch =  7  the loss is =  0.019312240183353424  and accuracy is =  0.851\n",
            "At step  10  and at epoch =  8  the loss is =  0.014558207243680954  and accuracy is =  0.862\n",
            "At step  10  and at epoch =  9  the loss is =  0.013329285196959972  and accuracy is =  0.906\n",
            "At step  10  and at epoch =  10  the loss is =  0.013364984653890133  and accuracy is =  0.9232\n",
            "At step  10  and at epoch =  11  the loss is =  0.013265585526823997  and accuracy is =  0.92\n",
            "At step  10  and at epoch =  12  the loss is =  0.012554625980556011  and accuracy is =  0.93\n",
            "At step  10  and at epoch =  13  the loss is =  0.011790131218731403  and accuracy is =  0.9342\n",
            "At step  10  and at epoch =  14  the loss is =  0.010988590307533741  and accuracy is =  0.9414\n",
            "At step  10  and at epoch =  15  the loss is =  0.010070178657770157  and accuracy is =  0.9594\n",
            "At step  10  and at epoch =  16  the loss is =  0.010409274138510227  and accuracy is =  0.9672\n",
            "At step  10  and at epoch =  17  the loss is =  0.010952307842671871  and accuracy is =  0.9738\n",
            "At step  10  and at epoch =  18  the loss is =  0.012145006097853184  and accuracy is =  0.9762\n",
            "At step  10  and at epoch =  19  the loss is =  0.012488835491240025  and accuracy is =  0.9786\n",
            "At step  10  and at epoch =  20  the loss is =  0.00977311097085476  and accuracy is =  0.9854\n",
            "At step  10  and at epoch =  21  the loss is =  0.009855017997324467  and accuracy is =  0.9916\n",
            "At step  10  and at epoch =  22  the loss is =  0.009835517965257168  and accuracy is =  0.9944\n",
            "At step  10  and at epoch =  23  the loss is =  0.008949372917413712  and accuracy is =  0.9946\n",
            "At step  10  and at epoch =  24  the loss is =  0.009747946634888649  and accuracy is =  0.9958\n",
            "At step  10  and at epoch =  25  the loss is =  0.010402528569102287  and accuracy is =  0.9968\n",
            "At step  10  and at epoch =  26  the loss is =  0.010662841610610485  and accuracy is =  0.9944\n",
            "At step  10  and at epoch =  27  the loss is =  0.010365226306021214  and accuracy is =  0.9952\n",
            "At step  10  and at epoch =  28  the loss is =  0.009129132144153118  and accuracy is =  0.9978\n",
            "At step  10  and at epoch =  29  the loss is =  0.009533807635307312  and accuracy is =  0.9994\n",
            "At step  10  and at epoch =  30  the loss is =  0.009373949840664864  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  31  the loss is =  0.009311273694038391  and accuracy is =  0.9994\n",
            "At step  10  and at epoch =  32  the loss is =  0.010252160020172596  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  33  the loss is =  0.009882842190563679  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  34  the loss is =  0.010039052926003933  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  35  the loss is =  0.009353848174214363  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  36  the loss is =  0.008727706037461758  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  37  the loss is =  0.008686091750860214  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  38  the loss is =  0.008657748810946941  and accuracy is =  0.9998\n",
            "At step  10  and at epoch =  39  the loss is =  0.008907726965844631  and accuracy is =  0.9998\n",
            "At step  10  and at epoch =  40  the loss is =  0.008863232098519802  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  41  the loss is =  0.008611450903117657  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  42  the loss is =  0.008727103471755981  and accuracy is =  0.9998\n",
            "At step  10  and at epoch =  43  the loss is =  0.00933963805437088  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  44  the loss is =  0.00880893412977457  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  45  the loss is =  0.008606044575572014  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  46  the loss is =  0.008593649603426456  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  47  the loss is =  0.008913520723581314  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  48  the loss is =  0.008977144956588745  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  49  the loss is =  0.008765537291765213  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  50  the loss is =  0.008643167093396187  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  51  the loss is =  0.008576789870858192  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  52  the loss is =  0.00852868054062128  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  53  the loss is =  0.008491571992635727  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  54  the loss is =  0.008462757803499699  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  55  the loss is =  0.008440203033387661  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  56  the loss is =  0.008421889506280422  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  57  the loss is =  0.008406925946474075  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  58  the loss is =  0.008394752629101276  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  59  the loss is =  0.008384691551327705  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  60  the loss is =  0.008376113139092922  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  61  the loss is =  0.008368799462914467  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  62  the loss is =  0.00836214516311884  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  63  the loss is =  0.008358369581401348  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  64  the loss is =  0.008356341160833836  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  65  the loss is =  0.008354797959327698  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  66  the loss is =  0.008353445678949356  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  67  the loss is =  0.008352233096957207  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  68  the loss is =  0.008351112715899944  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  69  the loss is =  0.008350078947842121  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "80\n",
            "Validation Loss: 0.034650277346372604 Validation Accuracy : 0.555\n",
            "task = 20 \n",
            "train col =  [75 89 23 99 39 66 54 69 84 61]\n",
            "train col =  [[75 89 23 99 39 66 54 69 84 61]]\n",
            "At step  20  and at epoch =  0  the loss is =  0.06169098615646362  and accuracy is =  0.4236\n",
            "At step  20  and at epoch =  1  the loss is =  0.04548990726470947  and accuracy is =  0.615\n",
            "At step  20  and at epoch =  2  the loss is =  0.03690212219953537  and accuracy is =  0.7134\n",
            "At step  20  and at epoch =  3  the loss is =  0.031166018918156624  and accuracy is =  0.8026\n",
            "At step  20  and at epoch =  4  the loss is =  0.03611619025468826  and accuracy is =  0.8644\n",
            "At step  20  and at epoch =  5  the loss is =  0.03396085277199745  and accuracy is =  0.8848\n",
            "At step  20  and at epoch =  6  the loss is =  0.03167423978447914  and accuracy is =  0.9072\n",
            "At step  20  and at epoch =  7  the loss is =  0.026829881593585014  and accuracy is =  0.9246\n",
            "At step  20  and at epoch =  8  the loss is =  0.025404730811715126  and accuracy is =  0.9428\n",
            "At step  20  and at epoch =  9  the loss is =  0.023282160982489586  and accuracy is =  0.9564\n",
            "At step  20  and at epoch =  10  the loss is =  0.022494111210107803  and accuracy is =  0.9638\n",
            "At step  20  and at epoch =  11  the loss is =  0.023804033175110817  and accuracy is =  0.9674\n",
            "At step  20  and at epoch =  12  the loss is =  0.02335174009203911  and accuracy is =  0.9782\n",
            "At step  20  and at epoch =  13  the loss is =  0.025355929508805275  and accuracy is =  0.9866\n",
            "At step  20  and at epoch =  14  the loss is =  0.02534158155322075  and accuracy is =  0.9902\n",
            "At step  20  and at epoch =  15  the loss is =  0.02125159651041031  and accuracy is =  0.9918\n",
            "At step  20  and at epoch =  16  the loss is =  0.020123876631259918  and accuracy is =  0.9946\n",
            "At step  20  and at epoch =  17  the loss is =  0.020894020795822144  and accuracy is =  0.9942\n",
            "At step  20  and at epoch =  18  the loss is =  0.021586189046502113  and accuracy is =  0.9968\n",
            "At step  20  and at epoch =  19  the loss is =  0.02196335978806019  and accuracy is =  0.9972\n",
            "At step  20  and at epoch =  20  the loss is =  0.02317819930613041  and accuracy is =  0.9966\n",
            "At step  20  and at epoch =  21  the loss is =  0.022020291537046432  and accuracy is =  0.9962\n",
            "At step  20  and at epoch =  22  the loss is =  0.02344256266951561  and accuracy is =  0.9978\n",
            "At step  20  and at epoch =  23  the loss is =  0.02092607691884041  and accuracy is =  0.9988\n",
            "At step  20  and at epoch =  24  the loss is =  0.020296338945627213  and accuracy is =  0.9994\n",
            "At step  20  and at epoch =  25  the loss is =  0.01967492513358593  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  26  the loss is =  0.02047743834555149  and accuracy is =  0.9988\n",
            "At step  20  and at epoch =  27  the loss is =  0.02071123570203781  and accuracy is =  0.9992\n",
            "At step  20  and at epoch =  28  the loss is =  0.020184151828289032  and accuracy is =  0.9986\n",
            "At step  20  and at epoch =  29  the loss is =  0.020572977140545845  and accuracy is =  0.999\n",
            "At step  20  and at epoch =  30  the loss is =  0.020724067464470863  and accuracy is =  0.9992\n",
            "At step  20  and at epoch =  31  the loss is =  0.019575782120227814  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  32  the loss is =  0.019825512543320656  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  33  the loss is =  0.020073937252163887  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  34  the loss is =  0.020473573356866837  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  35  the loss is =  0.020392408594489098  and accuracy is =  0.999\n",
            "At step  20  and at epoch =  36  the loss is =  0.02049095183610916  and accuracy is =  0.9984\n",
            "At step  20  and at epoch =  37  the loss is =  0.01998939737677574  and accuracy is =  0.999\n",
            "At step  20  and at epoch =  38  the loss is =  0.019534390419721603  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  39  the loss is =  0.019370799884200096  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  40  the loss is =  0.01974012330174446  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  41  the loss is =  0.019816413521766663  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  42  the loss is =  0.020230447873473167  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  43  the loss is =  0.020615320652723312  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  44  the loss is =  0.01952272281050682  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  45  the loss is =  0.01997453346848488  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  46  the loss is =  0.020198222249746323  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  47  the loss is =  0.020093316212296486  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  48  the loss is =  0.018992647528648376  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  49  the loss is =  0.018833482638001442  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  50  the loss is =  0.018749874085187912  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  51  the loss is =  0.018695084378123283  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  52  the loss is =  0.018659504130482674  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  53  the loss is =  0.018634971231222153  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  54  the loss is =  0.018616551533341408  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  55  the loss is =  0.018602054566144943  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  56  the loss is =  0.01859024353325367  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  57  the loss is =  0.018580544739961624  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  58  the loss is =  0.018571915104985237  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  59  the loss is =  0.018564118072390556  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  60  the loss is =  0.018557168543338776  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  61  the loss is =  0.018550744280219078  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  62  the loss is =  0.018542468547821045  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  63  the loss is =  0.018537193536758423  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  64  the loss is =  0.01853509619832039  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  65  the loss is =  0.018533380702137947  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  66  the loss is =  0.018531909212470055  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  67  the loss is =  0.0185305904597044  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  68  the loss is =  0.018529368564486504  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  69  the loss is =  0.01852823607623577  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "56\n",
            "Validation Loss: 0.03839506208896637 Validation Accuracy : 0.388\n",
            "task = 30 \n",
            "train col =  [85 24 98 41 73 58 78 77 70 49]\n",
            "train col =  [[85 24 98 41 73 58 78 77 70 49]]\n",
            "At step  30  and at epoch =  0  the loss is =  0.0850476324558258  and accuracy is =  0.4542\n",
            "At step  30  and at epoch =  1  the loss is =  0.06273782253265381  and accuracy is =  0.6526\n",
            "At step  30  and at epoch =  2  the loss is =  0.05499251186847687  and accuracy is =  0.7432\n",
            "At step  30  and at epoch =  3  the loss is =  0.050175298005342484  and accuracy is =  0.824\n",
            "At step  30  and at epoch =  4  the loss is =  0.04510525241494179  and accuracy is =  0.8824\n",
            "At step  30  and at epoch =  5  the loss is =  0.04247233271598816  and accuracy is =  0.9122\n",
            "At step  30  and at epoch =  6  the loss is =  0.03919469937682152  and accuracy is =  0.9176\n",
            "At step  30  and at epoch =  7  the loss is =  0.03992852941155434  and accuracy is =  0.929\n",
            "At step  30  and at epoch =  8  the loss is =  0.037447862327098846  and accuracy is =  0.9532\n",
            "At step  30  and at epoch =  9  the loss is =  0.03408924117684364  and accuracy is =  0.9566\n",
            "At step  30  and at epoch =  10  the loss is =  0.03268688917160034  and accuracy is =  0.969\n",
            "At step  30  and at epoch =  11  the loss is =  0.03184074908494949  and accuracy is =  0.9834\n",
            "At step  30  and at epoch =  12  the loss is =  0.03131699189543724  and accuracy is =  0.9882\n",
            "At step  30  and at epoch =  13  the loss is =  0.034238219261169434  and accuracy is =  0.9916\n",
            "At step  30  and at epoch =  14  the loss is =  0.03384667634963989  and accuracy is =  0.9914\n",
            "At step  30  and at epoch =  15  the loss is =  0.033217769116163254  and accuracy is =  0.9918\n",
            "At step  30  and at epoch =  16  the loss is =  0.0316971018910408  and accuracy is =  0.996\n",
            "At step  30  and at epoch =  17  the loss is =  0.033398889005184174  and accuracy is =  0.9954\n",
            "At step  30  and at epoch =  18  the loss is =  0.035630304366350174  and accuracy is =  0.997\n",
            "At step  30  and at epoch =  19  the loss is =  0.032516416162252426  and accuracy is =  0.999\n",
            "At step  30  and at epoch =  20  the loss is =  0.030938822776079178  and accuracy is =  0.9982\n",
            "At step  30  and at epoch =  21  the loss is =  0.03015310689806938  and accuracy is =  0.9992\n",
            "At step  30  and at epoch =  22  the loss is =  0.029970189556479454  and accuracy is =  0.9984\n",
            "At step  30  and at epoch =  23  the loss is =  0.03002522885799408  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  24  the loss is =  0.029578451067209244  and accuracy is =  0.9992\n",
            "At step  30  and at epoch =  25  the loss is =  0.030344028025865555  and accuracy is =  0.9992\n",
            "At step  30  and at epoch =  26  the loss is =  0.030560821294784546  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  27  the loss is =  0.03068813495337963  and accuracy is =  0.999\n",
            "At step  30  and at epoch =  28  the loss is =  0.02964628115296364  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  29  the loss is =  0.028638238087296486  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  30  the loss is =  0.029368137940764427  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  31  the loss is =  0.028965702280402184  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  32  the loss is =  0.030499452725052834  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  33  the loss is =  0.030552241951227188  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  34  the loss is =  0.029665671288967133  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  35  the loss is =  0.028905760496854782  and accuracy is =  0.9998\n",
            "At step  30  and at epoch =  36  the loss is =  0.02902199700474739  and accuracy is =  0.9992\n",
            "At step  30  and at epoch =  37  the loss is =  0.02883218415081501  and accuracy is =  0.9946\n",
            "At step  30  and at epoch =  38  the loss is =  0.029133304953575134  and accuracy is =  0.9986\n",
            "At step  30  and at epoch =  39  the loss is =  0.029086899012327194  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  40  the loss is =  0.02926808036863804  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  41  the loss is =  0.028776420280337334  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  42  the loss is =  0.029056191444396973  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  43  the loss is =  0.028514722362160683  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  44  the loss is =  0.028531912714242935  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  45  the loss is =  0.028108401224017143  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  46  the loss is =  0.028443418443202972  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  47  the loss is =  0.028759242966771126  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  48  the loss is =  0.02784499153494835  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  49  the loss is =  0.02746252529323101  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  50  the loss is =  0.02737579122185707  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  51  the loss is =  0.0273310374468565  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  52  the loss is =  0.027296338230371475  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  53  the loss is =  0.027269095182418823  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  54  the loss is =  0.027246445417404175  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  55  the loss is =  0.02722722664475441  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  56  the loss is =  0.02721051685512066  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  57  the loss is =  0.027195286005735397  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  58  the loss is =  0.027181509882211685  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  59  the loss is =  0.02716892957687378  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  60  the loss is =  0.02715744636952877  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  61  the loss is =  0.027146803215146065  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  62  the loss is =  0.0271640345454216  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  63  the loss is =  0.02714880369603634  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  64  the loss is =  0.027144672349095345  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  65  the loss is =  0.027140600606799126  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  66  the loss is =  0.027137167751789093  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  67  the loss is =  0.027134107425808907  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  68  the loss is =  0.027131279930472374  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  69  the loss is =  0.02712864987552166  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "32\n",
            "Validation Loss: 0.04448901116847992 Validation Accuracy : 0.3135\n",
            "task = 40 \n",
            "train col =  [65 88 36 93 45 10 90 17 32 59]\n",
            "train col =  [[65 88 36 93 45 10 90 17 32 59]]\n",
            "At step  40  and at epoch =  0  the loss is =  0.0824543759226799  and accuracy is =  0.3766\n",
            "At step  40  and at epoch =  1  the loss is =  0.07238561660051346  and accuracy is =  0.5784\n",
            "At step  40  and at epoch =  2  the loss is =  0.0737762600183487  and accuracy is =  0.666\n",
            "At step  40  and at epoch =  3  the loss is =  0.06132196635007858  and accuracy is =  0.7376\n",
            "At step  40  and at epoch =  4  the loss is =  0.056512463837862015  and accuracy is =  0.8132\n",
            "At step  40  and at epoch =  5  the loss is =  0.058492016047239304  and accuracy is =  0.8552\n",
            "At step  40  and at epoch =  6  the loss is =  0.05776508152484894  and accuracy is =  0.8654\n",
            "At step  40  and at epoch =  7  the loss is =  0.05949480086565018  and accuracy is =  0.8988\n",
            "At step  40  and at epoch =  8  the loss is =  0.05704385042190552  and accuracy is =  0.9218\n",
            "At step  40  and at epoch =  9  the loss is =  0.0528525710105896  and accuracy is =  0.9382\n",
            "At step  40  and at epoch =  10  the loss is =  0.05120306834578514  and accuracy is =  0.949\n",
            "At step  40  and at epoch =  11  the loss is =  0.04798780009150505  and accuracy is =  0.9552\n",
            "At step  40  and at epoch =  12  the loss is =  0.048247743397951126  and accuracy is =  0.971\n",
            "At step  40  and at epoch =  13  the loss is =  0.050136763602495193  and accuracy is =  0.9792\n",
            "At step  40  and at epoch =  14  the loss is =  0.048263464123010635  and accuracy is =  0.989\n",
            "At step  40  and at epoch =  15  the loss is =  0.0483686737716198  and accuracy is =  0.9914\n",
            "At step  40  and at epoch =  16  the loss is =  0.04738028347492218  and accuracy is =  0.9966\n",
            "At step  40  and at epoch =  17  the loss is =  0.0477760024368763  and accuracy is =  0.996\n",
            "At step  40  and at epoch =  18  the loss is =  0.048229169100522995  and accuracy is =  0.9976\n",
            "At step  40  and at epoch =  19  the loss is =  0.04780866578221321  and accuracy is =  0.9972\n",
            "At step  40  and at epoch =  20  the loss is =  0.048402249813079834  and accuracy is =  0.9978\n",
            "At step  40  and at epoch =  21  the loss is =  0.04784888029098511  and accuracy is =  0.9986\n",
            "At step  40  and at epoch =  22  the loss is =  0.04733071103692055  and accuracy is =  0.9986\n",
            "At step  40  and at epoch =  23  the loss is =  0.0463433600962162  and accuracy is =  0.998\n",
            "At step  40  and at epoch =  24  the loss is =  0.046211156994104385  and accuracy is =  0.9992\n",
            "At step  40  and at epoch =  25  the loss is =  0.04585342854261398  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  26  the loss is =  0.04671774432063103  and accuracy is =  0.9996\n",
            "At step  40  and at epoch =  27  the loss is =  0.04752383008599281  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  28  the loss is =  0.046545810997486115  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  29  the loss is =  0.04508176073431969  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  30  the loss is =  0.04485277086496353  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  31  the loss is =  0.04494335874915123  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  32  the loss is =  0.044986095279455185  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  33  the loss is =  0.0455179400742054  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  34  the loss is =  0.0459972508251667  and accuracy is =  0.9996\n",
            "At step  40  and at epoch =  35  the loss is =  0.04772888496518135  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  36  the loss is =  0.046731431037187576  and accuracy is =  0.9996\n",
            "At step  40  and at epoch =  37  the loss is =  0.04555775597691536  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  38  the loss is =  0.044842757284641266  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  39  the loss is =  0.044655103236436844  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  40  the loss is =  0.045286793261766434  and accuracy is =  0.9996\n",
            "At step  40  and at epoch =  41  the loss is =  0.045401155948638916  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  42  the loss is =  0.04549243673682213  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  43  the loss is =  0.04520350322127342  and accuracy is =  0.9996\n",
            "At step  40  and at epoch =  44  the loss is =  0.04578346759080887  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  45  the loss is =  0.046272456645965576  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  46  the loss is =  0.046294551342725754  and accuracy is =  0.9992\n",
            "At step  40  and at epoch =  47  the loss is =  0.045249033719301224  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  48  the loss is =  0.04455181956291199  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  49  the loss is =  0.04383477568626404  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  50  the loss is =  0.043633896857500076  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  51  the loss is =  0.04353659227490425  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  52  the loss is =  0.04347580671310425  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  53  the loss is =  0.04343339800834656  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  54  the loss is =  0.043401531875133514  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  55  the loss is =  0.04337650164961815  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  56  the loss is =  0.04335625469684601  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  57  the loss is =  0.043339088559150696  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  58  the loss is =  0.043324049562215805  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  59  the loss is =  0.043311070650815964  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  60  the loss is =  0.04329954460263252  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  61  the loss is =  0.043289124965667725  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  62  the loss is =  0.04328024759888649  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  63  the loss is =  0.043268460780382156  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  64  the loss is =  0.04326312988996506  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  65  the loss is =  0.043258532881736755  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  66  the loss is =  0.04325477033853531  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  67  the loss is =  0.04325155168771744  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  68  the loss is =  0.043248701840639114  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  69  the loss is =  0.04324614256620407  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "8\n",
            "Validation Loss: 0.08041160553693771 Validation Accuracy : 0.2392\n",
            "task = 50 \n",
            "train col =  [83 43 53 11 86 19 38 30 40 50]\n",
            "train col =  [[83 43 53 11 86 19 38 30 40 50]]\n",
            "At step  50  and at epoch =  0  the loss is =  0.09513994306325912  and accuracy is =  0.4244\n",
            "At step  50  and at epoch =  1  the loss is =  0.08412463963031769  and accuracy is =  0.6122\n",
            "At step  50  and at epoch =  2  the loss is =  0.07189945131540298  and accuracy is =  0.6884\n",
            "At step  50  and at epoch =  3  the loss is =  0.0673188641667366  and accuracy is =  0.7658\n",
            "At step  50  and at epoch =  4  the loss is =  0.0648830384016037  and accuracy is =  0.8494\n",
            "At step  50  and at epoch =  5  the loss is =  0.0632583498954773  and accuracy is =  0.896\n",
            "At step  50  and at epoch =  6  the loss is =  0.061961691826581955  and accuracy is =  0.9052\n",
            "At step  50  and at epoch =  7  the loss is =  0.06183101609349251  and accuracy is =  0.9132\n",
            "At step  50  and at epoch =  8  the loss is =  0.06274594366550446  and accuracy is =  0.9286\n",
            "At step  50  and at epoch =  9  the loss is =  0.0597812719643116  and accuracy is =  0.9506\n",
            "At step  50  and at epoch =  10  the loss is =  0.05751664936542511  and accuracy is =  0.9608\n",
            "At step  50  and at epoch =  11  the loss is =  0.05753514543175697  and accuracy is =  0.978\n",
            "At step  50  and at epoch =  12  the loss is =  0.05789573863148689  and accuracy is =  0.987\n",
            "At step  50  and at epoch =  13  the loss is =  0.05821729823946953  and accuracy is =  0.9912\n",
            "At step  50  and at epoch =  14  the loss is =  0.05699523910880089  and accuracy is =  0.9936\n",
            "At step  50  and at epoch =  15  the loss is =  0.05718349665403366  and accuracy is =  0.9974\n",
            "At step  50  and at epoch =  16  the loss is =  0.056343983858823776  and accuracy is =  0.9976\n",
            "At step  50  and at epoch =  17  the loss is =  0.054737791419029236  and accuracy is =  0.9988\n",
            "At step  50  and at epoch =  18  the loss is =  0.0550849623978138  and accuracy is =  0.9992\n",
            "At step  50  and at epoch =  19  the loss is =  0.055060748010873795  and accuracy is =  0.9992\n",
            "At step  50  and at epoch =  20  the loss is =  0.055630870163440704  and accuracy is =  0.9994\n",
            "At step  50  and at epoch =  21  the loss is =  0.05521329864859581  and accuracy is =  0.9986\n",
            "At step  50  and at epoch =  22  the loss is =  0.05526740103960037  and accuracy is =  0.9992\n",
            "At step  50  and at epoch =  23  the loss is =  0.055302493274211884  and accuracy is =  0.9994\n",
            "At step  50  and at epoch =  24  the loss is =  0.05368029326200485  and accuracy is =  0.9996\n",
            "At step  50  and at epoch =  25  the loss is =  0.053675416857004166  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  26  the loss is =  0.05391969531774521  and accuracy is =  0.9994\n",
            "At step  50  and at epoch =  27  the loss is =  0.05465561896562576  and accuracy is =  0.9982\n",
            "At step  50  and at epoch =  28  the loss is =  0.054061125963926315  and accuracy is =  0.9988\n",
            "At step  50  and at epoch =  29  the loss is =  0.05375656113028526  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  30  the loss is =  0.05398368835449219  and accuracy is =  0.9996\n",
            "At step  50  and at epoch =  31  the loss is =  0.05383336916565895  and accuracy is =  0.9996\n",
            "At step  50  and at epoch =  32  the loss is =  0.053947918117046356  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  33  the loss is =  0.05340268462896347  and accuracy is =  0.9996\n",
            "At step  50  and at epoch =  34  the loss is =  0.05337752401828766  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  35  the loss is =  0.053694114089012146  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  36  the loss is =  0.05304587259888649  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  37  the loss is =  0.05339236930012703  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  38  the loss is =  0.05388139188289642  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  39  the loss is =  0.053767964243888855  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  40  the loss is =  0.05376797169446945  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  41  the loss is =  0.05404914170503616  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  42  the loss is =  0.05392079055309296  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  43  the loss is =  0.05329767242074013  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  44  the loss is =  0.05326470360159874  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  45  the loss is =  0.05320366844534874  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  46  the loss is =  0.053508490324020386  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  47  the loss is =  0.053529586642980576  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  48  the loss is =  0.0519988089799881  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  49  the loss is =  0.05158477649092674  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  50  the loss is =  0.05147816613316536  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  51  the loss is =  0.05140300095081329  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  52  the loss is =  0.05134785175323486  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  53  the loss is =  0.0513058565557003  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  54  the loss is =  0.051272258162498474  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  55  the loss is =  0.05124393478035927  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  56  the loss is =  0.05121905729174614  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  57  the loss is =  0.05119704082608223  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  58  the loss is =  0.05117715895175934  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  59  the loss is =  0.05115905776619911  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  60  the loss is =  0.05114218220114708  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  61  the loss is =  0.051126375794410706  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  62  the loss is =  0.05103803053498268  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  63  the loss is =  0.05103281885385513  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  64  the loss is =  0.05102642998099327  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  65  the loss is =  0.05102156102657318  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  66  the loss is =  0.05101754143834114  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  67  the loss is =  0.0510140024125576  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  68  the loss is =  0.05101078748703003  and accuracy is =  1.0\n",
            "At step  50  and at epoch =  69  the loss is =  0.051007840782403946  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "112\n",
            "Validation Loss: 0.05954022333025932 Validation Accuracy : 0.20683333333333334\n",
            "task = 60 \n",
            "train col =  [57 81 12 95 25 47 34 52 44 72]\n",
            "train col =  [[57 81 12 95 25 47 34 52 44 72]]\n",
            "At step  60  and at epoch =  0  the loss is =  0.10171176493167877  and accuracy is =  0.4846\n",
            "At step  60  and at epoch =  1  the loss is =  0.09673277288675308  and accuracy is =  0.6512\n",
            "At step  60  and at epoch =  2  the loss is =  0.09276624768972397  and accuracy is =  0.7292\n",
            "At step  60  and at epoch =  3  the loss is =  0.07955560833215714  and accuracy is =  0.7972\n",
            "At step  60  and at epoch =  4  the loss is =  0.07324875146150589  and accuracy is =  0.8722\n",
            "At step  60  and at epoch =  5  the loss is =  0.07370059192180634  and accuracy is =  0.9212\n",
            "At step  60  and at epoch =  6  the loss is =  0.07383959740400314  and accuracy is =  0.9288\n",
            "At step  60  and at epoch =  7  the loss is =  0.07542119920253754  and accuracy is =  0.9298\n",
            "At step  60  and at epoch =  8  the loss is =  0.07289542257785797  and accuracy is =  0.9316\n",
            "At step  60  and at epoch =  9  the loss is =  0.06599156558513641  and accuracy is =  0.9626\n",
            "At step  60  and at epoch =  10  the loss is =  0.063921719789505  and accuracy is =  0.9828\n",
            "At step  60  and at epoch =  11  the loss is =  0.0628858283162117  and accuracy is =  0.9914\n",
            "At step  60  and at epoch =  12  the loss is =  0.0650802031159401  and accuracy is =  0.9966\n",
            "At step  60  and at epoch =  13  the loss is =  0.0653972402215004  and accuracy is =  0.996\n",
            "At step  60  and at epoch =  14  the loss is =  0.06495492905378342  and accuracy is =  0.9974\n",
            "At step  60  and at epoch =  15  the loss is =  0.06272707134485245  and accuracy is =  0.9978\n",
            "At step  60  and at epoch =  16  the loss is =  0.06233355775475502  and accuracy is =  0.9994\n",
            "At step  60  and at epoch =  17  the loss is =  0.06153586879372597  and accuracy is =  0.9984\n",
            "At step  60  and at epoch =  18  the loss is =  0.06148586794734001  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  19  the loss is =  0.06146099045872688  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  20  the loss is =  0.06147313863039017  and accuracy is =  0.9996\n",
            "At step  60  and at epoch =  21  the loss is =  0.06180550530552864  and accuracy is =  0.9994\n",
            "At step  60  and at epoch =  22  the loss is =  0.06352514773607254  and accuracy is =  0.9996\n",
            "At step  60  and at epoch =  23  the loss is =  0.06258131563663483  and accuracy is =  0.9996\n",
            "At step  60  and at epoch =  24  the loss is =  0.061672523617744446  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  25  the loss is =  0.061577510088682175  and accuracy is =  0.9994\n",
            "At step  60  and at epoch =  26  the loss is =  0.06256544589996338  and accuracy is =  0.9986\n",
            "At step  60  and at epoch =  27  the loss is =  0.06418672204017639  and accuracy is =  0.997\n",
            "At step  60  and at epoch =  28  the loss is =  0.065067358314991  and accuracy is =  0.9962\n",
            "At step  60  and at epoch =  29  the loss is =  0.062278904020786285  and accuracy is =  0.9942\n",
            "At step  60  and at epoch =  30  the loss is =  0.06194620579481125  and accuracy is =  0.997\n",
            "At step  60  and at epoch =  31  the loss is =  0.06253300607204437  and accuracy is =  0.9976\n",
            "At step  60  and at epoch =  32  the loss is =  0.061048734933137894  and accuracy is =  0.9994\n",
            "At step  60  and at epoch =  33  the loss is =  0.060478270053863525  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  34  the loss is =  0.06079971045255661  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  35  the loss is =  0.060910265892744064  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  36  the loss is =  0.0614449642598629  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  37  the loss is =  0.06104125827550888  and accuracy is =  0.9996\n",
            "At step  60  and at epoch =  38  the loss is =  0.060761693865060806  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  39  the loss is =  0.06048694625496864  and accuracy is =  0.999\n",
            "At step  60  and at epoch =  40  the loss is =  0.0606888085603714  and accuracy is =  0.9972\n",
            "At step  60  and at epoch =  41  the loss is =  0.06104632094502449  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  42  the loss is =  0.06122330203652382  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  43  the loss is =  0.06059524416923523  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  44  the loss is =  0.06086662784218788  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  45  the loss is =  0.059891585260629654  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  46  the loss is =  0.05986073985695839  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  47  the loss is =  0.06002330780029297  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  48  the loss is =  0.05937239155173302  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  49  the loss is =  0.05917052552103996  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  50  the loss is =  0.05909886211156845  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  51  the loss is =  0.05906447768211365  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  52  the loss is =  0.05904005095362663  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  53  the loss is =  0.05902015417814255  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  54  the loss is =  0.05900363251566887  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  55  the loss is =  0.058989305049180984  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  56  the loss is =  0.058976911008358  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  57  the loss is =  0.058966100215911865  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  58  the loss is =  0.05895615369081497  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  59  the loss is =  0.05894707515835762  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  60  the loss is =  0.05893837288022041  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  61  the loss is =  0.05893033370375633  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  62  the loss is =  0.05886422097682953  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  63  the loss is =  0.05885778367519379  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  64  the loss is =  0.05885247141122818  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  65  the loss is =  0.05884874239563942  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  66  the loss is =  0.05884556472301483  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  67  the loss is =  0.05884284898638725  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  68  the loss is =  0.05884042754769325  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  69  the loss is =  0.05883823707699776  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "88\n",
            "Validation Loss: 0.06688849627971649 Validation Accuracy : 0.1717142857142857\n",
            "task = 70 \n",
            "train col =  [46 79 20 28  5 71  8 18 33 15]\n",
            "train col =  [[46 79 20 28  5 71  8 18 33 15]]\n",
            "At step  70  and at epoch =  0  the loss is =  0.1214347630739212  and accuracy is =  0.4934\n",
            "At step  70  and at epoch =  1  the loss is =  0.10835107415914536  and accuracy is =  0.6798\n",
            "At step  70  and at epoch =  2  the loss is =  0.10336662083864212  and accuracy is =  0.7746\n",
            "At step  70  and at epoch =  3  the loss is =  0.1067521944642067  and accuracy is =  0.8362\n",
            "At step  70  and at epoch =  4  the loss is =  0.09854832291603088  and accuracy is =  0.8844\n",
            "At step  70  and at epoch =  5  the loss is =  0.0913369357585907  and accuracy is =  0.9228\n",
            "At step  70  and at epoch =  6  the loss is =  0.09203902631998062  and accuracy is =  0.946\n",
            "At step  70  and at epoch =  7  the loss is =  0.09605087339878082  and accuracy is =  0.961\n",
            "At step  70  and at epoch =  8  the loss is =  0.0876295417547226  and accuracy is =  0.957\n",
            "At step  70  and at epoch =  9  the loss is =  0.08524272590875626  and accuracy is =  0.9718\n",
            "At step  70  and at epoch =  10  the loss is =  0.0835859626531601  and accuracy is =  0.9866\n",
            "At step  70  and at epoch =  11  the loss is =  0.08277285099029541  and accuracy is =  0.991\n",
            "At step  70  and at epoch =  12  the loss is =  0.08393972367048264  and accuracy is =  0.9938\n",
            "At step  70  and at epoch =  13  the loss is =  0.08674941956996918  and accuracy is =  0.9942\n",
            "At step  70  and at epoch =  14  the loss is =  0.08602277934551239  and accuracy is =  0.9946\n",
            "At step  70  and at epoch =  15  the loss is =  0.08184225112199783  and accuracy is =  0.9962\n",
            "At step  70  and at epoch =  16  the loss is =  0.08095742762088776  and accuracy is =  0.9992\n",
            "At step  70  and at epoch =  17  the loss is =  0.08118751645088196  and accuracy is =  0.9992\n",
            "At step  70  and at epoch =  18  the loss is =  0.08150621503591537  and accuracy is =  0.999\n",
            "At step  70  and at epoch =  19  the loss is =  0.08136869221925735  and accuracy is =  0.9982\n",
            "At step  70  and at epoch =  20  the loss is =  0.08082307875156403  and accuracy is =  0.999\n",
            "At step  70  and at epoch =  21  the loss is =  0.08009005337953568  and accuracy is =  0.9996\n",
            "At step  70  and at epoch =  22  the loss is =  0.08036058396100998  and accuracy is =  0.9984\n",
            "At step  70  and at epoch =  23  the loss is =  0.079921655356884  and accuracy is =  0.999\n",
            "At step  70  and at epoch =  24  the loss is =  0.08041328191757202  and accuracy is =  0.9994\n",
            "At step  70  and at epoch =  25  the loss is =  0.08077216148376465  and accuracy is =  0.9996\n",
            "At step  70  and at epoch =  26  the loss is =  0.08110139518976212  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  27  the loss is =  0.08100958913564682  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  28  the loss is =  0.08086954057216644  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  29  the loss is =  0.08041860163211823  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  30  the loss is =  0.07998770475387573  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  31  the loss is =  0.07897946238517761  and accuracy is =  0.9992\n",
            "At step  70  and at epoch =  32  the loss is =  0.07886670529842377  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  33  the loss is =  0.08016838133335114  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  34  the loss is =  0.0796864777803421  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  35  the loss is =  0.07945603877305984  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  36  the loss is =  0.07860132306814194  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  37  the loss is =  0.07950714230537415  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  38  the loss is =  0.08009065687656403  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  39  the loss is =  0.08367660641670227  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  40  the loss is =  0.08289910852909088  and accuracy is =  0.9992\n",
            "At step  70  and at epoch =  41  the loss is =  0.07971649616956711  and accuracy is =  0.9994\n",
            "At step  70  and at epoch =  42  the loss is =  0.07877878844738007  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  43  the loss is =  0.07874603569507599  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  44  the loss is =  0.07859506458044052  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  45  the loss is =  0.07853502780199051  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  46  the loss is =  0.07877010852098465  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  47  the loss is =  0.07859985530376434  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  48  the loss is =  0.07767266780138016  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  49  the loss is =  0.07730081677436829  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  50  the loss is =  0.07716551423072815  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  51  the loss is =  0.0770966112613678  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  52  the loss is =  0.07704635709524155  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  53  the loss is =  0.07700885832309723  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  54  the loss is =  0.07697919756174088  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  55  the loss is =  0.07695470750331879  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  56  the loss is =  0.07693342864513397  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  57  the loss is =  0.07691439241170883  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  58  the loss is =  0.07689756900072098  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  59  the loss is =  0.07688203454017639  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  60  the loss is =  0.07686782628297806  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  61  the loss is =  0.0768546313047409  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  62  the loss is =  0.07693057507276535  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  63  the loss is =  0.07690481096506119  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  64  the loss is =  0.07689731568098068  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  65  the loss is =  0.07689016312360764  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  66  the loss is =  0.07688403874635696  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  67  the loss is =  0.07687859237194061  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  68  the loss is =  0.07687363028526306  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  69  the loss is =  0.07686904817819595  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "64\n",
            "Validation Loss: 0.06546439230442047 Validation Accuracy : 0.1655\n",
            "task = 80 \n",
            "train col =  [55 29 64 31 67  7 13 14 42  6]\n",
            "train col =  [[55 29 64 31 67  7 13 14 42  6]]\n",
            "At step  80  and at epoch =  0  the loss is =  0.1426357477903366  and accuracy is =  0.3726\n",
            "At step  80  and at epoch =  1  the loss is =  0.13483594357967377  and accuracy is =  0.564\n",
            "At step  80  and at epoch =  2  the loss is =  0.12949690222740173  and accuracy is =  0.6546\n",
            "At step  80  and at epoch =  3  the loss is =  0.12661190330982208  and accuracy is =  0.7294\n",
            "At step  80  and at epoch =  4  the loss is =  0.11894174665212631  and accuracy is =  0.8044\n",
            "At step  80  and at epoch =  5  the loss is =  0.1165740117430687  and accuracy is =  0.867\n",
            "At step  80  and at epoch =  6  the loss is =  0.11852174252271652  and accuracy is =  0.9038\n",
            "At step  80  and at epoch =  7  the loss is =  0.11258821934461594  and accuracy is =  0.9164\n",
            "At step  80  and at epoch =  8  the loss is =  0.11332648992538452  and accuracy is =  0.933\n",
            "At step  80  and at epoch =  9  the loss is =  0.1158854067325592  and accuracy is =  0.947\n",
            "At step  80  and at epoch =  10  the loss is =  0.11045417189598083  and accuracy is =  0.9568\n",
            "At step  80  and at epoch =  11  the loss is =  0.10691577941179276  and accuracy is =  0.9704\n",
            "At step  80  and at epoch =  12  the loss is =  0.10567062348127365  and accuracy is =  0.984\n",
            "At step  80  and at epoch =  13  the loss is =  0.10547032207250595  and accuracy is =  0.989\n",
            "At step  80  and at epoch =  14  the loss is =  0.10487039387226105  and accuracy is =  0.9958\n",
            "At step  80  and at epoch =  15  the loss is =  0.10404641926288605  and accuracy is =  0.9984\n",
            "At step  80  and at epoch =  16  the loss is =  0.10296722501516342  and accuracy is =  0.9988\n",
            "At step  80  and at epoch =  17  the loss is =  0.10176465660333633  and accuracy is =  0.9994\n",
            "At step  80  and at epoch =  18  the loss is =  0.10183922201395035  and accuracy is =  0.9996\n",
            "At step  80  and at epoch =  19  the loss is =  0.10267998278141022  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  20  the loss is =  0.10146340727806091  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  21  the loss is =  0.10094702243804932  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  22  the loss is =  0.10130882263183594  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  23  the loss is =  0.10172156244516373  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  24  the loss is =  0.10037669539451599  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  25  the loss is =  0.09965028613805771  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  26  the loss is =  0.09979122132062912  and accuracy is =  0.9996\n",
            "At step  80  and at epoch =  27  the loss is =  0.10001914948225021  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  28  the loss is =  0.1007080078125  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  29  the loss is =  0.10040716081857681  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  30  the loss is =  0.10155092179775238  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  31  the loss is =  0.10082507133483887  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  32  the loss is =  0.10041744261980057  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  33  the loss is =  0.09960579872131348  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  34  the loss is =  0.09991756081581116  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  35  the loss is =  0.1003178283572197  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  36  the loss is =  0.0998983085155487  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  37  the loss is =  0.0999789610505104  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  38  the loss is =  0.1000453382730484  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  39  the loss is =  0.09982013702392578  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  40  the loss is =  0.09989374876022339  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  41  the loss is =  0.09998244792222977  and accuracy is =  0.9988\n",
            "At step  80  and at epoch =  42  the loss is =  0.10072879493236542  and accuracy is =  0.9996\n",
            "At step  80  and at epoch =  43  the loss is =  0.10094014555215836  and accuracy is =  0.9988\n",
            "At step  80  and at epoch =  44  the loss is =  0.10172105580568314  and accuracy is =  0.9986\n",
            "At step  80  and at epoch =  45  the loss is =  0.09971334040164948  and accuracy is =  0.9978\n",
            "At step  80  and at epoch =  46  the loss is =  0.09934891760349274  and accuracy is =  0.9942\n",
            "At step  80  and at epoch =  47  the loss is =  0.1000281274318695  and accuracy is =  0.9888\n",
            "At step  80  and at epoch =  48  the loss is =  0.09855816513299942  and accuracy is =  0.9924\n",
            "At step  80  and at epoch =  49  the loss is =  0.09799381345510483  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  50  the loss is =  0.09770051389932632  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  51  the loss is =  0.09757380932569504  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  52  the loss is =  0.09748746454715729  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  53  the loss is =  0.09742221981287003  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  54  the loss is =  0.0973680317401886  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  55  the loss is =  0.09732472896575928  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  56  the loss is =  0.09728700667619705  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  57  the loss is =  0.09725367277860641  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  58  the loss is =  0.09722258150577545  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  59  the loss is =  0.09719478338956833  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  60  the loss is =  0.0971691757440567  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  61  the loss is =  0.09714563190937042  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  62  the loss is =  0.09705861657857895  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  63  the loss is =  0.09704165905714035  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  64  the loss is =  0.0970335379242897  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  65  the loss is =  0.09702610969543457  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  66  the loss is =  0.097019724547863  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  67  the loss is =  0.09701396524906158  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  68  the loss is =  0.097008615732193  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  69  the loss is =  0.09700363129377365  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "40\n",
            "Validation Loss: 0.08445140719413757 Validation Accuracy : 0.14166666666666666\n",
            "task = 90 \n",
            "train col =  [82  2 27 16 26  3  4  1  9  0]\n",
            "train col =  [[82  2 27 16 26  3  4  1  9  0]]\n",
            "At step  90  and at epoch =  0  the loss is =  0.15001945197582245  and accuracy is =  0.4682\n",
            "At step  90  and at epoch =  1  the loss is =  0.1485878974199295  and accuracy is =  0.6484\n",
            "At step  90  and at epoch =  2  the loss is =  0.13500764966011047  and accuracy is =  0.7144\n",
            "At step  90  and at epoch =  3  the loss is =  0.12368819117546082  and accuracy is =  0.7896\n",
            "At step  90  and at epoch =  4  the loss is =  0.12300626188516617  and accuracy is =  0.8634\n",
            "At step  90  and at epoch =  5  the loss is =  0.1255919188261032  and accuracy is =  0.9126\n",
            "At step  90  and at epoch =  6  the loss is =  0.1231437474489212  and accuracy is =  0.9254\n",
            "At step  90  and at epoch =  7  the loss is =  0.11964792758226395  and accuracy is =  0.943\n",
            "At step  90  and at epoch =  8  the loss is =  0.11834442615509033  and accuracy is =  0.963\n",
            "At step  90  and at epoch =  9  the loss is =  0.11402692645788193  and accuracy is =  0.9738\n",
            "At step  90  and at epoch =  10  the loss is =  0.11303029954433441  and accuracy is =  0.9804\n",
            "At step  90  and at epoch =  11  the loss is =  0.1128273531794548  and accuracy is =  0.99\n",
            "At step  90  and at epoch =  12  the loss is =  0.11502818018198013  and accuracy is =  0.9904\n",
            "At step  90  and at epoch =  13  the loss is =  0.11509136110544205  and accuracy is =  0.996\n",
            "At step  90  and at epoch =  14  the loss is =  0.11413286626338959  and accuracy is =  0.9954\n",
            "At step  90  and at epoch =  15  the loss is =  0.1114707887172699  and accuracy is =  0.9964\n",
            "At step  90  and at epoch =  16  the loss is =  0.11049949377775192  and accuracy is =  0.9978\n",
            "At step  90  and at epoch =  17  the loss is =  0.10984445363283157  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  18  the loss is =  0.1102069839835167  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  19  the loss is =  0.10982639342546463  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  20  the loss is =  0.1099654957652092  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  21  the loss is =  0.11060905456542969  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  22  the loss is =  0.11156383901834488  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  23  the loss is =  0.10916569828987122  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  24  the loss is =  0.10861959308385849  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  25  the loss is =  0.10852706432342529  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  26  the loss is =  0.10910403728485107  and accuracy is =  0.9996\n",
            "At step  90  and at epoch =  27  the loss is =  0.11001172661781311  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  28  the loss is =  0.1090259924530983  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  29  the loss is =  0.10958308726549149  and accuracy is =  0.9996\n",
            "At step  90  and at epoch =  30  the loss is =  0.10942652821540833  and accuracy is =  0.9996\n",
            "At step  90  and at epoch =  31  the loss is =  0.10905483365058899  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  32  the loss is =  0.10903900116682053  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  33  the loss is =  0.10894306749105453  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  34  the loss is =  0.10830450803041458  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  35  the loss is =  0.10875114053487778  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  36  the loss is =  0.10960544645786285  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  37  the loss is =  0.11034107208251953  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  38  the loss is =  0.11007552593946457  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  39  the loss is =  0.10975729674100876  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  40  the loss is =  0.10926127433776855  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  41  the loss is =  0.1097019761800766  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  42  the loss is =  0.10957208275794983  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  43  the loss is =  0.1090153306722641  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  44  the loss is =  0.10850028693675995  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  45  the loss is =  0.10824571549892426  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  46  the loss is =  0.10755513608455658  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  47  the loss is =  0.10713933408260345  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  48  the loss is =  0.10670918971300125  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  49  the loss is =  0.10625158995389938  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  50  the loss is =  0.10610608011484146  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  51  the loss is =  0.10603337734937668  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  52  the loss is =  0.105985127389431  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  53  the loss is =  0.10594972968101501  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  54  the loss is =  0.1059231162071228  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  55  the loss is =  0.10590087622404099  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  56  the loss is =  0.10588249564170837  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  57  the loss is =  0.10586604475975037  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  58  the loss is =  0.10585135966539383  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  59  the loss is =  0.1058381050825119  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  60  the loss is =  0.10582557320594788  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  61  the loss is =  0.1058138832449913  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  62  the loss is =  0.10572372376918793  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  63  the loss is =  0.10570292174816132  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  64  the loss is =  0.10569211095571518  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  65  the loss is =  0.10568410903215408  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  66  the loss is =  0.10567781329154968  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  67  the loss is =  0.1056726723909378  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  68  the loss is =  0.10566817969083786  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  69  the loss is =  0.10566426813602448  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "16\n",
            "Validation Loss: 0.09787975251674652 Validation Accuracy : 0.1382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptf2qZjFbWNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "2881ca90-fcd0-4bab-e41d-f3ae21f2ab44"
      },
      "source": [
        "plotTask(pars_tasks)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxU5dn/8c+VyUZ2kkBYAiTshB3CIqCEVgvUhfqgVYtofVTa+qDWtfRpa62/to9bq6LWSl1rK2itVVQUF4grsikuEJCENew7CRCyXb8/zkkyiRMywCSTTK736zWvmbPMmWtuhu+c3OfMfURVMcYY0/KFBbsAY4wxgWGBbowxIcIC3RhjQoQFujHGhAgLdGOMCREW6MYYEyIs0I0JQSKiItIz2HWYpmWBbk6KiOSKyAERiQp2LaFGRJ4Rkd8Huw7TclmgG7+JSAZwJqDABU382uFN+XqNLdTej2keLNDNybgC+BR4BrjSe4GIdBGRl0Vkj4jsE5FHvJZdKyJ5IlIkImtEZJg7v1a3gPceqojkiEihiPxCRHYCT4tIWxF53X2NA+7jdK/nJ4vI0yKy3V3+ijv/axE532u9CBHZKyJDfb1Jt958EdkvIvNFpJM7/zERub/Ouq+KyM3u404i8m+3vo0icoPXeneKyEsi8g8ROQz8uM52ZgDTgNtFpFhEXnPnzxKRAq+2u9DrOT1F5H0ROeS+nxfqeT/jRGSr26YiIg+IyG4ROSwiX4nIAF/PMy2QqtrNbn7dgHzgOmA4UAakufM9wBfAA0AsEA2Mc5ddDGwDRgAC9AS6ucsU6Om1/WeA37uPc4By4B4gCmgDpABTgRggHvgX8IrX898AXgDaAhHAeHf+7cALXutNAb6q5z1+B9gLDHNf92HgA3fZWcBWQNzptsAxoBPOztFK4A4gEugObAAmuuve6bbZD9x12/h47er37zXvYq/tXwIcATq6y+YCv3KXVbe5d9sCk9yaR7rzJ7p1Jrn/Hv2qtme3ln8LegF2axk3YJwbSKnu9FrgJvfxGcAeINzH8xYCN9azzYYCvRSIPkFNQ4AD7uOOQCXQ1sd6nYAiIMGdfgm4vZ5tPgnc6zUd577vDDcAtwBnucuuBRa5j0cBW+ps65fA0+7jO6u+GE7wfr4V6D7WWQVMcR//HZgDpNfTtr8ENgMDvOZ/B/gGGA2EBftzZbfA3qzLxfjrSuBtVd3rTj9PTbdLF2Czqpb7eF4XoOAUX3OPqpZUTYhIjIg8LiKb3W6LD4AkEfG4r7NfVQ/U3Yiqbgc+BqaKSBIwGfhnPa/ZCScEq55bDOwDOquTiPOAy9zFP/LaTjegk4gcrLoB/wukeW1760m+f0TkChFZ5bXNAUCqu/h2nC+ZZSKyWkT+u87Tfw68qKpfe72fRcAjwKPAbhGZIyIJJ1uXaZ7swIxpkIi0AX4IeNz+bHC6I5JEZDBOUHUVkXAfob4V6FHPpo/idJ9U6QAUek3XHQr0FqAPMEpVd4rIEOBznFDbCiSLSJKqHvTxWs8C1+B85peo6rZ6atqOE84AiEgsTldP1fpzgbdF5G6cvfKqPu2twEZV7VXPdn29nxMuF5FuwN+A77o1V4jIKpz3i6ruxPkrAREZB7wrIh+oar67iYuBJ0WkUFUfqn4R1dnAbBFpD7wI3Ab8poHaTAtge+jGHz8AKoAsnG6OITh9rx/iHChdBuwA7haRWBGJFpGx7nOfAG4VkeHuAbmeblCB033wIxHxiMgkYHwDdcTj9FkfFJFk4LdVC1R1B/Am8Bf34GmEiJzl9dxXcPrFb8TpqqjPXOAqERkizqmZfwSWquom93U+x+ljfwJY6PXlsQwocg/itnHf0wARGdHAe/K2C6fvvUosTsjvARCRq3D20HGnL/Y6KHzAXbfS6/nbcb4MbhSRn7nPGSEio0QkAqc/vqTOc0wLZoFu/HElTl/wFlXdWXXD+dN9Gs4e4/k4B+G24OxlXwKgqv8C/oDTRVOEE6zJ7nZvdJ930N3OKw3U8SDOwdG9OGfbvFVn+XSc/u61wG6cLgfcOo4B/wYygZfrewFVfRdnb/XfOF9SPYBL66z2PHC2e1/1vArgPJwvu43UhH5iA+/J25NAltu98oqqrgH+BCzBCfuBOF1HVUYAS0WkGJiPc6xiQ533swUn1GeJyDVAAs5e/wGcrqV9wH0nUaNpxqqO1hsT8kTkDqC3ql4e7FqMaQzWh25aBbeL5mqcvXhjQpJ1uZiQJyLX4hy0fFNVPwh2PcY0FutyMcaYEGF76MYYEyKC1oeempqqGRkZwXr5gDhy5AixsbHBLqPZsPaoYW1Rm7VHbafTHitXrtyrqu18LQtaoGdkZLBixYpgvXxA5ObmkpOTE+wymg1rjxrWFrVZe9R2Ou0hIpvrW2ZdLsYYEyIs0I0xJkRYoBtjTIjwqw/dHWfjIZxxr59Q1bvrLO+KM/hRkrvOLFVdEOBajTEtRFlZGYWFhZSUOINlJiYmkpeXF+Sqmg9/2iM6Opr09HQiIiL83m6Dge4OTfoocA7OGB3LRWS+O85ElV/jDNP5mIhkAQtwxo82xrRChYWFxMfHk5GRgYhQVFREfHx8sMtqNhpqD1Vl3759FBYWkpmZ6fd2/elyGQnkq+oGVS3FGQ96St3Xxxn0B5zBiLb7XYExJuSUlJSQkpKCiAS7lBZJREhJSan+C8df/nS5dKb2wPyFOONAe7sTZ4zo63GG/Dy7niJnADMA0tLSyM3NPalim5vi4uIW/x4CydqjRmtvi8TERIqLi6unKyoqKCoqCmJFzYu/7VFSUnJSn6NAnYd+GfCMqv5JRM4AnhORAapaa5xlVZ2Dc8kssrOz9VTOw1yz/TCvfbmd2yf2Cfq3v51bW5u1R43W3hZ5eXm1uhSsy6U2f9sjOjqaoUN9XsvcJ3+6XLbhXN6rSjo1V2+pcjXOlU9Q1SU4F6xNpREs27iPx3ILWLR2d2Ns3hgTQl555RVEhLVr1wa7lCbhT6AvB3qJSKaIROIM9j+/zjpVg+gjIv1wAn1PIAutMm10N7qnxvLHBXmUVdiFVowx9Zs7dy7jxo1j7ty5jfYaFRUVjbbtk9VgoLvXiJyJc/X2PJyzWVaLyF0icoG72i3AtSLyBc4lvH6sjTSMY4QnjFmT+1Kw5wjzlm1pjJcwxoSA4uJiPvroI5588knmzZsHOOF76623MmDAAAYNGsTDDz8MwPLlyxkzZgyDBw9m5MiRFBUV8cwzzzBz5szq7Z133nnV/dlxcXHccsstDB48mCVLlnDXXXcxYsQIBgwYwIwZM6iKv/z8fM4++2wGDx7MsGHDKCgo4IorruD111+v3u60adN49dVXA/Ke/epDd88pX1Bn3h1ej9cAY+s+r7Gck5XG6O7JPPDueqYM7UxCtP/naRpjmtbvXlvNV1sP4PF4ArbNrE4J/Pb8/idc59VXX2XSpEn07t2blJQUVq5cybJly9i0aROrVq0iPDyc/fv3U1payiWXXMILL7zAiBEjOHz4MG3atDnhto8cOcKoUaP405/+5NSTlcUddziROH36dF5//XXOP/98pk2bxqxZs7jwwgspKSmhsrKSq6++mvvuu4/LLruMQ4cO8cknn/Dss88GpF1a5C9FRYRfn5vFgaOlPLo4v+EnGGNanblz53Lppc7lYC+99FLmzp3Lu+++y09+8hPCw5192eTkZNatW0fHjh0ZMcK5nndCQkL18vp4PB6mTp1aPb148WJGjRrFwIEDWbRoEatXr6aoqIht27Zx4YUXAs4BzpiYGMaPH09BQQF79uxh7ty5TJ06tcHX81eLvQTdgM6JXDi0M09/tInLR3WjS3JMsEsyxvjw2/P7N/lZLvv372fRokV89dVXiAgVFRWISHVo+yM8PJzKyprjdN7nhEdHR1f/xVFSUsJ1113HihUr6NKlC3feeWeD549fdtll/OMf/2DevHk8/fTTJ/nu6tci99Cr3DaxD2FhcO/CdcEuxRjTjLz00ktMnz6dzZs3s2nTJrZu3UpmZiaDBw/m8ccfp7y8HHCCv0+fPuzYsYPly5cDzimF5eXlZGRksGrVKiorK9m6dSvLli3z+VpV4Z2amkpxcTEvvfQSAPHx8aSnp/PKK68AcPz4cY4ePQo4/eYPPvgg4HTXBEqLDvSOiW2YcWZ3XvtiO59tORDscowxzcTcuXOruzqqTJ06lR07dtC1a1cGDRrE4MGDef7554mMjOSFF17g+uuvZ/DgwZxzzjmUlJQwduxYMjMzycrK4oYbbmDYsGE+XyspKYlrr72WAQMGMHHixFp/BTz33HPMnj2bQYMGMWbMGHbu3AlA+/bt6devH1dddVVA33fQrimanZ2tgbjAxZHj5eTcn0uXtm3498/GNOmPjVr7j0fqsvao0drbIi8vj379+lVP2w+Latu1axdjxozhs88+IzExsd716rYjgIisVNVsX+u36D10gNiocG45pzefbTnIgq92BrscY4w5oXfffZcRI0Zw/fXXnzDMT0WLPSjq7eLsLjzzySbufiuPs7PaExUeuNOjjDEmkM4++2xWr17dKH+xtPg9dABPmPCrc/uxdf8xnv1kU7DLMcaYoAiJQAc4s1c7cvq04+FF+ew/UhrscowxpsmFTKAD/O/3+3HkeDmz31sf7FKMMabJhVSg906L57KRXfnHp5sp2FPc8BOMMSaEhFSgA9x0Tm+iIzz834LWMVymMca3uLi4YJfQ5EIu0FPjovhZTg/ezdvFkoJ9wS7HGGOaTMgFOsDV4zLpnNSG37+xhsrK4PxwyhjT/KxatYrRo0czaNAgLrzwQg4ccH5hPnv2bLKyshg0aFD1gF7vv/8+Q4YMYciQIQwdOrRFXEIvJM5Drys6wsPtk/pw47xVvPz5Ni4anh7skoxpvd6cRZttn4MngHHTYSBMvvukn3bFFVfw8MMPM378eO644w5+97vf8eCDD3L33XezceNGoqKiOHjwIAD3338/jz76KGPHjqW4uJjo6OjA1d9IQnIPHeD8QZ0YnJ7I/QvXcay0+VxRxBgTHIcOHeLgwYOMHz8egCuvvJIPPvgAgEGDBjFt2jT+8Y9/VA9lO3bsWG6++WZmz57NwYMHAzbEbWNq/hWeorAw4dfnZXHxX5fwtw83cMN3ewW7JGNap8l3c6yZj+Xyxhtv8MEHH/Daa6/xhz/8ga+++opZs2Zx7rnnsmDBAsaOHcvChQvp27dvsEs9Ib/20EVkkoisE5F8EZnlY/kDIrLKvX0jIgcDX+rJG5GRzOQBHfjr+wXsPnzi8YmNMaEtMTGRtm3b8uGHHwLOSIjjx4+vHh53woQJ3HPPPRw6dIji4mIKCgoYOHAgv/jFLxgxYkSLuNB0g3voIuIBHgXOAQqB5SIy373sHACqepPX+tcDQxuh1lMya3Jf3s3bxZ/e/oZ7LhoU7HKMMU3k6NGjpKfXHD+7+eabefbZZ/npT3/K0aNH6d69O08//TQVFRVcfvnlHDp0CFXlhhtuICkpid/85jcsXryYsLAw+vfvz+TJk4P4bvzjT5fLSCBfVTcAiMg8YAqwpp71LwN+G5jyTl+3lFiuOCODpz7eyI/HZtCvY0KwSzLGNAHvqw15+/TTT78176OPPvrWvKoLSLck/gR6Z2Cr13QhMMrXiiLSDcgEFtWzfAYwAyAtLa36CtqNbWikEhMOt/7jY27Njg7YmOnFxcVN9h5aAmuPGq29LRITE2ud5ldRUdEiTvtrKv62R0lJyUl9jgJ9UPRS4CVV9XlaiarOAeaAc4GLprwAwO6Yjdz1+hro1J+cPu0Dss3WfhGDuqw9arT2tsjLy6t1ENQucFGbv+0RHR3N0KH+92D7c1B0G9DFazrdnefLpcBcv1+9CV0+uhsZKTH84Y08yit8/ylmjAmcYF0NLVScSvv5E+jLgV4ikikikTihPb/uSiLSF2gLLDnpKppAZHgYsyb3I393MfOWb234CcaYUxYdHc2+ffss1E+RqrJv376T/jFTg10uqlouIjOBhYAHeEpVV4vIXcAKVa0K90uBedqM/wUn9k9jZEYyD7zzDVOGdCI+OiLYJRkTktLT0yksLGTPnj2A0xfcEn5p2VT8aY/o6OhaZ+n4w68+dFVdACyoM++OOtN3ntQrB4GI8Ovz+nHBIx/zl9wCfjGpef9IwJiWKiIigszMzOrp3Nzck+oLDnWN1R4h+9P/+gxKT+LCoZ158qONFB44GuxyjDEmYFpdoAPcNrEPAty3cF2wSzHGmIBplYHeKakN15yZyaurtrNqa7MYpcAYY05bqwx0gJ/l9CQ1LpLfv77GjsQbY0JCqw30uKhwbj6nDys2H+Ctr3cGuxxjjDltrTbQAX6YnU7vtDjufmstpeX2YyNjTMvWqgM93BPGr87NYvO+o/x9yaZgl2OMMaelVQc6wPje7Tirdztmv7eeA0dKg12OMcacslYf6AC/+n4/io+XM3vR+mCXYowxp8wCHejTIZ5LRnThuSWb2bj3SLDLMcaYU2KB7rrpnN5EhYdx95t5wS7FGGNOiQW6q318ND/L6cHC1bv4dMO+YJdjjDEnzQLdy9XjutMxMZo/vJFHZaX92MgY07JYoHtpE+nhtol9+GrbIV79or5reBhjTPNkgV7HD4Z0ZmDnRO59ax3HSn1eSc8YY5olC/Q6wsKEX5/bjx2HSnjyow3BLscYY/zmV6CLyCQRWSci+SIyq551figia0RktYg8H9gym9ao7il8LyuNx3IL2F1UEuxyjDHGLw0Guoh4gEeByUAWcJmIZNVZpxfwS2CsqvYHft4ItTapWZP7cry8kgfesR8bGWNaBn/20EcC+aq6QVVLgXnAlDrrXAs8qqoHAFR1d2DLbHrd28Ux/YxuvLB8C+t2FgW7HGOMaZA0NBa4iFwETFLVa9zp6cAoVZ3ptc4rwDfAWJwLSd+pqm/52NYMYAZAWlra8Hnz5gXqfTSK4lLl9g+O0j3Jw63Z376ga3FxMXFxcUGorHmy9qhhbVGbtUdtp9MeEyZMWKmq2b6W+XWRaD+EA72AHCAd+EBEBqpqrcsBqeocYA5Adna25uTkBOjlG8/ONhv4/Rt5SKf+jO/drtay3NxcWsJ7aCrWHjWsLWqz9qitsdrDny6XbUAXr+l0d563QmC+qpap6kacvfVegSkxuKaf0Y1uKTH84Y01lFfYmOnGmObLn0BfDvQSkUwRiQQuBebXWecVnL1zRCQV6A2ExDl/UeEeZk3qyze7inlxRWGwyzHGmHo1GOiqWg7MBBYCecCLqrpaRO4SkQvc1RYC+0RkDbAYuE1VQ2ZAlEkDOjAioy1/fmcdxcfLg12OMcb45Nd56Kq6QFV7q2oPVf2DO+8OVZ3vPlZVvVlVs1R1oKo276OdJ0lE+NW5WewtLuWvuQXBLscYY3yyX4r6aUiXJKYM6cTfPtzA9oPHgl2OMcZ8iwX6SbhtYh8UuG/humCXYowx32KBfhLS28Zw9bhM/vP5Nr4sPNjwE4wxpglZoJ+k63J6kBIbye/fyKOhH2UZY0xTskA/SfHREdx0Tm+WbdzPZ7tteF1jTPNhgX4KLh3RhV7t43g+r5SDR0uDXY4xxgAW6Kck3BPGPRcN4uBx5fq5n1Nhl6szxjQDFuinaFjXtkzPiuTD9Xu5/20768UYE3yBGpyrVcrpEsHx2A48llvAgE6JnDuoY7BLMsa0YraHfpruvCCLYV2TuO2lL2zcdGNMUFmgn6aocA+PXT6c2KhwZjy3gkNHy4JdkjGmlbJAD4C0hGgemzaM7QePceMLdpDUGBMcFugBkp2RzG/P70/uuj088M43wS7HGNMKWaAH0LRRXbkkuwuPLM7nra93BLscY0wrY4EeQCLC76b0Z3CXJG558QvW77KDpMaYpmOBHmDRER4ev3w4bSLDmfHcSg4ds4Okxpim4Vegi8gkEVknIvkiMsvH8h+LyB4RWeXergl8qS1Hh8Ro/jJtGFv3H+WmF1ZRaQdJjTFNoMFAFxEP8CgwGcgCLhORLB+rvqCqQ9zbEwGus8UZmZnMHednsWjtbh58b32wyzHGtAL+7KGPBPJVdYOqlgLzgCmNW1ZomD66GxcNT2f2e+t5e/XOYJdjjAlx/gR6Z2Cr13ShO6+uqSLypYi8JCJdAlJdCyci/P4HAxiUnsjNL35B/u7iYJdkjAlh0tBFGkTkImCSql7jTk8HRqnqTK91UoBiVT0uIj8BLlHV7/jY1gxgBkBaWtrwefNa9rWki4uLiYuLa3C9fccquXPJMWIjhDtGtyEmQpqguqbnb3u0BtYWtVl71HY67TFhwoSVqprta5k/g3NtA7z3uNPdedVUdZ/X5BPAvb42pKpzgDkA2dnZmpOT48fLN1+5ubn4+x4699nHtCeW8vL2eOZMH05YWOiF+sm0R6iztqjN2qO2xmoPf7pclgO9RCRTRCKBS4H53iuIiPcwgxcAeYErMTSM7p7Cr8/tx7t5u3h4UX6wyzHGhKAG99BVtVxEZgILAQ/wlKquFpG7gBWqOh+4QUQuAMqB/cCPG7HmFuvHYzL4qvAQD7z7DQM6J/DdfmnBLskYE0L8Gg9dVRcAC+rMu8Pr8S+BXwa2tNAjIvzxvwbyze4ifj5vFa/OHEv3dtavaIwJDPulaBOLjvDw18uHExEexoznVlJUYr8kNcYEhgV6EKS3jeGRHw1l494j3PLiF/ZLUmNMQFigB8mYHqn8cnJf3l6zi7/k2kFSY8zps0APoqvHZfKDIZ340zvfsHjt7mCXY4xp4SzQg0hE+L//GkS/DgncMO9zNu09EuySjDEtmAV6kLWJ9PD49OF4woQZz62g+Hh5sEsyxrRQFujNQJfkGB65bBj5u4u57V9f0NBwDMYY44sFejMxrlcqsyb35c2vd/LY+wXBLscY0wJZoDcj157ZnfMHd+K+hevIXWcHSY0xJ8cCvRkREe6ZOpA+afHcMPdzNu+zg6TGGP9ZoDczMZHhzJmejYjwk+dWcrTUDpIaY/xjgd4MdU2J4eHLhvLNriJue+lLO0hqjPGLBXozdVbvdtw2sS9vfLmDOR9sCHY5xpgWwAK9Gfvp+O6cO7Aj97y1lg/X7wl2OcaYZs4CvRkTEe69aBC92sdz/dzP2br/aLBLMsY0YxbozVxsVDiPTx9OZaUy47mVHCutCHZJxphmygK9BchIjeWhy4aydudhfvFvO0hqjPHNr0AXkUkisk5E8kVk1gnWmyoiKiI+r0htTt2EPu259Xt9mP/Fdp78aGOwyzHGNEMNBrqIeIBHgclAFnCZiGT5WC8euBFYGugijeO6nB5M6t+BPy7I45P8vcEuxxjTzPizhz4SyFfVDapaCswDpvhY7/8B9wAlAazPeBER7v/hYHq0i+N/nv+MwgN2kNQYU0Ma6o8VkYuASap6jTs9HRilqjO91hkG/EpVp4pILnCrqq7wsa0ZwAyAtLS04fPmzQvYGwmG4uJi4uKa/iLPO49U8rslx2gfE8b/joomyiNNXoMvwWqP5sjaojZrj9pOpz0mTJiwUlV9dmuHn1ZVgIiEAX8GftzQuqo6B5gDkJ2drTk5Oaf78kGVm5tLsN5D+x67uPrZFby5J4kHLhmCSPBDPZjt0dxYW9Rm7VFbY7WHP10u24AuXtPp7rwq8cAAIFdENgGjgfl2YLRxfadvGjed3ZtXVm3n7rfWUlJmpzMa09r5E+jLgV4ikikikcClwPyqhap6SFVTVTVDVTOAT4ELfHW5mMCaOaEnFw1P5/H3N/DdP73Pq6u2UVlppzQa01o1GOiqWg7MBBYCecCLqrpaRO4SkQsau0BTv7Aw4f6LB/P8taNIiongxnmruPAvH7N80/5gl2aMCQK/+tBVdQGwoM68O+pZN+f0yzInY0yPVF6bOY6XP9/G/QvXcfFflzCpfwdmTe5LRmpssMszxjSR0z4oapqHsDDhouHpnDuwI3/7cAN/fb+A99buYvroDG74bk+SYiKDXaIxppHZT/9DTJtIDzd8txe5t+YwdVg6z3yykfH35fLkRxspLa8MdnnGmEZkgR6i2idEc/fUQbxxw5kMSk/k/72+hu898D5vfb3DxoIxJkRZoIe4fh0T+Pt/j+Tpq0YQ4Qnjp//4jEse/5Qvth4MdmnGmACzQG8FRIQJfdrz5o1n8ocLB7BhbzFTHv2Yn8/7nG0HjwW7PGNMgFigtyLhnjCmjerG4ltzuC6nB29+vZPv3J/LvW+tpaikLNjlGWNOkwV6KxQfHcHtk/qy6NYcJg/owF9yC5hwfy7/XLqZ8go7cGpMS2WB3op1TmrDg5cO5dX/GUv31Dh+9Z+vmfzQhyxet9sOnBrTAlmgGwZ3SeKFn4zmr5cPo6yikqueXs4VTy0jb8fhYJdmjDkJFugGcA6cThrQkbdvGs9vzsviy8JDnDv7Q2b9+0t2F9kQ98a0BBboppbI8DCuHpfJ+7flcNXYTP79WSE59+Uy+731doFqY5o5C3TjU1JMJL85L4t3bhrPWb3a8ed3vmHC/bm8tLLQRnQ0ppmyQDcnlJEay1+nD+fFn5xBWkIUt/7rC85/5CM+KbBrmhrT3FigG7+MzEzmP9eN5aFLh3DwaBk/+ttSrnl2BQV7ioNdmjHGZYFu/BYWJkwZ0pn3bhnP7ZP68OmGfUx84AN+++rX7D9SGuzyjGn1bPhcc9KiIzxcl9OTH2Z34YF3vuG5Tzfz8ufbmNxNOGNcBVHhnmCXaEyr5NceuohMEpF1IpIvIrN8LP+piHwlIqtE5CMRyQp8qaa5SY2L4g8XDmThz88iu1tbXlxXxnf/9D6vfbHdfphkTBA0GOgi4gEeBSYDWcBlPgL7eVUdqKpDgHuBPwe8UtNs9UqL5+mrRnJbdjRxUeFcP/dz/uuxT1i52S6FZ0xT8mcPfSSQr6obVLUUmAdM8V5BVb1/UhgL2O5ZK9Q/1cMbN5zJvRcNYtuBY0x9bAn/88/P2LzvSLBLM6ZVkIb+NBaRi4BJqnqNOz0dGKWqM+us9z/AzUAk8B1VXe9jWzOAGQBpaWnD582bF5A3ESzFxcXExcUFu4xmw7s9jpcrb24qY8HGMioq4exu4VzQI5LYCAlylU3DPhu1WXvUdjrtMWHXA40AABfmSURBVGHChJWqmu1rWcAC3Wv9HwETVfXKE203OztbV6xY4U/9zVZubi45OTnBLqPZ8NUeuw6X8Ke31/GvlYUktonghu/04vLR3YgMD+0TrOyzUZu1R22n0x4iUm+g+/O/ahvQxWs63Z1Xn3nAD/wvz4SytIRo7r1oMAtuOJOBnRO5q/pSeDvtwKkxAeZPoC8HeolIpohEApcC871XEJFeXpPnAt/qbjGt27cvhbfSLoVnTIA1eB66qpaLyExgIeABnlLV1SJyF7BCVecDM0XkbKAMOACcsLvFtE5Vl8I7s2cqL6zYygPvfMOURz9mypBO3DaxD+ltY4JdojEtml8/LFLVBcCCOvPu8Hp8Y4DrMiGs6lJ4FwzuxF/fL+CJDzfy5tc7uXpcJj/L6UFCdESwSzSmRQrtI1OmWYuPjuC2iX1ZfGsO5w3syGO5BUy4L5fnlmyyS+EZcwos0E3QdUpqw58vGcJrM8fRs30cv3l1NRMf/ID38nbZgVNjToIFumk2BqYnMm/GaOZMH44qXP3sCqY9sZSvtx0KdmnGtAgW6KZZERG+178DC286i99d0J+8HYc5/5GPuOXFL9h5yC6FZ8yJWKCbZinCE8aVYzLIvW0CM87szmtfbCfn/sX8+e11HDleHuzyjGmWLNBNs5bYJoJffr8f790ynrP7pTF7UT7j78tl7rItVNil8IypxQLdtAhdkmN45EfDePm6MXRLieGXL3/F9x/6kPe/2RPs0oxpNizQTYsyrGtbXvrpGfxl2jCOlVVw5VPLuOKpZazbWRTs0owJOgt00+KICN8f2JF3bj6LX5/bj1VbDjD5oQ/45ctfsrvIDpya1ssuQWdarKhwD9ec2Z2pw9KZvWg9zy3ZzCufb2dszxRGZiYzKjOF/p0SCPfYfotpHSzQTYvXNjaS357fnyvOyOBvH25gScE+3s3bDUBspIdh3doyKjOZUd1TGJSeaNc8NSHLAt2EjMzUWP544UAAdh8uYdmm/SzbuJ+lG/Zz/9vfABAZHsbQLknVAT+0axIxkfbfwIQG+ySbkNQ+IZrzBnXivEGdADhwpJTlbsAv27SfRxbnM3tRPuFhwsD0RLeLJpnsjGQbHMy0WBboplVoGxvJ9/p34Hv9OwBQVFLGys0HnIDfuJ+nPtrI4+9vQASyOiZUB/yIjGRS4qKCXL0x/rFAN61SfHQEOX3ak9OnPQAlZRV8tqUm4Ocu28LTH28CoFf7OEZmJlcfaO2QGB3Eyo2pnwW6MUB0hIcxPVIZ0yMVgNLySr7adoilG/exbON+Xl21nX8u3QJA1+QYRnkFfJfkNoi0jotfm+bNAt0YHyLDwxjerS3Du7XluhyoqFTydhxm6cb9LN2wj3fzdvGvlYUAdEiIZlT35Opumh7t7Or2Jjj8CnQRmQQ8hHMJuidU9e46y28GrgHKgT3Af6vq5gDXakzQeMKEAZ0TGdA5kavHZVJZqeTvKWap20WzpGAfr67aDkBKbCSZcRVsjdrE2J6pZKbG2h68aRINBrqIeIBHgXOAQmC5iMxX1TVeq30OZKvqURH5GXAvcEljFGxMcxAWJvROi6d3WjzTR3dDVdm876hzmuTG/eSu2cZvXl0NQMfEaMb2TGVszxTG9kilfYL1wZvG4c8e+kggX1U3AIjIPGAKUB3oqrrYa/1PgcsDWaQxzZ2IkJEaS0ZqLD8c0YXFi/eTOXAkHxfs5eP8vbybt4uX3C6aXu3j3IBPZVR3O03SBI40dIkvEbkImKSq17jT04FRqjqznvUfAXaq6u99LJsBzABIS0sbPm/evNMsP7iKi4uJi7P+0irWHjXqtkWlKlsOV7JmfwVr9lbyzYEKSishTCAzIYysVA9ZyR56tg0jIiz0umfss1Hb6bTHhAkTVqpqtq9lAT0oKiKXA9nAeF/LVXUOMAcgOztbc3JyAvnyTS43N5eW/h4CydqjRkNtcby8gs82H+STgr18lL+XBRsP8VpBGdERYYzISGacuwef1TGBsBAIePts1NZY7eFPoG8DunhNp7vzahGRs4FfAeNV9XhgyjMmNEWFezijRwpn9Ejhlu/14XBJGUs37OfjfKeL5v/eXAtA25gIzuiR4nTR9EilW0qMHWA19fIn0JcDvUQkEyfILwV+5L2CiAwFHsfpmtkd8CqNCXEJ0RGck5XGOVlpgDMWjdP/vo+P8/ey4KudAHROauMcXO3pnDPfLt5+xWpqNBjoqlouIjOBhTinLT6lqqtF5C5gharOB+4D4oB/uXsPW1T1gkas25iQ1j4hmguHpnPh0HRUlY17j/BxwT4+Xr+Xhat38eIK5wBr3w7xjOmRyrheKYzMTCEuyn5a0pr59a+vqguABXXm3eH1+OwA12WMcYkI3dvF0b1dHNNHd6OiUlm9/VD13vs/l27mqY83Eh4mDOmSxJieqYzrmcqQLklEhttY8K2JfZ0b08J4woRB6UkMSk/iZzk9nHFoNh/g44K9fJS/j0cWrWf2e+uJifTQv1MC8dERxER6iI0Mp02kh9goDzGR4cRGeoiJCic2MpyYKGd5TKSH2KiaZTERnpA4KNtaWKAb08JFR3gY0zOVMT1TuW0iHDpWxqcb9vFJ/l7W7ixib/Fxjhwv52hpBUeOl3OktIKKyhOfruytTUTNl0BV4MdEeqq/JGq+DMJrvizqfGnsO1aJqtoB3UZmgW5MiElsE8HE/h2Y6A4VXJeqUlpRydHjFRwprQn6Wvel5SdcXny8nN2Hj9dafry88oR13bn0bfp1SKBvx3j6uvd90uKJtX7/gLGWNKaVERGiwj1EhXtoGxsZsO1WVCpHfXwBHCktJ3fZl1TEd2DtziL+vbKQI6UV1c/rmhxD3w7x9O2YQD/3vmtyDB7r6jlpFujGmIDwhAnx0RHE+xjKIGxnBDk5zuUBKyuVbQePsXZnEWt3HGbtziLydh7m3bxdVPUERUeE0SetZk++b4cE+naID+gXUCiyQDfGNKmwMKFLcgxdkmOqz7sHOFZawfrdRW7QF7F252HeXrOTF1ZsrV4nLSGqOuT7dUigT4d4erSLs7N5XBboxphmoU2kp/rsnSqqyp6i407I7zzM2h1F5O0s4pOCvZRVOLvz4WFCz/Zx1d02fTs4e/RpCVGt7iCsBboxptkSEdonRNM+IZqzerernl9WUcnGvUfIc7ts1roXH3nFHZMeICkmojrc+3WMp0+HBFLjIokMDyPSE1Z97wmTkAl+C3RjTIsT4QmrHo9+itf8Q0fLnD15d48+b0cRLyzfyrGyinq3JUKtgI8Md24RnrB65guR4R53nlQvi/Bax/t5vuYfKDnxGUGnygLdGBMyEmMiGNU9hVHdU6rnVVYqWw8cZe3OIg4dK6O0vJLS8krKKpz70gr3Vl5Ze1n1cqW0vIKjpeUcOqY1z/G6r9pWuZ/n91+RFcmFjfD+LdCNMSEtLEzolhJLt5TYRn+tykr91hdEVdgf93q8/ZsvGuX1LdCNMSZAwsKE6DAP0RGeE66Xu6Vxzsqxc32MMSZEWKAbY0yIsEA3xpgQYYFujDEhwq9AF5FJIrJORPJFZJaP5WeJyGciUi4iFwW+TGOMMQ1pMNBFxAM8CkwGsoDLRCSrzmpbgB8Dzwe6QGOMMf7x57TFkUC+qm4AEJF5wBRgTdUKqrrJXdY4P38yxphQUVGGVJY1yqb9CfTOwFav6UJg1Km8mIjMAGYApKWlkZubeyqbaTaKi4tb/HsIJGuPGtYWtbWG9pDKCiLKDhJ1fD+RpQeILN3vPva+P0BE2SGSul1Dbu63hxk+XU36wyJVnQPMAcjOztacnJymfPmAy83NpaW/h0Cy9qhhbVFbi26Pygo4sheKd0LRTijaUefevR3ZDVq3k0Igrj3Ed4DUPhCXBvEdKStu3yjt4U+gbwO6eE2nu/OMMablqqyEY/vrD+iq6eJdoD4G94pt5wR1XAfoMBDiOzrT8R0h3gluYtuD59sxW9xIf634E+jLgV4ikokT5JcCP2qUaowxJlAqyuDgFti/oeZ2qLAmsIt3ga++7JgUJ6TjO0D7LDekO3gFdgcnqMOb39WTGgx0VS0XkZnAQsADPKWqq0XkLmCFqs4XkRHAf4C2wPki8jtV7d+olRvj7dgB2PgBFK5w/sRN7e3ckrpC2InH1TAtmHdo7ytwg9u9P7gFKstr1o2Mg8QubvdHb6+g9grruDQIjwre+zlNfvWhq+oCYEGdeXd4PV6O0xVjTNMoL4XCZVCwGDYshu2fO/2XYeG1/xN7oiClJ6T2qgn51F7OLbLxR98zAVBRBgc21w7rfV6h7d0dEhkPKd2h42Do/1+Q3B1Sejj3se2cwc9DmI22aFoGVdiztibAN30MZUdAPJCeDWfdDj0mQOfhcLwI9n7jdVsPO7+EvPm1D1oldvEKeq/Aj0sL+f/4zU55qbunXfDtve2DW32HdqehMGBqTWAn94DY1Fb9b2eBbpqvol2wIdcJ8A25zkEqcPa4h/zICfCMcRCdWPt5McnQdbRz81Z+3AmJqqDf495/9pzz5VAlKtF30Cdngifwp5oFXGUFlBY7X2zlx52AkzDnyy/M49xLmPvY6756eZh7C3AwlpfCwc31d494f9lGJTgh3WkYDLy4JrCTu7f60D4RC3TTfJQehc2fOAFesBh2r3bmt0mG7uOh+wQnxJO6ntr2w6OgfT/n5k0VDm+v2ZuvCvwNufCF14+fw8KhbaYT7u161wR9Sk9ok8Rpq6xwQvj4Yfe+yPd0yeF61nNvpUWnXwvUhHx16Fc9rhv+npp59aw76sAOeH+P79DuPNwN7R41XSQxKRbap8AC3QRPZSXs/MIJ74JFsHUpVJSCJ9LZu/7ub50A7zDYCYbGIgKJnZ1bjwm1l5Uchn3rawf93vWw/u3aZ0jEpdXao0/dcwi+2us7dEsO+Q5i778S6i8WouK9bgnOXyiJ6TXT3svDowF1viy0wr2vdG7Vjytqlmul8+9S/dj7eepj3brb8/0aReVxtBl5hYV2I7NAN03r4JaafvAN7zvnAQOkDYCRM5xA7ToGImOCW2eV6ARnD7Lz8NrzK8qd7gPvvvo938DX/4aSQwwAWO21voR9O3BjUqBtRu1wrhXW8U5Ye09HxDbul1sjWZObS/uW+sOiFsQC3TSukkOw8cOabpT9Bc78+I7Qe5IT4N1znFMNWxJPuLOXmdID+kyuma8KR/ayYvGrZJ8x3iuIY2xv1DQ6C3QTWBVlzrngVQG+baXzZ3hErHMAc+S1Tl94uz6hGXAiENeO4nj3VEljmpAFujkxVSg76hywLC2G0iPOrexIzePSI043w1cL4JO1zkE5CXPOUDjzZifA00c0y1/WGRNKLNBDhSqUl7gBW+wGsPu47Kjv+aVH3GVVQe1r2RFA/SohNroDDLrYCfDMM6FN28Z9z8aYWizQT1dlJVQcd873LT/uPi498byKUid8q+dV3fua52sb7vMrSp1lZcecPeZvjfR2AuHRzi8lI2Kd+8hY50BkTHLNdK1ldW4+li1dsrLljqhnTAiwQG9IZSUcLnROVduX796vh30FnHV4O+T6GIXtlIhznrQnyrkPj3JO36u+j3a6LKLivz0vIsa5RcY641VUhXPV4wivx1XTPkaAM8a0bPa/usqxg86v16rOOd63HvbmO2dllJfUrBeV4PyQpNsYCg+U0bVHnzrBG+UE7cnOCwsPzYOExpgm07oCvaIMDmyqvae9N9+5P7KnZj3xOOcHp/ZyTqtL7eWEeEov5/Q6N3g35ObS1boYjDHNROgFuqoTztVdI/k1oX1gU+2R+GJSnbDuPckNbXcUvqRudkaGMabFabmBXnbMq4sk3wnuqsfHD9Ws54lyfvzRPguyptSEdkoPOwvDGBNSWl6gf/Z3eP8+OLSVWqfTJaQ7IT3oYje03S6SxC4t8qfSxhhzsvwKdBGZBDyEc8WiJ1T17jrLo4C/A8OBfcAlqropsKW6Yts7AzelXF4T2ik97GIFxphWr8FAFxEP8ChwDlAILBeR+aq6xmu1q4EDqtpTRC4F7gEuaYyC6TPJuRljjKnFn76IkUC+qm5Q1VJgHjClzjpTgGfdxy8B3xWxc/CMMaYp+RPonYGtXtOF7jyf66hqOXAISAlEgcYYY/zTpAdFRWQGMAMgLS2N3Nzcpnz5gCsuLm7x7yGQrD1qWFvUZu1RW2O1hz+Bvg3o4jWd7s7ztU6hiIQDiTgHR2tR1TnAHIDs7Gxt6eN+5Obm2tglXqw9alhb1GbtUVtjtYc/XS7LgV4ikikikcClwPw668wHrnQfXwQsUlX/hugzxhgTEA3uoatquYjMBBbinLb4lKquFpG7gBWqOh94EnhORPKB/Tihb4wxpgn51YeuqguABXXm3eH1uAS4OLClGWOMORn2E0pjjAkREqyubhHZA2wOyosHTiqwN9hFNCPWHjWsLWqz9qjtdNqjm6q287UgaIEeCkRkhapmB7uO5sLao4a1RW3WHrU1VntYl4sxxoQIC3RjjAkRFuinZ06wC2hmrD1qWFvUZu1RW6O0h/WhG2NMiLA9dGOMCREW6MYYEyIs0P0kIl1EZLGIrBGR1SJyozs/WUTeEZH17n2ruVCpiHhE5HMRed2dzhSRpSKSLyIvuGP/tAoikiQiL4nIWhHJE5EzWutnQ0Rucv+PfC0ic0UkujV9NkTkKRHZLSJfe83z+VkQx2y3Xb4UkWGn89oW6P4rB25R1SxgNPA/IpIFzALeU9VewHvudGtxI5DnNX0P8ICq9gQO4FzJqrV4CHhLVfsCg3HapdV9NkSkM3ADkK2qA3DGf6q6illr+Ww8A9S9rFp9n4XJQC/3NgN47LReWVXtdgo34FWcy/KtAzq68zoC64JdWxO9/3T3g/kd4HVAcH75Fu4uPwNYGOw6m6gtEoGNuCcZeM1vdZ8Nai52k4wzVtTrwMTW9tkAMoCvG/osAI8Dl/la71Rutod+CkQkAxgKLAXSVHWHu2gnkBaksprag8DtQKU7nQIcVOeKVeD7ylahKhPYAzztdkE9ISKxtMLPhqpuA+4HtgA7cK5etpLW+9moUt9nwZ8rwvnNAv0kiUgc8G/g56p62HuZOl+xIX8eqIicB+xW1ZXBrqWZCAeGAY+p6lDgCHW6V1rRZ6MtzjWGM4FOQCzf7n5o1Rrzs2CBfhJEJAInzP+pqi+7s3eJSEd3eUdgd7Dqa0JjgQtEZBPORcO/g9OHnOResQp8X9kqVBUChaq61J1+CSfgW+Nn42xgo6ruUdUy4GWcz0tr/WxUqe+z4M8V4fxmge4nERGcC3nkqeqfvRZ5X63pSpy+9ZCmqr9U1XRVzcA54LVIVacBi3GuWAWtpC0AVHUnsFVE+rizvgusoRV+NnC6WkaLSIz7f6aqLVrlZ8NLfZ+F+cAV7tkuo4FDXl0zJ81+KeonERkHfAh8RU2/8f/i9KO/CHTFGQ74h6q6PyhFBoGI5AC3qup5ItIdZ489GfgcuFxVjwezvqYiIkOAJ4BIYANwFc4OU6v7bIjI74BLcM4M+xy4BqdfuFV8NkRkLpCDM0TuLuC3wCv4+Cy4X3qP4HRLHQWuUtUVp/zaFujGGBMarMvFGGNChAW6McaECAt0Y4wJERboxhgTIizQjTEmRFigm5AlIhUissrrFrDBsUQkw3s0PWOag/CGVzGmxTqmqkOCXYQxTcX20E2rIyKbROReEflKRJaJSE93foaILHLHpX5PRLq689NE5D8i8oV7G+NuyiMif3PH/n5bRNoE7U0ZgwW6CW1t6nS5XOK17JCqDsT5ld6D7ryHgWdVdRDwT2C2O3828L6qDsYZo2W1O78X8Kiq9gcOAlMb+f0Yc0L2S1ETskSkWFXjfMzfBHxHVTe4A67tVNUUEdmLMxZ1mTt/h6qmisgeIN37p+ruEMrvqHPBAkTkF0CEqv6+8d+ZMb7ZHrpprbSexyfDeyySCuyYlAkyC3TTWl3idb/EffwJzuiRANNwBmMD58pMP4Pq66gmNlWRxpwM26MwoayNiKzymn5LVatOXWwrIl/i7GVf5s67HueqQ7fhXIHoKnf+jcAcEbkaZ0/8ZzhX4zGmWbE+dNPquH3o2aq6N9i1GBNI1uVijDEhwvbQjTEmRNgeujHGhAgLdGOMCREW6MYYEyIs0I0xJkRYoBtjTIj4/6NHJwJwnYcQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}