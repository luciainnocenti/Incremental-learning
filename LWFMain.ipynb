{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainAndTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb11f513c78e4414bc6e7f5756b0e040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b8f05f2486b14c869237c317c3b3305f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_89f15b75ebf94ed1bc190474c46a1233",
              "IPY_MODEL_ec7aaf43b62645d5813b3094382474f0"
            ]
          }
        },
        "b8f05f2486b14c869237c317c3b3305f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89f15b75ebf94ed1bc190474c46a1233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2a9ace368c1949868657d723a24f5b84",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_54b0902a32d74fb8926799e7177001d8"
          }
        },
        "ec7aaf43b62645d5813b3094382474f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_979a17667b714846afe02bdc90de3a78",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 32756293.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_283a37a52d1e45d49e245a478bf41739"
          }
        },
        "2a9ace368c1949868657d723a24f5b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "54b0902a32d74fb8926799e7177001d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "979a17667b714846afe02bdc90de3a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "283a37a52d1e45d49e245a478bf41739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/master/LWFMain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbtGDBU3QJaq",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf0TmOM3NdFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I0pKIVIM2KC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "026c9387-c32c-4d66-e43d-b0a826438f40"
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 135, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 1446 (delta 81), reused 0 (delta 0), pack-reused 1311\u001b[K\n",
            "Receiving objects: 100% (1446/1446), 991.93 KiB | 1.62 MiB/s, done.\n",
            "Resolving deltas: 100% (923/923), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLaS2laafBaG",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liUP5Kc1DMbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from DatasetCIFAR.data_set import Dataset \n",
        "from DatasetCIFAR.data_set import Subset\n",
        "from DatasetCIFAR import ResNet\n",
        "from DatasetCIFAR import utils\n",
        "from DatasetCIFAR import params\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "random.seed(params.SEED)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JToBIXRBKgOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dc813eb7-63a0-4b82-f524-e2ca6d5c79d5"
      },
      "source": [
        "print(params.SEED)\n",
        "print(params.NUM_WORKERS)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "653\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_vlqOL7ehLC",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWttFW3ljoMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resNet = ResNet.resnet32(num_classes=100)\n",
        "resNet = resNet.to(params.DEVICE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmohsyVWFpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transformer = transforms.Compose([transforms.RandomCrop(size = 32, padding=4),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_cyhIzFej5-",
        "colab_type": "text"
      },
      "source": [
        "# Define DataSets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcBNohmiYBtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "fb11f513c78e4414bc6e7f5756b0e040",
            "b8f05f2486b14c869237c317c3b3305f",
            "89f15b75ebf94ed1bc190474c46a1233",
            "ec7aaf43b62645d5813b3094382474f0",
            "2a9ace368c1949868657d723a24f5b84",
            "54b0902a32d74fb8926799e7177001d8",
            "979a17667b714846afe02bdc90de3a78",
            "283a37a52d1e45d49e245a478bf41739"
          ]
        },
        "outputId": "b7d4e8d2-228d-40cf-8b66-163373a53046"
      },
      "source": [
        "trainDS = Dataset(train=True)\n",
        "testDS = Dataset(train=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb11f513c78e4414bc6e7f5756b0e040",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFxMUO_FQZRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "bc818fc0-5093-449b-d882-cd7396053e80"
      },
      "source": [
        "train_splits = trainDS.splits\n",
        "test_splits = testDS.splits\n",
        "print(train_splits)\n",
        "print(test_splits)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[94.0, 63.0, 74.0, 21.0, 35.0, 56.0, 91.0, 96.0, 87.0, 48.0], [68.0, 80.0, 22.0, 37.0, 60.0, 97.0, 51.0, 62.0, 92.0, 76.0], [75.0, 89.0, 23.0, 99.0, 39.0, 66.0, 54.0, 69.0, 84.0, 61.0], [85.0, 24.0, 98.0, 41.0, 73.0, 58.0, 78.0, 77.0, 70.0, 49.0], [65.0, 88.0, 36.0, 93.0, 45.0, 10.0, 90.0, 17.0, 32.0, 59.0], [83.0, 43.0, 53.0, 11.0, 86.0, 19.0, 38.0, 30.0, 40.0, 50.0], [57.0, 81.0, 12.0, 95.0, 25.0, 47.0, 34.0, 52.0, 44.0, 72.0], [46.0, 79.0, 20.0, 28.0, 5.0, 71.0, 8.0, 18.0, 33.0, 15.0], [55.0, 29.0, 64.0, 31.0, 67.0, 7.0, 13.0, 14.0, 42.0, 6.0], [82.0, 2.0, 27.0, 16.0, 26.0, 3.0, 4.0, 1.0, 9.0, 0.0]]\n",
            "[[94.0, 63.0, 74.0, 21.0, 35.0, 56.0, 91.0, 96.0, 87.0, 48.0], [68.0, 80.0, 22.0, 37.0, 60.0, 97.0, 51.0, 62.0, 92.0, 76.0], [75.0, 89.0, 23.0, 99.0, 39.0, 66.0, 54.0, 69.0, 84.0, 61.0], [85.0, 24.0, 98.0, 41.0, 73.0, 58.0, 78.0, 77.0, 70.0, 49.0], [65.0, 88.0, 36.0, 93.0, 45.0, 10.0, 90.0, 17.0, 32.0, 59.0], [83.0, 43.0, 53.0, 11.0, 86.0, 19.0, 38.0, 30.0, 40.0, 50.0], [57.0, 81.0, 12.0, 95.0, 25.0, 47.0, 34.0, 52.0, 44.0, 72.0], [46.0, 79.0, 20.0, 28.0, 5.0, 71.0, 8.0, 18.0, 33.0, 15.0], [55.0, 29.0, 64.0, 31.0, 67.0, 7.0, 13.0, 14.0, 42.0, 6.0], [82.0, 2.0, 27.0, 16.0, 26.0, 3.0, 4.0, 1.0, 9.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAT2KQEersx",
        "colab_type": "text"
      },
      "source": [
        "# Useful plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1l7flYj4NJh",
        "colab_type": "text"
      },
      "source": [
        "The function plotEpoch plots, at the end of each task, how accuracy and loss change during the training phase. It show\n",
        "\n",
        "*   Validation and Training Accuracy\n",
        "*   Validation and Training Loss\n",
        "\n",
        "The function plotTask, for each task, how the accuracy on the validation set change when adding new tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr58kkHiIzZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTask(pars_tasks):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  x_tasks =  np.linspace(10, 100, 10)\n",
        "\n",
        "  plt.plot(x_tasks, pars_tasks, label=['Accuracy', 'Loss'])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.title('Accuracy over tasks')\n",
        "  plt.legend(['Accuracy', 'Loss'])\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iApKvCs942aS",
        "colab_type": "text"
      },
      "source": [
        "# Train and evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJse4JU7d9ck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25dfb89b-c9c8-4cc9-a3d3-c6765684f1b4"
      },
      "source": [
        "pars_tasks = []\n",
        "test_indexes = []\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "  pars_tasks.insert(task, 0)\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "\n",
        "  train_indexes = trainDS.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes + testDS.__getIndexesGroups__(task)\n",
        "\n",
        "  train_dataset = Subset(trainDS, train_indexes, train_transformer)\n",
        "  test_dataset = Subset(testDS, test_indexes, test_transformer)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE, shuffle = True)\n",
        "  test_loader = DataLoader( test_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE, shuffle = True )\n",
        "\n",
        "  if(task == 0):\n",
        "    torch.save(resNet, 'resNet_task{0}.pt'.format(task))\n",
        "  \n",
        "  \n",
        "\n",
        "  utils.trainfunction(task, train_loader, train_splits)\n",
        "  param = utils.evaluationTest(task, test_loader, test_splits)\n",
        "  pars_tasks[int(task/10)] = param #pars_task[i] = (accuracy, loss) at i-th task"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "task = 0 \n",
            "train col =  [94 63 74 21 35 56 91 96 87 48]\n",
            "train col =  [[94 63 74 21 35 56 91 96 87 48]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "At step  0  and at epoch =  0  the loss is =  0.031964968889951706  and accuracy is =  0.169\n",
            "At step  0  and at epoch =  1  the loss is =  0.023870963603258133  and accuracy is =  0.3538\n",
            "At step  0  and at epoch =  2  the loss is =  0.028029445558786392  and accuracy is =  0.4366\n",
            "At step  0  and at epoch =  3  the loss is =  0.03250391036272049  and accuracy is =  0.4782\n",
            "At step  0  and at epoch =  4  the loss is =  0.031980786472558975  and accuracy is =  0.5164\n",
            "At step  0  and at epoch =  5  the loss is =  0.02496189996600151  and accuracy is =  0.5498\n",
            "At step  0  and at epoch =  6  the loss is =  0.008919685147702694  and accuracy is =  0.585\n",
            "At step  0  and at epoch =  7  the loss is =  0.01897308975458145  and accuracy is =  0.6196\n",
            "At step  0  and at epoch =  8  the loss is =  0.02042660117149353  and accuracy is =  0.6444\n",
            "At step  0  and at epoch =  9  the loss is =  0.02938740700483322  and accuracy is =  0.6712\n",
            "At step  0  and at epoch =  10  the loss is =  0.011121355928480625  and accuracy is =  0.6598\n",
            "At step  0  and at epoch =  11  the loss is =  0.015545913949608803  and accuracy is =  0.7052\n",
            "At step  0  and at epoch =  12  the loss is =  0.012032520957291126  and accuracy is =  0.7094\n",
            "At step  0  and at epoch =  13  the loss is =  0.023287514224648476  and accuracy is =  0.7258\n",
            "At step  0  and at epoch =  14  the loss is =  0.011889862827956676  and accuracy is =  0.7266\n",
            "At step  0  and at epoch =  15  the loss is =  0.01659935712814331  and accuracy is =  0.7584\n",
            "At step  0  and at epoch =  16  the loss is =  0.021506723016500473  and accuracy is =  0.7478\n",
            "At step  0  and at epoch =  17  the loss is =  0.017697233706712723  and accuracy is =  0.768\n",
            "At step  0  and at epoch =  18  the loss is =  0.025738701224327087  and accuracy is =  0.781\n",
            "At step  0  and at epoch =  19  the loss is =  0.009670808911323547  and accuracy is =  0.777\n",
            "At step  0  and at epoch =  20  the loss is =  0.007356451358646154  and accuracy is =  0.8116\n",
            "At step  0  and at epoch =  21  the loss is =  0.014020672999322414  and accuracy is =  0.8186\n",
            "At step  0  and at epoch =  22  the loss is =  0.015566243790090084  and accuracy is =  0.8098\n",
            "At step  0  and at epoch =  23  the loss is =  0.00881472323089838  and accuracy is =  0.7958\n",
            "At step  0  and at epoch =  24  the loss is =  0.0035534799098968506  and accuracy is =  0.8292\n",
            "At step  0  and at epoch =  25  the loss is =  0.026082463562488556  and accuracy is =  0.8486\n",
            "At step  0  and at epoch =  26  the loss is =  0.007285485044121742  and accuracy is =  0.8398\n",
            "At step  0  and at epoch =  27  the loss is =  0.020306028425693512  and accuracy is =  0.8468\n",
            "At step  0  and at epoch =  28  the loss is =  0.010251504369080067  and accuracy is =  0.8512\n",
            "At step  0  and at epoch =  29  the loss is =  0.010440818965435028  and accuracy is =  0.8462\n",
            "At step  0  and at epoch =  30  the loss is =  0.029737787321209908  and accuracy is =  0.8404\n",
            "At step  0  and at epoch =  31  the loss is =  0.004920031875371933  and accuracy is =  0.8344\n",
            "At step  0  and at epoch =  32  the loss is =  0.00812864676117897  and accuracy is =  0.864\n",
            "At step  0  and at epoch =  33  the loss is =  0.007247745990753174  and accuracy is =  0.8598\n",
            "At step  0  and at epoch =  34  the loss is =  0.024831268936395645  and accuracy is =  0.8698\n",
            "At step  0  and at epoch =  35  the loss is =  0.002037459285929799  and accuracy is =  0.8578\n",
            "At step  0  and at epoch =  36  the loss is =  0.003502050880342722  and accuracy is =  0.886\n",
            "At step  0  and at epoch =  37  the loss is =  0.010609210468828678  and accuracy is =  0.8904\n",
            "At step  0  and at epoch =  38  the loss is =  0.01654244214296341  and accuracy is =  0.8478\n",
            "At step  0  and at epoch =  39  the loss is =  0.010606758296489716  and accuracy is =  0.8704\n",
            "At step  0  and at epoch =  40  the loss is =  0.006416087970137596  and accuracy is =  0.8696\n",
            "At step  0  and at epoch =  41  the loss is =  0.011262087151408195  and accuracy is =  0.8902\n",
            "At step  0  and at epoch =  42  the loss is =  0.018525835126638412  and accuracy is =  0.8852\n",
            "At step  0  and at epoch =  43  the loss is =  0.019489014521241188  and accuracy is =  0.8466\n",
            "At step  0  and at epoch =  44  the loss is =  0.019902631640434265  and accuracy is =  0.8678\n",
            "At step  0  and at epoch =  45  the loss is =  0.028655335307121277  and accuracy is =  0.8724\n",
            "At step  0  and at epoch =  46  the loss is =  0.01045736763626337  and accuracy is =  0.8766\n",
            "At step  0  and at epoch =  47  the loss is =  0.037429384887218475  and accuracy is =  0.9038\n",
            "At step  0  and at epoch =  48  the loss is =  0.003733393969014287  and accuracy is =  0.915\n",
            "At step  0  and at epoch =  49  the loss is =  0.015287920832633972  and accuracy is =  0.9442\n",
            "At step  0  and at epoch =  50  the loss is =  0.004744637291878462  and accuracy is =  0.9458\n",
            "At step  0  and at epoch =  51  the loss is =  0.00280054216273129  and accuracy is =  0.9514\n",
            "At step  0  and at epoch =  52  the loss is =  0.0034279334358870983  and accuracy is =  0.957\n",
            "At step  0  and at epoch =  53  the loss is =  0.002899336162954569  and accuracy is =  0.9584\n",
            "At step  0  and at epoch =  54  the loss is =  0.02322499267756939  and accuracy is =  0.9606\n",
            "At step  0  and at epoch =  55  the loss is =  0.001686186995357275  and accuracy is =  0.9558\n",
            "At step  0  and at epoch =  56  the loss is =  0.001890715560875833  and accuracy is =  0.9536\n",
            "At step  0  and at epoch =  57  the loss is =  0.022535061463713646  and accuracy is =  0.9646\n",
            "At step  0  and at epoch =  58  the loss is =  0.012692769058048725  and accuracy is =  0.9548\n",
            "At step  0  and at epoch =  59  the loss is =  0.0007393983541987836  and accuracy is =  0.959\n",
            "At step  0  and at epoch =  60  the loss is =  0.021276529878377914  and accuracy is =  0.9586\n",
            "At step  0  and at epoch =  61  the loss is =  0.006494807545095682  and accuracy is =  0.9624\n",
            "At step  0  and at epoch =  62  the loss is =  0.0037551173008978367  and accuracy is =  0.968\n",
            "At step  0  and at epoch =  63  the loss is =  0.0027858864050358534  and accuracy is =  0.9726\n",
            "At step  0  and at epoch =  64  the loss is =  0.014804649166762829  and accuracy is =  0.9714\n",
            "At step  0  and at epoch =  65  the loss is =  0.015271355397999287  and accuracy is =  0.97\n",
            "At step  0  and at epoch =  66  the loss is =  0.017602642998099327  and accuracy is =  0.9744\n",
            "At step  0  and at epoch =  67  the loss is =  0.004467095714062452  and accuracy is =  0.9668\n",
            "At step  0  and at epoch =  68  the loss is =  0.0019997237250208855  and accuracy is =  0.9726\n",
            "At step  0  and at epoch =  69  the loss is =  0.01362621784210205  and accuracy is =  0.9744\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "104\n",
            "Validation Loss: 0.0068700555711984634 Validation Accuracy : 0.891\n",
            "task = 10 \n",
            "train col =  [68 80 22 37 60 97 51 62 92 76]\n",
            "train col =  [[68 80 22 37 60 97 51 62 92 76]]\n",
            "At step  10  and at epoch =  0  the loss is =  0.05356012284755707  and accuracy is =  0.3296\n",
            "At step  10  and at epoch =  1  the loss is =  0.03420894220471382  and accuracy is =  0.5812\n",
            "At step  10  and at epoch =  2  the loss is =  0.044698577374219894  and accuracy is =  0.6514\n",
            "At step  10  and at epoch =  3  the loss is =  0.034712765365839005  and accuracy is =  0.679\n",
            "At step  10  and at epoch =  4  the loss is =  0.02759445086121559  and accuracy is =  0.7052\n",
            "At step  10  and at epoch =  5  the loss is =  0.02410195767879486  and accuracy is =  0.7462\n",
            "At step  10  and at epoch =  6  the loss is =  0.02379925735294819  and accuracy is =  0.7628\n",
            "At step  10  and at epoch =  7  the loss is =  0.0383828766644001  and accuracy is =  0.7794\n",
            "At step  10  and at epoch =  8  the loss is =  0.04355990141630173  and accuracy is =  0.7836\n",
            "At step  10  and at epoch =  9  the loss is =  0.024713894352316856  and accuracy is =  0.7822\n",
            "At step  10  and at epoch =  10  the loss is =  0.036266181617975235  and accuracy is =  0.802\n",
            "At step  10  and at epoch =  11  the loss is =  0.046291761100292206  and accuracy is =  0.799\n",
            "At step  10  and at epoch =  12  the loss is =  0.03820270299911499  and accuracy is =  0.8134\n",
            "At step  10  and at epoch =  13  the loss is =  0.02453237771987915  and accuracy is =  0.8224\n",
            "At step  10  and at epoch =  14  the loss is =  0.028194652870297432  and accuracy is =  0.832\n",
            "At step  10  and at epoch =  15  the loss is =  0.024764137342572212  and accuracy is =  0.8484\n",
            "At step  10  and at epoch =  16  the loss is =  0.03986068814992905  and accuracy is =  0.8554\n",
            "At step  10  and at epoch =  17  the loss is =  0.030680334195494652  and accuracy is =  0.843\n",
            "At step  10  and at epoch =  18  the loss is =  0.02708880789577961  and accuracy is =  0.8486\n",
            "At step  10  and at epoch =  19  the loss is =  0.04097351059317589  and accuracy is =  0.8554\n",
            "At step  10  and at epoch =  20  the loss is =  0.022764192894101143  and accuracy is =  0.8586\n",
            "At step  10  and at epoch =  21  the loss is =  0.04027113318443298  and accuracy is =  0.8748\n",
            "At step  10  and at epoch =  22  the loss is =  0.026219047605991364  and accuracy is =  0.8598\n",
            "At step  10  and at epoch =  23  the loss is =  0.0308742206543684  and accuracy is =  0.8508\n",
            "At step  10  and at epoch =  24  the loss is =  0.018803859129548073  and accuracy is =  0.8672\n",
            "At step  10  and at epoch =  25  the loss is =  0.03490125387907028  and accuracy is =  0.8946\n",
            "At step  10  and at epoch =  26  the loss is =  0.03602287173271179  and accuracy is =  0.8674\n",
            "At step  10  and at epoch =  27  the loss is =  0.04747890681028366  and accuracy is =  0.8706\n",
            "At step  10  and at epoch =  28  the loss is =  0.025232437998056412  and accuracy is =  0.847\n",
            "At step  10  and at epoch =  29  the loss is =  0.027762312442064285  and accuracy is =  0.8846\n",
            "At step  10  and at epoch =  30  the loss is =  0.02725723385810852  and accuracy is =  0.8886\n",
            "At step  10  and at epoch =  31  the loss is =  0.03426530584692955  and accuracy is =  0.8938\n",
            "At step  10  and at epoch =  32  the loss is =  0.027008987963199615  and accuracy is =  0.8976\n",
            "At step  10  and at epoch =  33  the loss is =  0.05879811570048332  and accuracy is =  0.8998\n",
            "At step  10  and at epoch =  34  the loss is =  0.034154992550611496  and accuracy is =  0.8906\n",
            "At step  10  and at epoch =  35  the loss is =  0.05356950685381889  and accuracy is =  0.8882\n",
            "At step  10  and at epoch =  36  the loss is =  0.03874595835804939  and accuracy is =  0.8708\n",
            "At step  10  and at epoch =  37  the loss is =  0.02823328971862793  and accuracy is =  0.8878\n",
            "At step  10  and at epoch =  38  the loss is =  0.030184319242835045  and accuracy is =  0.9164\n",
            "At step  10  and at epoch =  39  the loss is =  0.02765401266515255  and accuracy is =  0.92\n",
            "At step  10  and at epoch =  40  the loss is =  0.027429576963186264  and accuracy is =  0.9102\n",
            "At step  10  and at epoch =  41  the loss is =  0.026850080117583275  and accuracy is =  0.9028\n",
            "At step  10  and at epoch =  42  the loss is =  0.02370442822575569  and accuracy is =  0.9176\n",
            "At step  10  and at epoch =  43  the loss is =  0.021435968577861786  and accuracy is =  0.9284\n",
            "At step  10  and at epoch =  44  the loss is =  0.021780313923954964  and accuracy is =  0.9274\n",
            "At step  10  and at epoch =  45  the loss is =  0.026242226362228394  and accuracy is =  0.9252\n",
            "At step  10  and at epoch =  46  the loss is =  0.020932035520672798  and accuracy is =  0.9174\n",
            "At step  10  and at epoch =  47  the loss is =  0.029375052079558372  and accuracy is =  0.917\n",
            "At step  10  and at epoch =  48  the loss is =  0.019769586622714996  and accuracy is =  0.953\n",
            "At step  10  and at epoch =  49  the loss is =  0.02014455571770668  and accuracy is =  0.9672\n",
            "At step  10  and at epoch =  50  the loss is =  0.019022125750780106  and accuracy is =  0.9722\n",
            "At step  10  and at epoch =  51  the loss is =  0.020665235817432404  and accuracy is =  0.9708\n",
            "At step  10  and at epoch =  52  the loss is =  0.023587195202708244  and accuracy is =  0.972\n",
            "At step  10  and at epoch =  53  the loss is =  0.04890240728855133  and accuracy is =  0.9772\n",
            "At step  10  and at epoch =  54  the loss is =  0.0425797700881958  and accuracy is =  0.972\n",
            "At step  10  and at epoch =  55  the loss is =  0.028426378965377808  and accuracy is =  0.9624\n",
            "At step  10  and at epoch =  56  the loss is =  0.03427066281437874  and accuracy is =  0.9748\n",
            "At step  10  and at epoch =  57  the loss is =  0.017544038593769073  and accuracy is =  0.9728\n",
            "At step  10  and at epoch =  58  the loss is =  0.052298255264759064  and accuracy is =  0.983\n",
            "At step  10  and at epoch =  59  the loss is =  0.02578548900783062  and accuracy is =  0.9634\n",
            "At step  10  and at epoch =  60  the loss is =  0.026487823575735092  and accuracy is =  0.9782\n",
            "At step  10  and at epoch =  61  the loss is =  0.028017744421958923  and accuracy is =  0.9786\n",
            "At step  10  and at epoch =  62  the loss is =  0.0345490500330925  and accuracy is =  0.9814\n",
            "At step  10  and at epoch =  63  the loss is =  0.03926881402730942  and accuracy is =  0.985\n",
            "At step  10  and at epoch =  64  the loss is =  0.02311873435974121  and accuracy is =  0.9878\n",
            "At step  10  and at epoch =  65  the loss is =  0.03772119805216789  and accuracy is =  0.9836\n",
            "At step  10  and at epoch =  66  the loss is =  0.03219889476895332  and accuracy is =  0.9856\n",
            "At step  10  and at epoch =  67  the loss is =  0.031676631420850754  and accuracy is =  0.9856\n",
            "At step  10  and at epoch =  68  the loss is =  0.019407551735639572  and accuracy is =  0.9884\n",
            "At step  10  and at epoch =  69  the loss is =  0.040050044655799866  and accuracy is =  0.9852\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "80\n",
            "Validation Loss: 0.03244268149137497 Validation Accuracy : 0.7155\n",
            "task = 20 \n",
            "train col =  [75 89 23 99 39 66 54 69 84 61]\n",
            "train col =  [[75 89 23 99 39 66 54 69 84 61]]\n",
            "At step  20  and at epoch =  0  the loss is =  0.06188558042049408  and accuracy is =  0.5194\n",
            "At step  20  and at epoch =  1  the loss is =  0.05629822984337807  and accuracy is =  0.6864\n",
            "At step  20  and at epoch =  2  the loss is =  0.05102473497390747  and accuracy is =  0.741\n",
            "At step  20  and at epoch =  3  the loss is =  0.05484504997730255  and accuracy is =  0.7784\n",
            "At step  20  and at epoch =  4  the loss is =  0.0522523857653141  and accuracy is =  0.7962\n",
            "At step  20  and at epoch =  5  the loss is =  0.04881904646754265  and accuracy is =  0.8158\n",
            "At step  20  and at epoch =  6  the loss is =  0.05276767536997795  and accuracy is =  0.8342\n",
            "At step  20  and at epoch =  7  the loss is =  0.03912004455924034  and accuracy is =  0.8344\n",
            "At step  20  and at epoch =  8  the loss is =  0.05188363045454025  and accuracy is =  0.855\n",
            "At step  20  and at epoch =  9  the loss is =  0.044123608618974686  and accuracy is =  0.8604\n",
            "At step  20  and at epoch =  10  the loss is =  0.08097696304321289  and accuracy is =  0.8648\n",
            "At step  20  and at epoch =  11  the loss is =  0.052664756774902344  and accuracy is =  0.84\n",
            "At step  20  and at epoch =  12  the loss is =  0.04604589194059372  and accuracy is =  0.8728\n",
            "At step  20  and at epoch =  13  the loss is =  0.03987331688404083  and accuracy is =  0.8924\n",
            "At step  20  and at epoch =  14  the loss is =  0.0486011877655983  and accuracy is =  0.9002\n",
            "At step  20  and at epoch =  15  the loss is =  0.05285361036658287  and accuracy is =  0.8934\n",
            "At step  20  and at epoch =  16  the loss is =  0.06924531608819962  and accuracy is =  0.8984\n",
            "At step  20  and at epoch =  17  the loss is =  0.058552417904138565  and accuracy is =  0.9006\n",
            "At step  20  and at epoch =  18  the loss is =  0.050919320434331894  and accuracy is =  0.869\n",
            "At step  20  and at epoch =  19  the loss is =  0.04127267748117447  and accuracy is =  0.8922\n",
            "At step  20  and at epoch =  20  the loss is =  0.04832428693771362  and accuracy is =  0.9178\n",
            "At step  20  and at epoch =  21  the loss is =  0.05557169020175934  and accuracy is =  0.9162\n",
            "At step  20  and at epoch =  22  the loss is =  0.05476084724068642  and accuracy is =  0.9044\n",
            "At step  20  and at epoch =  23  the loss is =  0.07584359496831894  and accuracy is =  0.9132\n",
            "At step  20  and at epoch =  24  the loss is =  0.04083830863237381  and accuracy is =  0.8576\n",
            "At step  20  and at epoch =  25  the loss is =  0.04803202673792839  and accuracy is =  0.9096\n",
            "At step  20  and at epoch =  26  the loss is =  0.03752212971448898  and accuracy is =  0.9192\n",
            "At step  20  and at epoch =  27  the loss is =  0.05092018470168114  and accuracy is =  0.934\n",
            "At step  20  and at epoch =  28  the loss is =  0.07243049144744873  and accuracy is =  0.9394\n",
            "At step  20  and at epoch =  29  the loss is =  0.05159495398402214  and accuracy is =  0.8854\n",
            "At step  20  and at epoch =  30  the loss is =  0.03385409340262413  and accuracy is =  0.916\n",
            "At step  20  and at epoch =  31  the loss is =  0.04963526129722595  and accuracy is =  0.9426\n",
            "At step  20  and at epoch =  32  the loss is =  0.04790334776043892  and accuracy is =  0.9274\n",
            "At step  20  and at epoch =  33  the loss is =  0.045694220811128616  and accuracy is =  0.9366\n",
            "At step  20  and at epoch =  34  the loss is =  0.04994528740644455  and accuracy is =  0.9368\n",
            "At step  20  and at epoch =  35  the loss is =  0.04811984300613403  and accuracy is =  0.921\n",
            "At step  20  and at epoch =  36  the loss is =  0.037865303456783295  and accuracy is =  0.939\n",
            "At step  20  and at epoch =  37  the loss is =  0.046089719980955124  and accuracy is =  0.9448\n",
            "At step  20  and at epoch =  38  the loss is =  0.04903218150138855  and accuracy is =  0.9494\n",
            "At step  20  and at epoch =  39  the loss is =  0.04427823796868324  and accuracy is =  0.9442\n",
            "At step  20  and at epoch =  40  the loss is =  0.03137919679284096  and accuracy is =  0.96\n",
            "At step  20  and at epoch =  41  the loss is =  0.04954071715474129  and accuracy is =  0.952\n",
            "At step  20  and at epoch =  42  the loss is =  0.04199081286787987  and accuracy is =  0.9506\n",
            "At step  20  and at epoch =  43  the loss is =  0.05629868432879448  and accuracy is =  0.9238\n",
            "At step  20  and at epoch =  44  the loss is =  0.06055418401956558  and accuracy is =  0.9196\n",
            "At step  20  and at epoch =  45  the loss is =  0.04208501800894737  and accuracy is =  0.9398\n",
            "At step  20  and at epoch =  46  the loss is =  0.04515562951564789  and accuracy is =  0.9368\n",
            "At step  20  and at epoch =  47  the loss is =  0.05078815296292305  and accuracy is =  0.9474\n",
            "At step  20  and at epoch =  48  the loss is =  0.061098210513591766  and accuracy is =  0.9696\n",
            "At step  20  and at epoch =  49  the loss is =  0.04200305789709091  and accuracy is =  0.974\n",
            "At step  20  and at epoch =  50  the loss is =  0.03469057381153107  and accuracy is =  0.9834\n",
            "At step  20  and at epoch =  51  the loss is =  0.042721085250377655  and accuracy is =  0.9834\n",
            "At step  20  and at epoch =  52  the loss is =  0.03392970189452171  and accuracy is =  0.9848\n",
            "At step  20  and at epoch =  53  the loss is =  0.049040183424949646  and accuracy is =  0.9822\n",
            "At step  20  and at epoch =  54  the loss is =  0.06310823559761047  and accuracy is =  0.982\n",
            "At step  20  and at epoch =  55  the loss is =  0.05383860319852829  and accuracy is =  0.9804\n",
            "At step  20  and at epoch =  56  the loss is =  0.051478803157806396  and accuracy is =  0.9878\n",
            "At step  20  and at epoch =  57  the loss is =  0.043604157865047455  and accuracy is =  0.9818\n",
            "At step  20  and at epoch =  58  the loss is =  0.06303039938211441  and accuracy is =  0.9872\n",
            "At step  20  and at epoch =  59  the loss is =  0.0435115247964859  and accuracy is =  0.9836\n",
            "At step  20  and at epoch =  60  the loss is =  0.03813514858484268  and accuracy is =  0.9908\n",
            "At step  20  and at epoch =  61  the loss is =  0.03676842898130417  and accuracy is =  0.987\n",
            "At step  20  and at epoch =  62  the loss is =  0.05032853037118912  and accuracy is =  0.9892\n",
            "At step  20  and at epoch =  63  the loss is =  0.0472903810441494  and accuracy is =  0.9906\n",
            "At step  20  and at epoch =  64  the loss is =  0.04274095594882965  and accuracy is =  0.992\n",
            "At step  20  and at epoch =  65  the loss is =  0.04063691943883896  and accuracy is =  0.9914\n",
            "At step  20  and at epoch =  66  the loss is =  0.034827109426259995  and accuracy is =  0.9928\n",
            "At step  20  and at epoch =  67  the loss is =  0.04233145713806152  and accuracy is =  0.993\n",
            "At step  20  and at epoch =  68  the loss is =  0.03661182150244713  and accuracy is =  0.9896\n",
            "At step  20  and at epoch =  69  the loss is =  0.0442192442715168  and accuracy is =  0.9906\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "56\n",
            "Validation Loss: 0.04736844822764397 Validation Accuracy : 0.601\n",
            "task = 30 \n",
            "train col =  [85 24 98 41 73 58 78 77 70 49]\n",
            "train col =  [[85 24 98 41 73 58 78 77 70 49]]\n",
            "At step  30  and at epoch =  0  the loss is =  0.06895406544208527  and accuracy is =  0.5416\n",
            "At step  30  and at epoch =  1  the loss is =  0.0587579719722271  and accuracy is =  0.727\n",
            "At step  30  and at epoch =  2  the loss is =  0.07354974001646042  and accuracy is =  0.7826\n",
            "At step  30  and at epoch =  3  the loss is =  0.08204450458288193  and accuracy is =  0.8036\n",
            "At step  30  and at epoch =  4  the loss is =  0.09865909069776535  and accuracy is =  0.823\n",
            "At step  30  and at epoch =  5  the loss is =  0.07680114358663559  and accuracy is =  0.842\n",
            "At step  30  and at epoch =  6  the loss is =  0.07535581290721893  and accuracy is =  0.8408\n",
            "At step  30  and at epoch =  7  the loss is =  0.06895625591278076  and accuracy is =  0.8528\n",
            "At step  30  and at epoch =  8  the loss is =  0.06390964984893799  and accuracy is =  0.8642\n",
            "At step  30  and at epoch =  9  the loss is =  0.05169013515114784  and accuracy is =  0.8712\n",
            "At step  30  and at epoch =  10  the loss is =  0.08243371546268463  and accuracy is =  0.8896\n",
            "At step  30  and at epoch =  11  the loss is =  0.07195942848920822  and accuracy is =  0.8778\n",
            "At step  30  and at epoch =  12  the loss is =  0.06936822831630707  and accuracy is =  0.8916\n",
            "At step  30  and at epoch =  13  the loss is =  0.07816936820745468  and accuracy is =  0.8968\n",
            "At step  30  and at epoch =  14  the loss is =  0.05852644890546799  and accuracy is =  0.8916\n",
            "At step  30  and at epoch =  15  the loss is =  0.07762084901332855  and accuracy is =  0.9136\n",
            "At step  30  and at epoch =  16  the loss is =  0.07464540004730225  and accuracy is =  0.9106\n",
            "At step  30  and at epoch =  17  the loss is =  0.06532202661037445  and accuracy is =  0.9182\n",
            "At step  30  and at epoch =  18  the loss is =  0.06608099490404129  and accuracy is =  0.9168\n",
            "At step  30  and at epoch =  19  the loss is =  0.09398908168077469  and accuracy is =  0.9178\n",
            "At step  30  and at epoch =  20  the loss is =  0.0736403837800026  and accuracy is =  0.8966\n",
            "At step  30  and at epoch =  21  the loss is =  0.06557278335094452  and accuracy is =  0.912\n",
            "At step  30  and at epoch =  22  the loss is =  0.05264846608042717  and accuracy is =  0.9196\n",
            "At step  30  and at epoch =  23  the loss is =  0.05665922909975052  and accuracy is =  0.9346\n",
            "At step  30  and at epoch =  24  the loss is =  0.07563433051109314  and accuracy is =  0.9382\n",
            "At step  30  and at epoch =  25  the loss is =  0.07499974220991135  and accuracy is =  0.9012\n",
            "At step  30  and at epoch =  26  the loss is =  0.0719744935631752  and accuracy is =  0.9126\n",
            "At step  30  and at epoch =  27  the loss is =  0.06332281231880188  and accuracy is =  0.928\n",
            "At step  30  and at epoch =  28  the loss is =  0.0749048963189125  and accuracy is =  0.944\n",
            "At step  30  and at epoch =  29  the loss is =  0.06318063288927078  and accuracy is =  0.9144\n",
            "At step  30  and at epoch =  30  the loss is =  0.0835549458861351  and accuracy is =  0.9304\n",
            "At step  30  and at epoch =  31  the loss is =  0.05170729383826256  and accuracy is =  0.942\n",
            "At step  30  and at epoch =  32  the loss is =  0.06269754469394684  and accuracy is =  0.9512\n",
            "At step  30  and at epoch =  33  the loss is =  0.06801901012659073  and accuracy is =  0.9306\n",
            "At step  30  and at epoch =  34  the loss is =  0.051088325679302216  and accuracy is =  0.9414\n",
            "At step  30  and at epoch =  35  the loss is =  0.06177126616239548  and accuracy is =  0.9578\n",
            "At step  30  and at epoch =  36  the loss is =  0.0626908615231514  and accuracy is =  0.951\n",
            "At step  30  and at epoch =  37  the loss is =  0.05560439079999924  and accuracy is =  0.95\n",
            "At step  30  and at epoch =  38  the loss is =  0.0686863511800766  and accuracy is =  0.9658\n",
            "At step  30  and at epoch =  39  the loss is =  0.06408707797527313  and accuracy is =  0.9642\n",
            "At step  30  and at epoch =  40  the loss is =  0.07692217826843262  and accuracy is =  0.9468\n",
            "At step  30  and at epoch =  41  the loss is =  0.058382630348205566  and accuracy is =  0.9302\n",
            "At step  30  and at epoch =  42  the loss is =  0.07663466036319733  and accuracy is =  0.9466\n",
            "At step  30  and at epoch =  43  the loss is =  0.06286770105361938  and accuracy is =  0.9252\n",
            "At step  30  and at epoch =  44  the loss is =  0.06659366935491562  and accuracy is =  0.9538\n",
            "At step  30  and at epoch =  45  the loss is =  0.07847187668085098  and accuracy is =  0.96\n",
            "At step  30  and at epoch =  46  the loss is =  0.08460225909948349  and accuracy is =  0.9462\n",
            "At step  30  and at epoch =  47  the loss is =  0.061745766550302505  and accuracy is =  0.925\n",
            "At step  30  and at epoch =  48  the loss is =  0.05925292521715164  and accuracy is =  0.9656\n",
            "At step  30  and at epoch =  49  the loss is =  0.05708783119916916  and accuracy is =  0.9772\n",
            "At step  30  and at epoch =  50  the loss is =  0.0524664930999279  and accuracy is =  0.9826\n",
            "At step  30  and at epoch =  51  the loss is =  0.07012319564819336  and accuracy is =  0.9864\n",
            "At step  30  and at epoch =  52  the loss is =  0.05927633121609688  and accuracy is =  0.9872\n",
            "At step  30  and at epoch =  53  the loss is =  0.06586021929979324  and accuracy is =  0.987\n",
            "At step  30  and at epoch =  54  the loss is =  0.05002550780773163  and accuracy is =  0.9828\n",
            "At step  30  and at epoch =  55  the loss is =  0.05432587489485741  and accuracy is =  0.9878\n",
            "At step  30  and at epoch =  56  the loss is =  0.05920524522662163  and accuracy is =  0.989\n",
            "At step  30  and at epoch =  57  the loss is =  0.050157710909843445  and accuracy is =  0.9908\n",
            "At step  30  and at epoch =  58  the loss is =  0.06689027696847916  and accuracy is =  0.989\n",
            "At step  30  and at epoch =  59  the loss is =  0.06047944724559784  and accuracy is =  0.9878\n",
            "At step  30  and at epoch =  60  the loss is =  0.06513762474060059  and accuracy is =  0.9878\n",
            "At step  30  and at epoch =  61  the loss is =  0.0588693805038929  and accuracy is =  0.9846\n",
            "At step  30  and at epoch =  62  the loss is =  0.06295705586671829  and accuracy is =  0.9906\n",
            "At step  30  and at epoch =  63  the loss is =  0.06308536231517792  and accuracy is =  0.9942\n",
            "At step  30  and at epoch =  64  the loss is =  0.07147565484046936  and accuracy is =  0.9934\n",
            "At step  30  and at epoch =  65  the loss is =  0.06984802335500717  and accuracy is =  0.9918\n",
            "At step  30  and at epoch =  66  the loss is =  0.05977765843272209  and accuracy is =  0.9938\n",
            "At step  30  and at epoch =  67  the loss is =  0.051495593041181564  and accuracy is =  0.9926\n",
            "At step  30  and at epoch =  68  the loss is =  0.0701993927359581  and accuracy is =  0.992\n",
            "At step  30  and at epoch =  69  the loss is =  0.05543461814522743  and accuracy is =  0.9926\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "32\n",
            "Validation Loss: 0.0744757130742073 Validation Accuracy : 0.504\n",
            "task = 40 \n",
            "train col =  [65 88 36 93 45 10 90 17 32 59]\n",
            "train col =  [[65 88 36 93 45 10 90 17 32 59]]\n",
            "At step  40  and at epoch =  0  the loss is =  0.10344400256872177  and accuracy is =  0.4532\n",
            "At step  40  and at epoch =  1  the loss is =  0.09449630230665207  and accuracy is =  0.6256\n",
            "At step  40  and at epoch =  2  the loss is =  0.10372161865234375  and accuracy is =  0.6748\n",
            "At step  40  and at epoch =  3  the loss is =  0.09038561582565308  and accuracy is =  0.6926\n",
            "At step  40  and at epoch =  4  the loss is =  0.09983992576599121  and accuracy is =  0.7242\n",
            "At step  40  and at epoch =  5  the loss is =  0.09623142331838608  and accuracy is =  0.7464\n",
            "At step  40  and at epoch =  6  the loss is =  0.08368629217147827  and accuracy is =  0.7582\n",
            "At step  40  and at epoch =  7  the loss is =  0.10707448422908783  and accuracy is =  0.771\n",
            "At step  40  and at epoch =  8  the loss is =  0.08957359939813614  and accuracy is =  0.7746\n",
            "At step  40  and at epoch =  9  the loss is =  0.09436336159706116  and accuracy is =  0.7978\n",
            "At step  40  and at epoch =  10  the loss is =  0.09283733367919922  and accuracy is =  0.8116\n",
            "At step  40  and at epoch =  11  the loss is =  0.08323650062084198  and accuracy is =  0.813\n",
            "At step  40  and at epoch =  12  the loss is =  0.0646984875202179  and accuracy is =  0.828\n",
            "At step  40  and at epoch =  13  the loss is =  0.07384109497070312  and accuracy is =  0.8388\n",
            "At step  40  and at epoch =  14  the loss is =  0.09813991189002991  and accuracy is =  0.856\n",
            "At step  40  and at epoch =  15  the loss is =  0.10429268330335617  and accuracy is =  0.8454\n",
            "At step  40  and at epoch =  16  the loss is =  0.08717189729213715  and accuracy is =  0.8502\n",
            "At step  40  and at epoch =  17  the loss is =  0.0869048684835434  and accuracy is =  0.8632\n",
            "At step  40  and at epoch =  18  the loss is =  0.1038619875907898  and accuracy is =  0.8656\n",
            "At step  40  and at epoch =  19  the loss is =  0.08461957424879074  and accuracy is =  0.874\n",
            "At step  40  and at epoch =  20  the loss is =  0.08492687344551086  and accuracy is =  0.8656\n",
            "At step  40  and at epoch =  21  the loss is =  0.07812647521495819  and accuracy is =  0.8842\n",
            "At step  40  and at epoch =  22  the loss is =  0.08561687171459198  and accuracy is =  0.8896\n",
            "At step  40  and at epoch =  23  the loss is =  0.08370514959096909  and accuracy is =  0.8656\n",
            "At step  40  and at epoch =  24  the loss is =  0.0846962034702301  and accuracy is =  0.8814\n",
            "At step  40  and at epoch =  25  the loss is =  0.09805408120155334  and accuracy is =  0.9022\n",
            "At step  40  and at epoch =  26  the loss is =  0.09745584428310394  and accuracy is =  0.8974\n",
            "At step  40  and at epoch =  27  the loss is =  0.09328073263168335  and accuracy is =  0.8722\n",
            "At step  40  and at epoch =  28  the loss is =  0.10391035676002502  and accuracy is =  0.8886\n",
            "At step  40  and at epoch =  29  the loss is =  0.08277035504579544  and accuracy is =  0.8846\n",
            "At step  40  and at epoch =  30  the loss is =  0.08025050908327103  and accuracy is =  0.9098\n",
            "At step  40  and at epoch =  31  the loss is =  0.08778243511915207  and accuracy is =  0.9228\n",
            "At step  40  and at epoch =  32  the loss is =  0.09142325073480606  and accuracy is =  0.9018\n",
            "At step  40  and at epoch =  33  the loss is =  0.08353877812623978  and accuracy is =  0.9124\n",
            "At step  40  and at epoch =  34  the loss is =  0.08256644755601883  and accuracy is =  0.9096\n",
            "At step  40  and at epoch =  35  the loss is =  0.10301559418439865  and accuracy is =  0.898\n",
            "At step  40  and at epoch =  36  the loss is =  0.07905235141515732  and accuracy is =  0.8958\n",
            "At step  40  and at epoch =  37  the loss is =  0.08376092463731766  and accuracy is =  0.9206\n",
            "At step  40  and at epoch =  38  the loss is =  0.08363161981105804  and accuracy is =  0.9136\n",
            "At step  40  and at epoch =  39  the loss is =  0.08832551538944244  and accuracy is =  0.9178\n",
            "At step  40  and at epoch =  40  the loss is =  0.0899675190448761  and accuracy is =  0.9022\n",
            "At step  40  and at epoch =  41  the loss is =  0.08039359003305435  and accuracy is =  0.922\n",
            "At step  40  and at epoch =  42  the loss is =  0.07581654191017151  and accuracy is =  0.9312\n",
            "At step  40  and at epoch =  43  the loss is =  0.09666923433542252  and accuracy is =  0.9396\n",
            "At step  40  and at epoch =  44  the loss is =  0.08172717690467834  and accuracy is =  0.9162\n",
            "At step  40  and at epoch =  45  the loss is =  0.08303850889205933  and accuracy is =  0.9366\n",
            "At step  40  and at epoch =  46  the loss is =  0.08015305548906326  and accuracy is =  0.9272\n",
            "At step  40  and at epoch =  47  the loss is =  0.08164012432098389  and accuracy is =  0.934\n",
            "At step  40  and at epoch =  48  the loss is =  0.09003186970949173  and accuracy is =  0.9656\n",
            "At step  40  and at epoch =  49  the loss is =  0.09423001855611801  and accuracy is =  0.9758\n",
            "At step  40  and at epoch =  50  the loss is =  0.09136173874139786  and accuracy is =  0.971\n",
            "At step  40  and at epoch =  51  the loss is =  0.09598730504512787  and accuracy is =  0.9778\n",
            "At step  40  and at epoch =  52  the loss is =  0.07718911021947861  and accuracy is =  0.9766\n",
            "At step  40  and at epoch =  53  the loss is =  0.09796110540628433  and accuracy is =  0.9774\n",
            "At step  40  and at epoch =  54  the loss is =  0.08713805675506592  and accuracy is =  0.979\n",
            "At step  40  and at epoch =  55  the loss is =  0.0862414538860321  and accuracy is =  0.9804\n",
            "At step  40  and at epoch =  56  the loss is =  0.07435135543346405  and accuracy is =  0.9814\n",
            "At step  40  and at epoch =  57  the loss is =  0.11691119521856308  and accuracy is =  0.984\n",
            "At step  40  and at epoch =  58  the loss is =  0.08268717676401138  and accuracy is =  0.9766\n",
            "At step  40  and at epoch =  59  the loss is =  0.08163420855998993  and accuracy is =  0.9822\n",
            "At step  40  and at epoch =  60  the loss is =  0.0840759351849556  and accuracy is =  0.9798\n",
            "At step  40  and at epoch =  61  the loss is =  0.08486945927143097  and accuracy is =  0.9828\n",
            "At step  40  and at epoch =  62  the loss is =  0.07098466157913208  and accuracy is =  0.9868\n",
            "At step  40  and at epoch =  63  the loss is =  0.07109487056732178  and accuracy is =  0.9888\n",
            "At step  40  and at epoch =  64  the loss is =  0.10499045997858047  and accuracy is =  0.9878\n",
            "At step  40  and at epoch =  65  the loss is =  0.08079244196414948  and accuracy is =  0.9912\n",
            "At step  40  and at epoch =  66  the loss is =  0.08649173378944397  and accuracy is =  0.9892\n",
            "At step  40  and at epoch =  67  the loss is =  0.07820391654968262  and accuracy is =  0.9906\n",
            "At step  40  and at epoch =  68  the loss is =  0.08968517184257507  and accuracy is =  0.9908\n",
            "At step  40  and at epoch =  69  the loss is =  0.07855606079101562  and accuracy is =  0.989\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "8\n",
            "Validation Loss: 0.08001692593097687 Validation Accuracy : 0.4418\n",
            "task = 50 \n",
            "train col =  [83 43 53 11 86 19 38 30 40 50]\n",
            "train col =  [[83 43 53 11 86 19 38 30 40 50]]\n",
            "At step  50  and at epoch =  0  the loss is =  0.10950067639350891  and accuracy is =  0.5134\n",
            "At step  50  and at epoch =  1  the loss is =  0.1059604212641716  and accuracy is =  0.6998\n",
            "At step  50  and at epoch =  2  the loss is =  0.12783685326576233  and accuracy is =  0.736\n",
            "At step  50  and at epoch =  3  the loss is =  0.11486009508371353  and accuracy is =  0.7514\n",
            "At step  50  and at epoch =  4  the loss is =  0.10587089508771896  and accuracy is =  0.7718\n",
            "At step  50  and at epoch =  5  the loss is =  0.11305543780326843  and accuracy is =  0.799\n",
            "At step  50  and at epoch =  6  the loss is =  0.1035367101430893  and accuracy is =  0.8046\n",
            "At step  50  and at epoch =  7  the loss is =  0.12053583562374115  and accuracy is =  0.8164\n",
            "At step  50  and at epoch =  8  the loss is =  0.11306310445070267  and accuracy is =  0.8192\n",
            "At step  50  and at epoch =  9  the loss is =  0.0978449210524559  and accuracy is =  0.839\n",
            "At step  50  and at epoch =  10  the loss is =  0.10196313261985779  and accuracy is =  0.84\n",
            "At step  50  and at epoch =  11  the loss is =  0.09234023839235306  and accuracy is =  0.8472\n",
            "At step  50  and at epoch =  12  the loss is =  0.1017887070775032  and accuracy is =  0.862\n",
            "At step  50  and at epoch =  13  the loss is =  0.09129204601049423  and accuracy is =  0.8696\n",
            "At step  50  and at epoch =  14  the loss is =  0.11280015856027603  and accuracy is =  0.8728\n",
            "At step  50  and at epoch =  15  the loss is =  0.11072994023561478  and accuracy is =  0.8714\n",
            "At step  50  and at epoch =  16  the loss is =  0.1158066838979721  and accuracy is =  0.8804\n",
            "At step  50  and at epoch =  17  the loss is =  0.08929001539945602  and accuracy is =  0.8884\n",
            "At step  50  and at epoch =  18  the loss is =  0.11032971739768982  and accuracy is =  0.8872\n",
            "At step  50  and at epoch =  19  the loss is =  0.11587655544281006  and accuracy is =  0.8826\n",
            "At step  50  and at epoch =  20  the loss is =  0.11745418608188629  and accuracy is =  0.8824\n",
            "At step  50  and at epoch =  21  the loss is =  0.12453771382570267  and accuracy is =  0.8914\n",
            "At step  50  and at epoch =  22  the loss is =  0.11769193410873413  and accuracy is =  0.8802\n",
            "At step  50  and at epoch =  23  the loss is =  0.11903619766235352  and accuracy is =  0.8808\n",
            "At step  50  and at epoch =  24  the loss is =  0.10275369137525558  and accuracy is =  0.8946\n",
            "At step  50  and at epoch =  25  the loss is =  0.10596559196710587  and accuracy is =  0.9066\n",
            "At step  50  and at epoch =  26  the loss is =  0.10811807215213776  and accuracy is =  0.8972\n",
            "At step  50  and at epoch =  27  the loss is =  0.10020662099123001  and accuracy is =  0.917\n",
            "At step  50  and at epoch =  28  the loss is =  0.09149934351444244  and accuracy is =  0.9198\n",
            "At step  50  and at epoch =  29  the loss is =  0.10836057364940643  and accuracy is =  0.9298\n",
            "At step  50  and at epoch =  30  the loss is =  0.11954723298549652  and accuracy is =  0.9112\n",
            "At step  50  and at epoch =  31  the loss is =  0.10259570926427841  and accuracy is =  0.9152\n",
            "At step  50  and at epoch =  32  the loss is =  0.10176166146993637  and accuracy is =  0.9332\n",
            "At step  50  and at epoch =  33  the loss is =  0.10312256217002869  and accuracy is =  0.9358\n",
            "At step  50  and at epoch =  34  the loss is =  0.0961262509226799  and accuracy is =  0.9052\n",
            "At step  50  and at epoch =  35  the loss is =  0.12519921362400055  and accuracy is =  0.9392\n",
            "At step  50  and at epoch =  36  the loss is =  0.11795178800821304  and accuracy is =  0.9208\n",
            "At step  50  and at epoch =  37  the loss is =  0.1158687099814415  and accuracy is =  0.9292\n",
            "At step  50  and at epoch =  38  the loss is =  0.10376689583063126  and accuracy is =  0.9012\n",
            "At step  50  and at epoch =  39  the loss is =  0.10337769985198975  and accuracy is =  0.9188\n",
            "At step  50  and at epoch =  40  the loss is =  0.10789525508880615  and accuracy is =  0.9332\n",
            "At step  50  and at epoch =  41  the loss is =  0.09366574138402939  and accuracy is =  0.931\n",
            "At step  50  and at epoch =  42  the loss is =  0.10851366072893143  and accuracy is =  0.9518\n",
            "At step  50  and at epoch =  43  the loss is =  0.10286223888397217  and accuracy is =  0.93\n",
            "At step  50  and at epoch =  44  the loss is =  0.12540937960147858  and accuracy is =  0.9364\n",
            "At step  50  and at epoch =  45  the loss is =  0.08800188452005386  and accuracy is =  0.9114\n",
            "At step  50  and at epoch =  46  the loss is =  0.09988215565681458  and accuracy is =  0.9546\n",
            "At step  50  and at epoch =  47  the loss is =  0.09250999987125397  and accuracy is =  0.9452\n",
            "At step  50  and at epoch =  48  the loss is =  0.10465932637453079  and accuracy is =  0.974\n",
            "At step  50  and at epoch =  49  the loss is =  0.11949368566274643  and accuracy is =  0.978\n",
            "At step  50  and at epoch =  50  the loss is =  0.09397535026073456  and accuracy is =  0.9738\n",
            "At step  50  and at epoch =  51  the loss is =  0.09889265149831772  and accuracy is =  0.9768\n",
            "At step  50  and at epoch =  52  the loss is =  0.09887373447418213  and accuracy is =  0.98\n",
            "At step  50  and at epoch =  53  the loss is =  0.10325974971055984  and accuracy is =  0.9842\n",
            "At step  50  and at epoch =  54  the loss is =  0.08575202524662018  and accuracy is =  0.981\n",
            "At step  50  and at epoch =  55  the loss is =  0.09671754390001297  and accuracy is =  0.9822\n",
            "At step  50  and at epoch =  56  the loss is =  0.09766820818185806  and accuracy is =  0.9824\n",
            "At step  50  and at epoch =  57  the loss is =  0.09658637642860413  and accuracy is =  0.9838\n",
            "At step  50  and at epoch =  58  the loss is =  0.10626370459794998  and accuracy is =  0.9788\n",
            "At step  50  and at epoch =  59  the loss is =  0.10366349667310715  and accuracy is =  0.9832\n",
            "At step  50  and at epoch =  60  the loss is =  0.08948735147714615  and accuracy is =  0.9804\n",
            "At step  50  and at epoch =  61  the loss is =  0.10308250039815903  and accuracy is =  0.9866\n",
            "At step  50  and at epoch =  62  the loss is =  0.1026328057050705  and accuracy is =  0.9896\n",
            "At step  50  and at epoch =  63  the loss is =  0.10991515964269638  and accuracy is =  0.9894\n",
            "At step  50  and at epoch =  64  the loss is =  0.10269976407289505  and accuracy is =  0.9876\n",
            "At step  50  and at epoch =  65  the loss is =  0.09975327551364899  and accuracy is =  0.9902\n",
            "At step  50  and at epoch =  66  the loss is =  0.09549277275800705  and accuracy is =  0.99\n",
            "At step  50  and at epoch =  67  the loss is =  0.10244835913181305  and accuracy is =  0.99\n",
            "At step  50  and at epoch =  68  the loss is =  0.09069351106882095  and accuracy is =  0.9892\n",
            "At step  50  and at epoch =  69  the loss is =  0.11069070547819138  and accuracy is =  0.9892\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "112\n",
            "Validation Loss: 0.08970896899700165 Validation Accuracy : 0.399\n",
            "task = 60 \n",
            "train col =  [57 81 12 95 25 47 34 52 44 72]\n",
            "train col =  [[57 81 12 95 25 47 34 52 44 72]]\n",
            "At step  60  and at epoch =  0  the loss is =  0.11992142349481583  and accuracy is =  0.5506\n",
            "At step  60  and at epoch =  1  the loss is =  0.11837804317474365  and accuracy is =  0.7158\n",
            "At step  60  and at epoch =  2  the loss is =  0.12368730455636978  and accuracy is =  0.745\n",
            "At step  60  and at epoch =  3  the loss is =  0.1513195037841797  and accuracy is =  0.7576\n",
            "At step  60  and at epoch =  4  the loss is =  0.12459909915924072  and accuracy is =  0.7692\n",
            "At step  60  and at epoch =  5  the loss is =  0.11966945230960846  and accuracy is =  0.7896\n",
            "At step  60  and at epoch =  6  the loss is =  0.12325235456228256  and accuracy is =  0.7976\n",
            "At step  60  and at epoch =  7  the loss is =  0.13481484353542328  and accuracy is =  0.8028\n",
            "At step  60  and at epoch =  8  the loss is =  0.14232821762561798  and accuracy is =  0.8072\n",
            "At step  60  and at epoch =  9  the loss is =  0.12346238642930984  and accuracy is =  0.8122\n",
            "At step  60  and at epoch =  10  the loss is =  0.1181897297501564  and accuracy is =  0.8192\n",
            "At step  60  and at epoch =  11  the loss is =  0.11356107145547867  and accuracy is =  0.8338\n",
            "At step  60  and at epoch =  12  the loss is =  0.11327574402093887  and accuracy is =  0.8454\n",
            "At step  60  and at epoch =  13  the loss is =  0.11851917952299118  and accuracy is =  0.85\n",
            "At step  60  and at epoch =  14  the loss is =  0.12049101293087006  and accuracy is =  0.849\n",
            "At step  60  and at epoch =  15  the loss is =  0.1412619650363922  and accuracy is =  0.8586\n",
            "At step  60  and at epoch =  16  the loss is =  0.12144196033477783  and accuracy is =  0.8518\n",
            "At step  60  and at epoch =  17  the loss is =  0.1455131620168686  and accuracy is =  0.8668\n",
            "At step  60  and at epoch =  18  the loss is =  0.12056990712881088  and accuracy is =  0.8512\n",
            "At step  60  and at epoch =  19  the loss is =  0.15325382351875305  and accuracy is =  0.8748\n",
            "At step  60  and at epoch =  20  the loss is =  0.11915116757154465  and accuracy is =  0.8704\n",
            "At step  60  and at epoch =  21  the loss is =  0.11958732455968857  and accuracy is =  0.8736\n",
            "At step  60  and at epoch =  22  the loss is =  0.13057565689086914  and accuracy is =  0.8874\n",
            "At step  60  and at epoch =  23  the loss is =  0.11715757101774216  and accuracy is =  0.8718\n",
            "At step  60  and at epoch =  24  the loss is =  0.11678191274404526  and accuracy is =  0.8914\n",
            "At step  60  and at epoch =  25  the loss is =  0.11548454314470291  and accuracy is =  0.8848\n",
            "At step  60  and at epoch =  26  the loss is =  0.15579338371753693  and accuracy is =  0.891\n",
            "At step  60  and at epoch =  27  the loss is =  0.14061759412288666  and accuracy is =  0.8658\n",
            "At step  60  and at epoch =  28  the loss is =  0.12288592010736465  and accuracy is =  0.8902\n",
            "At step  60  and at epoch =  29  the loss is =  0.11453410238027573  and accuracy is =  0.8902\n",
            "At step  60  and at epoch =  30  the loss is =  0.14581206440925598  and accuracy is =  0.9122\n",
            "At step  60  and at epoch =  31  the loss is =  0.1381198614835739  and accuracy is =  0.8918\n",
            "At step  60  and at epoch =  32  the loss is =  0.11356989294290543  and accuracy is =  0.907\n",
            "At step  60  and at epoch =  33  the loss is =  0.11640170961618423  and accuracy is =  0.9126\n",
            "At step  60  and at epoch =  34  the loss is =  0.10498978197574615  and accuracy is =  0.899\n",
            "At step  60  and at epoch =  35  the loss is =  0.11884304136037827  and accuracy is =  0.9222\n",
            "At step  60  and at epoch =  36  the loss is =  0.1266581267118454  and accuracy is =  0.9246\n",
            "At step  60  and at epoch =  37  the loss is =  0.12586969137191772  and accuracy is =  0.9046\n",
            "At step  60  and at epoch =  38  the loss is =  0.13234615325927734  and accuracy is =  0.9154\n",
            "At step  60  and at epoch =  39  the loss is =  0.09948905557394028  and accuracy is =  0.9106\n",
            "At step  60  and at epoch =  40  the loss is =  0.1411217451095581  and accuracy is =  0.9324\n",
            "At step  60  and at epoch =  41  the loss is =  0.11483451724052429  and accuracy is =  0.9182\n",
            "At step  60  and at epoch =  42  the loss is =  0.12441886961460114  and accuracy is =  0.9316\n",
            "At step  60  and at epoch =  43  the loss is =  0.13883636891841888  and accuracy is =  0.9122\n",
            "At step  60  and at epoch =  44  the loss is =  0.12417665123939514  and accuracy is =  0.9332\n",
            "At step  60  and at epoch =  45  the loss is =  0.13765080273151398  and accuracy is =  0.9\n",
            "At step  60  and at epoch =  46  the loss is =  0.12147697806358337  and accuracy is =  0.891\n",
            "At step  60  and at epoch =  47  the loss is =  0.11345726996660233  and accuracy is =  0.9128\n",
            "At step  60  and at epoch =  48  the loss is =  0.13306455314159393  and accuracy is =  0.9558\n",
            "At step  60  and at epoch =  49  the loss is =  0.1250145435333252  and accuracy is =  0.958\n",
            "At step  60  and at epoch =  50  the loss is =  0.14714138209819794  and accuracy is =  0.964\n",
            "At step  60  and at epoch =  51  the loss is =  0.11052869260311127  and accuracy is =  0.9612\n",
            "At step  60  and at epoch =  52  the loss is =  0.10880355536937714  and accuracy is =  0.9664\n",
            "At step  60  and at epoch =  53  the loss is =  0.12584689259529114  and accuracy is =  0.9676\n",
            "At step  60  and at epoch =  54  the loss is =  0.11823740601539612  and accuracy is =  0.9684\n",
            "At step  60  and at epoch =  55  the loss is =  0.09705659747123718  and accuracy is =  0.9726\n",
            "At step  60  and at epoch =  56  the loss is =  0.1257648766040802  and accuracy is =  0.9722\n",
            "At step  60  and at epoch =  57  the loss is =  0.12965987622737885  and accuracy is =  0.9716\n",
            "At step  60  and at epoch =  58  the loss is =  0.11111976206302643  and accuracy is =  0.974\n",
            "At step  60  and at epoch =  59  the loss is =  0.1266719400882721  and accuracy is =  0.975\n",
            "At step  60  and at epoch =  60  the loss is =  0.12797479331493378  and accuracy is =  0.9756\n",
            "At step  60  and at epoch =  61  the loss is =  0.12115947157144547  and accuracy is =  0.9726\n",
            "At step  60  and at epoch =  62  the loss is =  0.15442976355552673  and accuracy is =  0.9782\n",
            "At step  60  and at epoch =  63  the loss is =  0.13446776568889618  and accuracy is =  0.9828\n",
            "At step  60  and at epoch =  64  the loss is =  0.10843414813280106  and accuracy is =  0.9838\n",
            "At step  60  and at epoch =  65  the loss is =  0.14361613988876343  and accuracy is =  0.9832\n",
            "At step  60  and at epoch =  66  the loss is =  0.1262398064136505  and accuracy is =  0.9844\n",
            "At step  60  and at epoch =  67  the loss is =  0.11424858123064041  and accuracy is =  0.9838\n",
            "At step  60  and at epoch =  68  the loss is =  0.11801168322563171  and accuracy is =  0.982\n",
            "At step  60  and at epoch =  69  the loss is =  0.09228135645389557  and accuracy is =  0.9828\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "88\n",
            "Validation Loss: 0.10776004195213318 Validation Accuracy : 0.32771428571428574\n",
            "task = 70 \n",
            "train col =  [46 79 20 28  5 71  8 18 33 15]\n",
            "train col =  [[46 79 20 28  5 71  8 18 33 15]]\n",
            "At step  70  and at epoch =  0  the loss is =  0.1659255027770996  and accuracy is =  0.5752\n",
            "At step  70  and at epoch =  1  the loss is =  0.1472710520029068  and accuracy is =  0.7618\n",
            "At step  70  and at epoch =  2  the loss is =  0.14301419258117676  and accuracy is =  0.796\n",
            "At step  70  and at epoch =  3  the loss is =  0.15298692882061005  and accuracy is =  0.817\n",
            "At step  70  and at epoch =  4  the loss is =  0.13820451498031616  and accuracy is =  0.8212\n",
            "At step  70  and at epoch =  5  the loss is =  0.13778668642044067  and accuracy is =  0.8458\n",
            "At step  70  and at epoch =  6  the loss is =  0.1311812698841095  and accuracy is =  0.8554\n",
            "At step  70  and at epoch =  7  the loss is =  0.13268791139125824  and accuracy is =  0.8694\n",
            "At step  70  and at epoch =  8  the loss is =  0.12698118388652802  and accuracy is =  0.878\n",
            "At step  70  and at epoch =  9  the loss is =  0.14457425475120544  and accuracy is =  0.8862\n",
            "At step  70  and at epoch =  10  the loss is =  0.17396722733974457  and accuracy is =  0.8804\n",
            "At step  70  and at epoch =  11  the loss is =  0.152652308344841  and accuracy is =  0.8752\n",
            "At step  70  and at epoch =  12  the loss is =  0.135639950633049  and accuracy is =  0.8962\n",
            "At step  70  and at epoch =  13  the loss is =  0.1530950665473938  and accuracy is =  0.9\n",
            "At step  70  and at epoch =  14  the loss is =  0.13267137110233307  and accuracy is =  0.8972\n",
            "At step  70  and at epoch =  15  the loss is =  0.15456941723823547  and accuracy is =  0.9056\n",
            "At step  70  and at epoch =  16  the loss is =  0.13826622068881989  and accuracy is =  0.9062\n",
            "At step  70  and at epoch =  17  the loss is =  0.14830085635185242  and accuracy is =  0.9104\n",
            "At step  70  and at epoch =  18  the loss is =  0.13400399684906006  and accuracy is =  0.9134\n",
            "At step  70  and at epoch =  19  the loss is =  0.13141129910945892  and accuracy is =  0.9204\n",
            "At step  70  and at epoch =  20  the loss is =  0.14714951813220978  and accuracy is =  0.9248\n",
            "At step  70  and at epoch =  21  the loss is =  0.15486837923526764  and accuracy is =  0.9238\n",
            "At step  70  and at epoch =  22  the loss is =  0.15438617765903473  and accuracy is =  0.931\n",
            "At step  70  and at epoch =  23  the loss is =  0.14319273829460144  and accuracy is =  0.9266\n",
            "At step  70  and at epoch =  24  the loss is =  0.14903582632541656  and accuracy is =  0.9362\n",
            "At step  70  and at epoch =  25  the loss is =  0.15835987031459808  and accuracy is =  0.9298\n",
            "At step  70  and at epoch =  26  the loss is =  0.13473553955554962  and accuracy is =  0.9286\n",
            "At step  70  and at epoch =  27  the loss is =  0.1597919911146164  and accuracy is =  0.933\n",
            "At step  70  and at epoch =  28  the loss is =  0.13005687296390533  and accuracy is =  0.9304\n",
            "At step  70  and at epoch =  29  the loss is =  0.14650489389896393  and accuracy is =  0.9372\n",
            "At step  70  and at epoch =  30  the loss is =  0.13898076117038727  and accuracy is =  0.9406\n",
            "At step  70  and at epoch =  31  the loss is =  0.1357845813035965  and accuracy is =  0.9522\n",
            "At step  70  and at epoch =  32  the loss is =  0.15790386497974396  and accuracy is =  0.9566\n",
            "At step  70  and at epoch =  33  the loss is =  0.1537594050168991  and accuracy is =  0.935\n",
            "At step  70  and at epoch =  34  the loss is =  0.14391817152500153  and accuracy is =  0.9368\n",
            "At step  70  and at epoch =  35  the loss is =  0.13845451176166534  and accuracy is =  0.9442\n",
            "At step  70  and at epoch =  36  the loss is =  0.16136690974235535  and accuracy is =  0.9268\n",
            "At step  70  and at epoch =  37  the loss is =  0.15530791878700256  and accuracy is =  0.9362\n",
            "At step  70  and at epoch =  38  the loss is =  0.14195124804973602  and accuracy is =  0.9458\n",
            "At step  70  and at epoch =  39  the loss is =  0.14082284271717072  and accuracy is =  0.9552\n",
            "At step  70  and at epoch =  40  the loss is =  0.1279410868883133  and accuracy is =  0.9388\n",
            "At step  70  and at epoch =  41  the loss is =  0.12944607436656952  and accuracy is =  0.9622\n",
            "At step  70  and at epoch =  42  the loss is =  0.11667085438966751  and accuracy is =  0.9596\n",
            "At step  70  and at epoch =  43  the loss is =  0.14198201894760132  and accuracy is =  0.9678\n",
            "At step  70  and at epoch =  44  the loss is =  0.1404677778482437  and accuracy is =  0.968\n",
            "At step  70  and at epoch =  45  the loss is =  0.1322554051876068  and accuracy is =  0.959\n",
            "At step  70  and at epoch =  46  the loss is =  0.13044573366641998  and accuracy is =  0.9696\n",
            "At step  70  and at epoch =  47  the loss is =  0.13694384694099426  and accuracy is =  0.9638\n",
            "At step  70  and at epoch =  48  the loss is =  0.15100108087062836  and accuracy is =  0.972\n",
            "At step  70  and at epoch =  49  the loss is =  0.14029264450073242  and accuracy is =  0.9846\n",
            "At step  70  and at epoch =  50  the loss is =  0.13218118250370026  and accuracy is =  0.986\n",
            "At step  70  and at epoch =  51  the loss is =  0.13879914581775665  and accuracy is =  0.9874\n",
            "At step  70  and at epoch =  52  the loss is =  0.13645212352275848  and accuracy is =  0.988\n",
            "At step  70  and at epoch =  53  the loss is =  0.15524551272392273  and accuracy is =  0.9874\n",
            "At step  70  and at epoch =  54  the loss is =  0.1263231486082077  and accuracy is =  0.9894\n",
            "At step  70  and at epoch =  55  the loss is =  0.16290810704231262  and accuracy is =  0.9888\n",
            "At step  70  and at epoch =  56  the loss is =  0.14323444664478302  and accuracy is =  0.984\n",
            "At step  70  and at epoch =  57  the loss is =  0.1549798846244812  and accuracy is =  0.9884\n",
            "At step  70  and at epoch =  58  the loss is =  0.1989791989326477  and accuracy is =  0.9856\n",
            "At step  70  and at epoch =  59  the loss is =  0.12596385180950165  and accuracy is =  0.9824\n",
            "At step  70  and at epoch =  60  the loss is =  0.14733470976352692  and accuracy is =  0.9928\n",
            "At step  70  and at epoch =  61  the loss is =  0.1371559053659439  and accuracy is =  0.9874\n",
            "At step  70  and at epoch =  62  the loss is =  0.15360061824321747  and accuracy is =  0.991\n",
            "At step  70  and at epoch =  63  the loss is =  0.13157348334789276  and accuracy is =  0.9894\n",
            "At step  70  and at epoch =  64  the loss is =  0.14217665791511536  and accuracy is =  0.993\n",
            "At step  70  and at epoch =  65  the loss is =  0.13959680497646332  and accuracy is =  0.992\n",
            "At step  70  and at epoch =  66  the loss is =  0.15727947652339935  and accuracy is =  0.9908\n",
            "At step  70  and at epoch =  67  the loss is =  0.1412704586982727  and accuracy is =  0.9902\n",
            "At step  70  and at epoch =  68  the loss is =  0.14876775443553925  and accuracy is =  0.9924\n",
            "At step  70  and at epoch =  69  the loss is =  0.14229707419872284  and accuracy is =  0.9926\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "64\n",
            "Validation Loss: 0.11937987804412842 Validation Accuracy : 0.299\n",
            "task = 80 \n",
            "train col =  [55 29 64 31 67  7 13 14 42  6]\n",
            "train col =  [[55 29 64 31 67  7 13 14 42  6]]\n",
            "At step  80  and at epoch =  0  the loss is =  0.1812763512134552  and accuracy is =  0.5018\n",
            "At step  80  and at epoch =  1  the loss is =  0.17104342579841614  and accuracy is =  0.6536\n",
            "At step  80  and at epoch =  2  the loss is =  0.1565704494714737  and accuracy is =  0.691\n",
            "At step  80  and at epoch =  3  the loss is =  0.169941246509552  and accuracy is =  0.7132\n",
            "At step  80  and at epoch =  4  the loss is =  0.18354985117912292  and accuracy is =  0.7274\n",
            "At step  80  and at epoch =  5  the loss is =  0.17434722185134888  and accuracy is =  0.7402\n",
            "At step  80  and at epoch =  6  the loss is =  0.187224879860878  and accuracy is =  0.7466\n",
            "At step  80  and at epoch =  7  the loss is =  0.1862189769744873  and accuracy is =  0.7642\n",
            "At step  80  and at epoch =  8  the loss is =  0.17213745415210724  and accuracy is =  0.7802\n",
            "At step  80  and at epoch =  9  the loss is =  0.19978149235248566  and accuracy is =  0.7898\n",
            "At step  80  and at epoch =  10  the loss is =  0.17809909582138062  and accuracy is =  0.7846\n",
            "At step  80  and at epoch =  11  the loss is =  0.16972370445728302  and accuracy is =  0.8014\n",
            "At step  80  and at epoch =  12  the loss is =  0.17919589579105377  and accuracy is =  0.807\n",
            "At step  80  and at epoch =  13  the loss is =  0.1577388197183609  and accuracy is =  0.812\n",
            "At step  80  and at epoch =  14  the loss is =  0.15736334025859833  and accuracy is =  0.8314\n",
            "At step  80  and at epoch =  15  the loss is =  0.18063004314899445  and accuracy is =  0.831\n",
            "At step  80  and at epoch =  16  the loss is =  0.18399551510810852  and accuracy is =  0.8348\n",
            "At step  80  and at epoch =  17  the loss is =  0.1597997099161148  and accuracy is =  0.8292\n",
            "At step  80  and at epoch =  18  the loss is =  0.15263840556144714  and accuracy is =  0.8492\n",
            "At step  80  and at epoch =  19  the loss is =  0.171854168176651  and accuracy is =  0.8678\n",
            "At step  80  and at epoch =  20  the loss is =  0.16303928196430206  and accuracy is =  0.852\n",
            "At step  80  and at epoch =  21  the loss is =  0.18661795556545258  and accuracy is =  0.8718\n",
            "At step  80  and at epoch =  22  the loss is =  0.16195964813232422  and accuracy is =  0.8698\n",
            "At step  80  and at epoch =  23  the loss is =  0.19119307398796082  and accuracy is =  0.881\n",
            "At step  80  and at epoch =  24  the loss is =  0.17435401678085327  and accuracy is =  0.8742\n",
            "At step  80  and at epoch =  25  the loss is =  0.17146921157836914  and accuracy is =  0.872\n",
            "At step  80  and at epoch =  26  the loss is =  0.17059606313705444  and accuracy is =  0.8932\n",
            "At step  80  and at epoch =  27  the loss is =  0.1668587327003479  and accuracy is =  0.8838\n",
            "At step  80  and at epoch =  28  the loss is =  0.17673702538013458  and accuracy is =  0.8866\n",
            "At step  80  and at epoch =  29  the loss is =  0.15698516368865967  and accuracy is =  0.8842\n",
            "At step  80  and at epoch =  30  the loss is =  0.138925239443779  and accuracy is =  0.8996\n",
            "At step  80  and at epoch =  31  the loss is =  0.1690727323293686  and accuracy is =  0.8984\n",
            "At step  80  and at epoch =  32  the loss is =  0.1639871895313263  and accuracy is =  0.8926\n",
            "At step  80  and at epoch =  33  the loss is =  0.15384109318256378  and accuracy is =  0.9098\n",
            "At step  80  and at epoch =  34  the loss is =  0.1562207043170929  and accuracy is =  0.9148\n",
            "At step  80  and at epoch =  35  the loss is =  0.15584638714790344  and accuracy is =  0.9192\n",
            "At step  80  and at epoch =  36  the loss is =  0.17184975743293762  and accuracy is =  0.9036\n",
            "At step  80  and at epoch =  37  the loss is =  0.1674993932247162  and accuracy is =  0.9168\n",
            "At step  80  and at epoch =  38  the loss is =  0.16993339359760284  and accuracy is =  0.9222\n",
            "At step  80  and at epoch =  39  the loss is =  0.16316981613636017  and accuracy is =  0.9244\n",
            "At step  80  and at epoch =  40  the loss is =  0.1706245392560959  and accuracy is =  0.9238\n",
            "At step  80  and at epoch =  41  the loss is =  0.1770354062318802  and accuracy is =  0.924\n",
            "At step  80  and at epoch =  42  the loss is =  0.17115327715873718  and accuracy is =  0.9154\n",
            "At step  80  and at epoch =  43  the loss is =  0.15313686430454254  and accuracy is =  0.8934\n",
            "At step  80  and at epoch =  44  the loss is =  0.1772201806306839  and accuracy is =  0.9126\n",
            "At step  80  and at epoch =  45  the loss is =  0.1687839925289154  and accuracy is =  0.932\n",
            "At step  80  and at epoch =  46  the loss is =  0.16346614062786102  and accuracy is =  0.9316\n",
            "At step  80  and at epoch =  47  the loss is =  0.15786272287368774  and accuracy is =  0.9384\n",
            "At step  80  and at epoch =  48  the loss is =  0.15332913398742676  and accuracy is =  0.9652\n",
            "At step  80  and at epoch =  49  the loss is =  0.16235890984535217  and accuracy is =  0.9766\n",
            "At step  80  and at epoch =  50  the loss is =  0.156577467918396  and accuracy is =  0.9774\n",
            "At step  80  and at epoch =  51  the loss is =  0.16070273518562317  and accuracy is =  0.9772\n",
            "At step  80  and at epoch =  52  the loss is =  0.16824403405189514  and accuracy is =  0.9778\n",
            "At step  80  and at epoch =  53  the loss is =  0.15667369961738586  and accuracy is =  0.978\n",
            "At step  80  and at epoch =  54  the loss is =  0.15179046988487244  and accuracy is =  0.9748\n",
            "At step  80  and at epoch =  55  the loss is =  0.15535730123519897  and accuracy is =  0.9808\n",
            "At step  80  and at epoch =  56  the loss is =  0.15067778527736664  and accuracy is =  0.9806\n",
            "At step  80  and at epoch =  57  the loss is =  0.14692354202270508  and accuracy is =  0.98\n",
            "At step  80  and at epoch =  58  the loss is =  0.18599748611450195  and accuracy is =  0.9812\n",
            "At step  80  and at epoch =  59  the loss is =  0.18125680088996887  and accuracy is =  0.971\n",
            "At step  80  and at epoch =  60  the loss is =  0.15636979043483734  and accuracy is =  0.9802\n",
            "At step  80  and at epoch =  61  the loss is =  0.15605245530605316  and accuracy is =  0.9816\n",
            "At step  80  and at epoch =  62  the loss is =  0.17492930591106415  and accuracy is =  0.9856\n",
            "At step  80  and at epoch =  63  the loss is =  0.15108628571033478  and accuracy is =  0.987\n",
            "At step  80  and at epoch =  64  the loss is =  0.1619190126657486  and accuracy is =  0.9856\n",
            "At step  80  and at epoch =  65  the loss is =  0.14322690665721893  and accuracy is =  0.9872\n",
            "At step  80  and at epoch =  66  the loss is =  0.15271282196044922  and accuracy is =  0.9876\n",
            "At step  80  and at epoch =  67  the loss is =  0.1598321795463562  and accuracy is =  0.989\n",
            "At step  80  and at epoch =  68  the loss is =  0.1667754203081131  and accuracy is =  0.9878\n",
            "At step  80  and at epoch =  69  the loss is =  0.15452827513217926  and accuracy is =  0.988\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "40\n",
            "Validation Loss: 0.12972238659858704 Validation Accuracy : 0.26211111111111113\n",
            "task = 90 \n",
            "train col =  [82  2 27 16 26  3  4  1  9  0]\n",
            "train col =  [[82  2 27 16 26  3  4  1  9  0]]\n",
            "At step  90  and at epoch =  0  the loss is =  0.19946923851966858  and accuracy is =  0.5354\n",
            "At step  90  and at epoch =  1  the loss is =  0.19897067546844482  and accuracy is =  0.7234\n",
            "At step  90  and at epoch =  2  the loss is =  0.20655298233032227  and accuracy is =  0.76\n",
            "At step  90  and at epoch =  3  the loss is =  0.20487569272518158  and accuracy is =  0.7754\n",
            "At step  90  and at epoch =  4  the loss is =  0.19575603306293488  and accuracy is =  0.7968\n",
            "At step  90  and at epoch =  5  the loss is =  0.20079901814460754  and accuracy is =  0.8084\n",
            "At step  90  and at epoch =  6  the loss is =  0.18567641079425812  and accuracy is =  0.8244\n",
            "At step  90  and at epoch =  7  the loss is =  0.19347766041755676  and accuracy is =  0.8306\n",
            "At step  90  and at epoch =  8  the loss is =  0.18004369735717773  and accuracy is =  0.8436\n",
            "At step  90  and at epoch =  9  the loss is =  0.19745607674121857  and accuracy is =  0.8412\n",
            "At step  90  and at epoch =  10  the loss is =  0.20022808015346527  and accuracy is =  0.855\n",
            "At step  90  and at epoch =  11  the loss is =  0.20436634123325348  and accuracy is =  0.859\n",
            "At step  90  and at epoch =  12  the loss is =  0.19210372865200043  and accuracy is =  0.8652\n",
            "At step  90  and at epoch =  13  the loss is =  0.17152738571166992  and accuracy is =  0.8706\n",
            "At step  90  and at epoch =  14  the loss is =  0.18463578820228577  and accuracy is =  0.8858\n",
            "At step  90  and at epoch =  15  the loss is =  0.2017713338136673  and accuracy is =  0.889\n",
            "At step  90  and at epoch =  16  the loss is =  0.19306646287441254  and accuracy is =  0.8658\n",
            "At step  90  and at epoch =  17  the loss is =  0.16821475327014923  and accuracy is =  0.8852\n",
            "At step  90  and at epoch =  18  the loss is =  0.17258571088314056  and accuracy is =  0.8886\n",
            "At step  90  and at epoch =  19  the loss is =  0.19429419934749603  and accuracy is =  0.8952\n",
            "At step  90  and at epoch =  20  the loss is =  0.16817443072795868  and accuracy is =  0.8964\n",
            "At step  90  and at epoch =  21  the loss is =  0.18599650263786316  and accuracy is =  0.9056\n",
            "At step  90  and at epoch =  22  the loss is =  0.20959843695163727  and accuracy is =  0.9084\n",
            "At step  90  and at epoch =  23  the loss is =  0.20779643952846527  and accuracy is =  0.8976\n",
            "At step  90  and at epoch =  24  the loss is =  0.16431766748428345  and accuracy is =  0.9042\n",
            "At step  90  and at epoch =  25  the loss is =  0.2224562019109726  and accuracy is =  0.9214\n",
            "At step  90  and at epoch =  26  the loss is =  0.1809857338666916  and accuracy is =  0.891\n",
            "At step  90  and at epoch =  27  the loss is =  0.17062556743621826  and accuracy is =  0.911\n",
            "At step  90  and at epoch =  28  the loss is =  0.20300370454788208  and accuracy is =  0.9238\n",
            "At step  90  and at epoch =  29  the loss is =  0.1714271754026413  and accuracy is =  0.9062\n",
            "At step  90  and at epoch =  30  the loss is =  0.17291252315044403  and accuracy is =  0.9318\n",
            "At step  90  and at epoch =  31  the loss is =  0.16746237874031067  and accuracy is =  0.933\n",
            "At step  90  and at epoch =  32  the loss is =  0.2096613198518753  and accuracy is =  0.9308\n",
            "At step  90  and at epoch =  33  the loss is =  0.19590215384960175  and accuracy is =  0.926\n",
            "At step  90  and at epoch =  34  the loss is =  0.1993354707956314  and accuracy is =  0.9248\n",
            "At step  90  and at epoch =  35  the loss is =  0.17392948269844055  and accuracy is =  0.9302\n",
            "At step  90  and at epoch =  36  the loss is =  0.17514903843402863  and accuracy is =  0.9372\n",
            "At step  90  and at epoch =  37  the loss is =  0.18644976615905762  and accuracy is =  0.9442\n",
            "At step  90  and at epoch =  38  the loss is =  0.16610832512378693  and accuracy is =  0.9448\n",
            "At step  90  and at epoch =  39  the loss is =  0.1906863898038864  and accuracy is =  0.9468\n",
            "At step  90  and at epoch =  40  the loss is =  0.18639005720615387  and accuracy is =  0.9392\n",
            "At step  90  and at epoch =  41  the loss is =  0.18662118911743164  and accuracy is =  0.9276\n",
            "At step  90  and at epoch =  42  the loss is =  0.19073019921779633  and accuracy is =  0.926\n",
            "At step  90  and at epoch =  43  the loss is =  0.1765056997537613  and accuracy is =  0.9502\n",
            "At step  90  and at epoch =  44  the loss is =  0.1877589374780655  and accuracy is =  0.945\n",
            "At step  90  and at epoch =  45  the loss is =  0.17733697593212128  and accuracy is =  0.9474\n",
            "At step  90  and at epoch =  46  the loss is =  0.19459117949008942  and accuracy is =  0.9494\n",
            "At step  90  and at epoch =  47  the loss is =  0.20649223029613495  and accuracy is =  0.9544\n",
            "At step  90  and at epoch =  48  the loss is =  0.17018508911132812  and accuracy is =  0.9724\n",
            "At step  90  and at epoch =  49  the loss is =  0.19217412173748016  and accuracy is =  0.9792\n",
            "At step  90  and at epoch =  50  the loss is =  0.18065372109413147  and accuracy is =  0.9822\n",
            "At step  90  and at epoch =  51  the loss is =  0.18158411979675293  and accuracy is =  0.9814\n",
            "At step  90  and at epoch =  52  the loss is =  0.1748674362897873  and accuracy is =  0.983\n",
            "At step  90  and at epoch =  53  the loss is =  0.1728409230709076  and accuracy is =  0.9862\n",
            "At step  90  and at epoch =  54  the loss is =  0.17999573051929474  and accuracy is =  0.9866\n",
            "At step  90  and at epoch =  55  the loss is =  0.18916064500808716  and accuracy is =  0.9844\n",
            "At step  90  and at epoch =  56  the loss is =  0.17646163702011108  and accuracy is =  0.984\n",
            "At step  90  and at epoch =  57  the loss is =  0.19453613460063934  and accuracy is =  0.9856\n",
            "At step  90  and at epoch =  58  the loss is =  0.18931937217712402  and accuracy is =  0.9884\n",
            "At step  90  and at epoch =  59  the loss is =  0.18830673396587372  and accuracy is =  0.9874\n",
            "At step  90  and at epoch =  60  the loss is =  0.17623962461948395  and accuracy is =  0.984\n",
            "At step  90  and at epoch =  61  the loss is =  0.1663365513086319  and accuracy is =  0.9852\n",
            "At step  90  and at epoch =  62  the loss is =  0.18907706439495087  and accuracy is =  0.9914\n",
            "At step  90  and at epoch =  63  the loss is =  0.1937836855649948  and accuracy is =  0.9918\n",
            "At step  90  and at epoch =  64  the loss is =  0.23057352006435394  and accuracy is =  0.989\n",
            "At step  90  and at epoch =  65  the loss is =  0.17143931984901428  and accuracy is =  0.9894\n",
            "At step  90  and at epoch =  66  the loss is =  0.20978446304798126  and accuracy is =  0.9922\n",
            "At step  90  and at epoch =  67  the loss is =  0.1848326027393341  and accuracy is =  0.992\n",
            "At step  90  and at epoch =  68  the loss is =  0.1718057245016098  and accuracy is =  0.9932\n",
            "At step  90  and at epoch =  69  the loss is =  0.17591792345046997  and accuracy is =  0.9904\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "16\n",
            "Validation Loss: 0.12680700421333313 Validation Accuracy : 0.2471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptf2qZjFbWNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d583053b-1743-4d76-8793-6ce09515e7f4"
      },
      "source": [
        "plotTask(pars_tasks)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}