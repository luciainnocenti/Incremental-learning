{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainAndTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "916122c844e947c88db797a4a49d8221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75b70705bffa43d49d31db618ddeae93",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_80504a69f92e4fd3b601c30e960165f4",
              "IPY_MODEL_c36d624f1b824bc0b172cb85bbc7438e"
            ]
          }
        },
        "75b70705bffa43d49d31db618ddeae93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80504a69f92e4fd3b601c30e960165f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9da1b14930d447c5a3bc16f8912d3c51",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a91e70225b9b4e9683099f59fa83e056"
          }
        },
        "c36d624f1b824bc0b172cb85bbc7438e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd51d810c8994d11909ece2e759b53cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:07&lt;00:00, 23447093.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da2f4045df634a4b97d22ebe669ee6d4"
          }
        },
        "9da1b14930d447c5a3bc16f8912d3c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a91e70225b9b4e9683099f59fa83e056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd51d810c8994d11909ece2e759b53cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da2f4045df634a4b97d22ebe669ee6d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/master/LWFMain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbtGDBU3QJaq",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf0TmOM3NdFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I0pKIVIM2KC",
        "colab_type": "code",
        "outputId": "a026c6b4-9f77-4342-b1ec-2609815f9126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 197, done.\u001b[K\n",
            "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 729 (delta 122), reused 0 (delta 0), pack-reused 532\u001b[K\n",
            "Receiving objects: 100% (729/729), 496.70 KiB | 5.98 MiB/s, done.\n",
            "Resolving deltas: 100% (451/451), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLaS2laafBaG",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liUP5Kc1DMbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from DatasetCIFAR.data_set import Dataset \n",
        "from DatasetCIFAR import ResNet\n",
        "from DatasetCIFAR import utils\n",
        "from DatasetCIFAR import params\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "random.seed(params.SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_vlqOL7ehLC",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWttFW3ljoMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resNet = ResNet.resnet32(num_classes=100)\n",
        "resNet = resNet.to(params.DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmohsyVWFpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transformer = transforms.Compose([transforms.RandomCrop(size = 32, padding=4),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_cyhIzFej5-",
        "colab_type": "text"
      },
      "source": [
        "# Define DataSets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcBNohmiYBtP",
        "colab_type": "code",
        "outputId": "32b6f5c0-8ea4-445b-e22c-a7a19aa63597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "916122c844e947c88db797a4a49d8221",
            "75b70705bffa43d49d31db618ddeae93",
            "80504a69f92e4fd3b601c30e960165f4",
            "c36d624f1b824bc0b172cb85bbc7438e",
            "9da1b14930d447c5a3bc16f8912d3c51",
            "a91e70225b9b4e9683099f59fa83e056",
            "bd51d810c8994d11909ece2e759b53cc",
            "da2f4045df634a4b97d22ebe669ee6d4"
          ]
        }
      },
      "source": [
        "trainDS = Dataset(train=True, transform = train_transformer)\n",
        "testDS = Dataset(train=False, transform = test_transformer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "916122c844e947c88db797a4a49d8221",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFxMUO_FQZRo",
        "colab_type": "code",
        "outputId": "168f4b28-9bf5-4752-abd7-a4d7f60f1c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "train_splits = trainDS.splits\n",
        "test_splits = testDS.splits\n",
        "print(train_splits)\n",
        "print(test_splits)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[94.0, 63.0, 74.0, 21.0, 35.0, 56.0, 91.0, 96.0, 87.0, 48.0], [68.0, 80.0, 22.0, 37.0, 60.0, 97.0, 51.0, 62.0, 92.0, 76.0], [75.0, 89.0, 23.0, 99.0, 39.0, 66.0, 54.0, 69.0, 84.0, 61.0], [85.0, 24.0, 98.0, 41.0, 73.0, 58.0, 78.0, 77.0, 70.0, 49.0], [65.0, 88.0, 36.0, 93.0, 45.0, 10.0, 90.0, 17.0, 32.0, 59.0], [83.0, 43.0, 53.0, 11.0, 86.0, 19.0, 38.0, 30.0, 40.0, 50.0], [57.0, 81.0, 12.0, 95.0, 25.0, 47.0, 34.0, 52.0, 44.0, 72.0], [46.0, 79.0, 20.0, 28.0, 5.0, 71.0, 8.0, 18.0, 33.0, 15.0], [55.0, 29.0, 64.0, 31.0, 67.0, 7.0, 13.0, 14.0, 42.0, 6.0], [82.0, 2.0, 27.0, 16.0, 26.0, 3.0, 4.0, 1.0, 9.0, 0.0]]\n",
            "[[94.0, 63.0, 74.0, 21.0, 35.0, 56.0, 91.0, 96.0, 87.0, 48.0], [68.0, 80.0, 22.0, 37.0, 60.0, 97.0, 51.0, 62.0, 92.0, 76.0], [75.0, 89.0, 23.0, 99.0, 39.0, 66.0, 54.0, 69.0, 84.0, 61.0], [85.0, 24.0, 98.0, 41.0, 73.0, 58.0, 78.0, 77.0, 70.0, 49.0], [65.0, 88.0, 36.0, 93.0, 45.0, 10.0, 90.0, 17.0, 32.0, 59.0], [83.0, 43.0, 53.0, 11.0, 86.0, 19.0, 38.0, 30.0, 40.0, 50.0], [57.0, 81.0, 12.0, 95.0, 25.0, 47.0, 34.0, 52.0, 44.0, 72.0], [46.0, 79.0, 20.0, 28.0, 5.0, 71.0, 8.0, 18.0, 33.0, 15.0], [55.0, 29.0, 64.0, 31.0, 67.0, 7.0, 13.0, 14.0, 42.0, 6.0], [82.0, 2.0, 27.0, 16.0, 26.0, 3.0, 4.0, 1.0, 9.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAT2KQEersx",
        "colab_type": "text"
      },
      "source": [
        "# Useful plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1l7flYj4NJh",
        "colab_type": "text"
      },
      "source": [
        "The function plotEpoch plots, at the end of each task, how accuracy and loss change during the training phase. It show\n",
        "\n",
        "*   Validation and Training Accuracy\n",
        "*   Validation and Training Loss\n",
        "\n",
        "The function plotTask, for each task, how the accuracy on the validation set change when adding new tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr58kkHiIzZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTask(pars_tasks):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  x_tasks =  np.linspace(10, 100, 10)\n",
        "\n",
        "  plt.plot(x_tasks, pars_tasks, label=['Accuracy', 'Loss'])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.title('Accuracy over tasks')\n",
        "  plt.legend(['Accuracy', 'Loss'])\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iApKvCs942aS",
        "colab_type": "text"
      },
      "source": [
        "# Train and evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJse4JU7d9ck",
        "colab_type": "code",
        "outputId": "3f5e53d1-5a60-416c-8fa0-73abc330253b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pars_tasks = []\n",
        "test_indexes = []\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "  pars_tasks.insert(task, 0)\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "\n",
        "  train_indexes = trainDS.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes + testDS.__getIndexesGroups__(task)\n",
        "\n",
        "  train_dataset = Subset(trainDS, train_indexes)\n",
        "  test_dataset = Subset(testDS, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE, shuffle = True)\n",
        "  test_loader = DataLoader( test_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE, shuffle = True )\n",
        "\n",
        "  if(task == 0):\n",
        "    torch.save(resNet, 'resNet_task{0}.pt'.format(task))\n",
        "  \n",
        "  \n",
        "\n",
        "  utils.trainfunction(task, train_loader, train_splits)\n",
        "  param = utils.evaluationTest(task, test_loader, test_splits)\n",
        "  pars_tasks[int(task/10)] = param #pars_task[i] = (accuracy, loss) at i-th task\t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "task = 0 \n",
            "train col =  [94 63 74 21 35 56 91 96 87 48]\n",
            "train col =  [[94 63 74 21 35 56 91 96 87 48]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "At step  0  and at epoch =  0  the loss is =  0.03493708744645119  and accuracy is =  0.2134\n",
            "At step  0  and at epoch =  1  the loss is =  0.02573636919260025  and accuracy is =  0.4224\n",
            "At step  0  and at epoch =  2  the loss is =  0.017128275707364082  and accuracy is =  0.5356\n",
            "At step  0  and at epoch =  3  the loss is =  0.011843006126582623  and accuracy is =  0.5854\n",
            "At step  0  and at epoch =  4  the loss is =  0.006436775904148817  and accuracy is =  0.634\n",
            "At step  0  and at epoch =  5  the loss is =  0.01656145229935646  and accuracy is =  0.6964\n",
            "At step  0  and at epoch =  6  the loss is =  0.020760251209139824  and accuracy is =  0.725\n",
            "At step  0  and at epoch =  7  the loss is =  0.015979358926415443  and accuracy is =  0.7466\n",
            "At step  0  and at epoch =  8  the loss is =  0.019129959866404533  and accuracy is =  0.7656\n",
            "At step  0  and at epoch =  9  the loss is =  0.017124861478805542  and accuracy is =  0.7966\n",
            "At step  0  and at epoch =  10  the loss is =  0.012401318177580833  and accuracy is =  0.8036\n",
            "At step  0  and at epoch =  11  the loss is =  0.007040626835078001  and accuracy is =  0.8264\n",
            "At step  0  and at epoch =  12  the loss is =  0.009144126437604427  and accuracy is =  0.8742\n",
            "At step  0  and at epoch =  13  the loss is =  0.01960526406764984  and accuracy is =  0.8502\n",
            "At step  0  and at epoch =  14  the loss is =  0.012113447301089764  and accuracy is =  0.8248\n",
            "At step  0  and at epoch =  15  the loss is =  0.015583864413201809  and accuracy is =  0.89\n",
            "At step  0  and at epoch =  16  the loss is =  0.007440316490828991  and accuracy is =  0.9018\n",
            "At step  0  and at epoch =  17  the loss is =  0.00874305423349142  and accuracy is =  0.934\n",
            "At step  0  and at epoch =  18  the loss is =  0.05165663734078407  and accuracy is =  0.8738\n",
            "At step  0  and at epoch =  19  the loss is =  0.01639508828520775  and accuracy is =  0.8472\n",
            "At step  0  and at epoch =  20  the loss is =  0.04567466303706169  and accuracy is =  0.8742\n",
            "At step  0  and at epoch =  21  the loss is =  0.012918940745294094  and accuracy is =  0.9146\n",
            "At step  0  and at epoch =  22  the loss is =  0.01970616541802883  and accuracy is =  0.9398\n",
            "At step  0  and at epoch =  23  the loss is =  0.020735785365104675  and accuracy is =  0.8528\n",
            "At step  0  and at epoch =  24  the loss is =  0.021478448063135147  and accuracy is =  0.9218\n",
            "At step  0  and at epoch =  25  the loss is =  0.004281371366232634  and accuracy is =  0.938\n",
            "At step  0  and at epoch =  26  the loss is =  0.007001141086220741  and accuracy is =  0.9594\n",
            "At step  0  and at epoch =  27  the loss is =  0.01274435967206955  and accuracy is =  0.9764\n",
            "At step  0  and at epoch =  28  the loss is =  0.010339655913412571  and accuracy is =  0.9628\n",
            "At step  0  and at epoch =  29  the loss is =  0.008127691224217415  and accuracy is =  0.8686\n",
            "At step  0  and at epoch =  30  the loss is =  0.00229091034270823  and accuracy is =  0.9198\n",
            "At step  0  and at epoch =  31  the loss is =  0.014275259338319302  and accuracy is =  0.979\n",
            "At step  0  and at epoch =  32  the loss is =  0.008445321582257748  and accuracy is =  0.9536\n",
            "At step  0  and at epoch =  33  the loss is =  0.03731798380613327  and accuracy is =  0.9418\n",
            "At step  0  and at epoch =  34  the loss is =  0.015862982720136642  and accuracy is =  0.8944\n",
            "At step  0  and at epoch =  35  the loss is =  0.0009205075330100954  and accuracy is =  0.9604\n",
            "At step  0  and at epoch =  36  the loss is =  0.010414889082312584  and accuracy is =  0.9926\n",
            "At step  0  and at epoch =  37  the loss is =  0.01095521543174982  and accuracy is =  0.9636\n",
            "At step  0  and at epoch =  38  the loss is =  0.004878423176705837  and accuracy is =  0.9638\n",
            "At step  0  and at epoch =  39  the loss is =  0.0012799275573343039  and accuracy is =  0.9786\n",
            "At step  0  and at epoch =  40  the loss is =  0.0054495204240083694  and accuracy is =  0.9916\n",
            "At step  0  and at epoch =  41  the loss is =  0.029801130294799805  and accuracy is =  0.977\n",
            "At step  0  and at epoch =  42  the loss is =  0.007129212375730276  and accuracy is =  0.9584\n",
            "At step  0  and at epoch =  43  the loss is =  0.00212770514190197  and accuracy is =  0.9718\n",
            "At step  0  and at epoch =  44  the loss is =  0.011276685632765293  and accuracy is =  0.9858\n",
            "At step  0  and at epoch =  45  the loss is =  0.008893158286809921  and accuracy is =  0.9652\n",
            "At step  0  and at epoch =  46  the loss is =  0.00596905592828989  and accuracy is =  0.9308\n",
            "At step  0  and at epoch =  47  the loss is =  0.008358110673725605  and accuracy is =  0.9646\n",
            "At step  0  and at epoch =  48  the loss is =  0.002109626540914178  and accuracy is =  0.9906\n",
            "At step  0  and at epoch =  49  the loss is =  0.013203122653067112  and accuracy is =  0.9986\n",
            "At step  0  and at epoch =  50  the loss is =  0.005731154698878527  and accuracy is =  0.9976\n",
            "At step  0  and at epoch =  51  the loss is =  0.005153017584234476  and accuracy is =  0.9978\n",
            "At step  0  and at epoch =  52  the loss is =  0.0032543849665671587  and accuracy is =  0.9976\n",
            "At step  0  and at epoch =  53  the loss is =  0.0030408387538045645  and accuracy is =  0.9988\n",
            "At step  0  and at epoch =  54  the loss is =  0.0022620924282819033  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  55  the loss is =  0.006218960974365473  and accuracy is =  0.9994\n",
            "At step  0  and at epoch =  56  the loss is =  0.0001958702487172559  and accuracy is =  0.9988\n",
            "At step  0  and at epoch =  57  the loss is =  0.001572744338773191  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  58  the loss is =  0.014483198523521423  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  59  the loss is =  0.0016857254086062312  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  60  the loss is =  0.0018796364311128855  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  61  the loss is =  0.00029435797478072345  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  62  the loss is =  0.005205878056585789  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  63  the loss is =  0.013685815967619419  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  64  the loss is =  0.0003358077083248645  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  65  the loss is =  0.005524916108697653  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  66  the loss is =  0.0002463581913616508  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  67  the loss is =  0.004218223039060831  and accuracy is =  1.0\n",
            "At step  0  and at epoch =  68  the loss is =  0.020809223875403404  and accuracy is =  0.9994\n",
            "At step  0  and at epoch =  69  the loss is =  0.008880514651536942  and accuracy is =  0.9998\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "104\n",
            "Validation Loss: 0.013062515296041965 Validation Accuracy : 0.806\n",
            "task = 10 \n",
            "train col =  [68 80 22 37 60 97 51 62 92 76]\n",
            "train col =  [[68 80 22 37 60 97 51 62 92 76]]\n",
            "At step  10  and at epoch =  0  the loss is =  0.04920191690325737  and accuracy is =  0.3022\n",
            "At step  10  and at epoch =  1  the loss is =  0.03986630216240883  and accuracy is =  0.545\n",
            "At step  10  and at epoch =  2  the loss is =  0.03926577791571617  and accuracy is =  0.6294\n",
            "At step  10  and at epoch =  3  the loss is =  0.0330042727291584  and accuracy is =  0.6842\n",
            "At step  10  and at epoch =  4  the loss is =  0.030912484973669052  and accuracy is =  0.746\n",
            "At step  10  and at epoch =  5  the loss is =  0.042512163519859314  and accuracy is =  0.7942\n",
            "At step  10  and at epoch =  6  the loss is =  0.02785407565534115  and accuracy is =  0.809\n",
            "At step  10  and at epoch =  7  the loss is =  0.02686528116464615  and accuracy is =  0.85\n",
            "At step  10  and at epoch =  8  the loss is =  0.028372205793857574  and accuracy is =  0.8964\n",
            "At step  10  and at epoch =  9  the loss is =  0.01800893060863018  and accuracy is =  0.9156\n",
            "At step  10  and at epoch =  10  the loss is =  0.03905372694134712  and accuracy is =  0.9436\n",
            "At step  10  and at epoch =  11  the loss is =  0.01818644069135189  and accuracy is =  0.8966\n",
            "At step  10  and at epoch =  12  the loss is =  0.03776232898235321  and accuracy is =  0.957\n",
            "At step  10  and at epoch =  13  the loss is =  0.05058833956718445  and accuracy is =  0.896\n",
            "At step  10  and at epoch =  14  the loss is =  0.018695902079343796  and accuracy is =  0.9186\n",
            "At step  10  and at epoch =  15  the loss is =  0.026734355837106705  and accuracy is =  0.9686\n",
            "At step  10  and at epoch =  16  the loss is =  0.01453818567097187  and accuracy is =  0.9574\n",
            "At step  10  and at epoch =  17  the loss is =  0.031728800386190414  and accuracy is =  0.9892\n",
            "At step  10  and at epoch =  18  the loss is =  0.051671624183654785  and accuracy is =  0.9054\n",
            "At step  10  and at epoch =  19  the loss is =  0.050666388124227524  and accuracy is =  0.9334\n",
            "At step  10  and at epoch =  20  the loss is =  0.049347199499607086  and accuracy is =  0.9554\n",
            "At step  10  and at epoch =  21  the loss is =  0.037189412862062454  and accuracy is =  0.9604\n",
            "At step  10  and at epoch =  22  the loss is =  0.03527706861495972  and accuracy is =  0.9322\n",
            "At step  10  and at epoch =  23  the loss is =  0.03900569677352905  and accuracy is =  0.958\n",
            "At step  10  and at epoch =  24  the loss is =  0.019252320751547813  and accuracy is =  0.9714\n",
            "At step  10  and at epoch =  25  the loss is =  0.025501880794763565  and accuracy is =  0.9854\n",
            "At step  10  and at epoch =  26  the loss is =  0.033377811312675476  and accuracy is =  0.9606\n",
            "At step  10  and at epoch =  27  the loss is =  0.029286183416843414  and accuracy is =  0.9418\n",
            "At step  10  and at epoch =  28  the loss is =  0.016206316649913788  and accuracy is =  0.9556\n",
            "At step  10  and at epoch =  29  the loss is =  0.03426322713494301  and accuracy is =  0.9772\n",
            "At step  10  and at epoch =  30  the loss is =  0.023239612579345703  and accuracy is =  0.985\n",
            "At step  10  and at epoch =  31  the loss is =  0.0284255538135767  and accuracy is =  0.9702\n",
            "At step  10  and at epoch =  32  the loss is =  0.015183784998953342  and accuracy is =  0.9392\n",
            "At step  10  and at epoch =  33  the loss is =  0.030631495639681816  and accuracy is =  0.982\n",
            "At step  10  and at epoch =  34  the loss is =  0.02012179046869278  and accuracy is =  0.9866\n",
            "At step  10  and at epoch =  35  the loss is =  0.015459637157619  and accuracy is =  0.9916\n",
            "At step  10  and at epoch =  36  the loss is =  0.03076893277466297  and accuracy is =  0.9978\n",
            "At step  10  and at epoch =  37  the loss is =  0.016873128712177277  and accuracy is =  0.9842\n",
            "At step  10  and at epoch =  38  the loss is =  0.015813183039426804  and accuracy is =  0.9966\n",
            "At step  10  and at epoch =  39  the loss is =  0.01809084229171276  and accuracy is =  0.9982\n",
            "At step  10  and at epoch =  40  the loss is =  0.020699700340628624  and accuracy is =  0.9958\n",
            "At step  10  and at epoch =  41  the loss is =  0.028053101152181625  and accuracy is =  0.9988\n",
            "At step  10  and at epoch =  42  the loss is =  0.023591896519064903  and accuracy is =  0.98\n",
            "At step  10  and at epoch =  43  the loss is =  0.02377542480826378  and accuracy is =  0.9488\n",
            "At step  10  and at epoch =  44  the loss is =  0.01841541938483715  and accuracy is =  0.9814\n",
            "At step  10  and at epoch =  45  the loss is =  0.021990938112139702  and accuracy is =  0.995\n",
            "At step  10  and at epoch =  46  the loss is =  0.03862981125712395  and accuracy is =  0.9816\n",
            "At step  10  and at epoch =  47  the loss is =  0.016063928604125977  and accuracy is =  0.9552\n",
            "At step  10  and at epoch =  48  the loss is =  0.016068018972873688  and accuracy is =  0.9938\n",
            "At step  10  and at epoch =  49  the loss is =  0.033669937402009964  and accuracy is =  0.9986\n",
            "At step  10  and at epoch =  50  the loss is =  0.017676319926977158  and accuracy is =  0.9978\n",
            "At step  10  and at epoch =  51  the loss is =  0.015370743349194527  and accuracy is =  0.9988\n",
            "At step  10  and at epoch =  52  the loss is =  0.02315688319504261  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  53  the loss is =  0.017445089295506477  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  54  the loss is =  0.014552250504493713  and accuracy is =  0.9994\n",
            "At step  10  and at epoch =  55  the loss is =  0.022701410576701164  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  56  the loss is =  0.015655959025025368  and accuracy is =  0.9994\n",
            "At step  10  and at epoch =  57  the loss is =  0.015307027846574783  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  58  the loss is =  0.01963881403207779  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  59  the loss is =  0.017649341374635696  and accuracy is =  0.9994\n",
            "At step  10  and at epoch =  60  the loss is =  0.030241413041949272  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  61  the loss is =  0.044343337416648865  and accuracy is =  0.9988\n",
            "At step  10  and at epoch =  62  the loss is =  0.013263775035738945  and accuracy is =  0.9994\n",
            "At step  10  and at epoch =  63  the loss is =  0.028018884360790253  and accuracy is =  0.9992\n",
            "At step  10  and at epoch =  64  the loss is =  0.027220159769058228  and accuracy is =  0.9992\n",
            "At step  10  and at epoch =  65  the loss is =  0.026609718799591064  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  66  the loss is =  0.02113133855164051  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  67  the loss is =  0.013679713942110538  and accuracy is =  1.0\n",
            "At step  10  and at epoch =  68  the loss is =  0.029476551339030266  and accuracy is =  0.9996\n",
            "At step  10  and at epoch =  69  the loss is =  0.015401049517095089  and accuracy is =  0.9998\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "80\n",
            "Validation Loss: 0.03237190097570419 Validation Accuracy : 0.594\n",
            "task = 20 \n",
            "train col =  [75 89 23 99 39 66 54 69 84 61]\n",
            "train col =  [[75 89 23 99 39 66 54 69 84 61]]\n",
            "At step  20  and at epoch =  0  the loss is =  0.07401086390018463  and accuracy is =  0.433\n",
            "At step  20  and at epoch =  1  the loss is =  0.052825018763542175  and accuracy is =  0.6112\n",
            "At step  20  and at epoch =  2  the loss is =  0.05628734454512596  and accuracy is =  0.6994\n",
            "At step  20  and at epoch =  3  the loss is =  0.0398031584918499  and accuracy is =  0.7558\n",
            "At step  20  and at epoch =  4  the loss is =  0.04772408306598663  and accuracy is =  0.808\n",
            "At step  20  and at epoch =  5  the loss is =  0.049387186765670776  and accuracy is =  0.8472\n",
            "At step  20  and at epoch =  6  the loss is =  0.0332820899784565  and accuracy is =  0.8744\n",
            "At step  20  and at epoch =  7  the loss is =  0.04074295982718468  and accuracy is =  0.9252\n",
            "At step  20  and at epoch =  8  the loss is =  0.05435037612915039  and accuracy is =  0.944\n",
            "At step  20  and at epoch =  9  the loss is =  0.034265030175447464  and accuracy is =  0.9292\n",
            "At step  20  and at epoch =  10  the loss is =  0.035753410309553146  and accuracy is =  0.9534\n",
            "At step  20  and at epoch =  11  the loss is =  0.034930191934108734  and accuracy is =  0.9748\n",
            "At step  20  and at epoch =  12  the loss is =  0.03129033371806145  and accuracy is =  0.9786\n",
            "At step  20  and at epoch =  13  the loss is =  0.032382868230342865  and accuracy is =  0.9846\n",
            "At step  20  and at epoch =  14  the loss is =  0.04196104779839516  and accuracy is =  0.9896\n",
            "At step  20  and at epoch =  15  the loss is =  0.039992135018110275  and accuracy is =  0.9372\n",
            "At step  20  and at epoch =  16  the loss is =  0.0626668781042099  and accuracy is =  0.9454\n",
            "At step  20  and at epoch =  17  the loss is =  0.04816005751490593  and accuracy is =  0.9572\n",
            "At step  20  and at epoch =  18  the loss is =  0.03129641339182854  and accuracy is =  0.9694\n",
            "At step  20  and at epoch =  19  the loss is =  0.03590991348028183  and accuracy is =  0.992\n",
            "At step  20  and at epoch =  20  the loss is =  0.029633015394210815  and accuracy is =  0.9874\n",
            "At step  20  and at epoch =  21  the loss is =  0.03294986113905907  and accuracy is =  0.9926\n",
            "At step  20  and at epoch =  22  the loss is =  0.02860919199883938  and accuracy is =  0.9946\n",
            "At step  20  and at epoch =  23  the loss is =  0.03548583388328552  and accuracy is =  0.9954\n",
            "At step  20  and at epoch =  24  the loss is =  0.041730981320142746  and accuracy is =  0.9808\n",
            "At step  20  and at epoch =  25  the loss is =  0.028433246538043022  and accuracy is =  0.9686\n",
            "At step  20  and at epoch =  26  the loss is =  0.038677509874105453  and accuracy is =  0.9844\n",
            "At step  20  and at epoch =  27  the loss is =  0.042589373886585236  and accuracy is =  0.9492\n",
            "At step  20  and at epoch =  28  the loss is =  0.04789406806230545  and accuracy is =  0.9526\n",
            "At step  20  and at epoch =  29  the loss is =  0.04029366374015808  and accuracy is =  0.9122\n",
            "At step  20  and at epoch =  30  the loss is =  0.029666850343346596  and accuracy is =  0.9578\n",
            "At step  20  and at epoch =  31  the loss is =  0.03305397927761078  and accuracy is =  0.9944\n",
            "At step  20  and at epoch =  32  the loss is =  0.03758040443062782  and accuracy is =  0.9976\n",
            "At step  20  and at epoch =  33  the loss is =  0.03358452767133713  and accuracy is =  0.9772\n",
            "At step  20  and at epoch =  34  the loss is =  0.03135381639003754  and accuracy is =  0.9922\n",
            "At step  20  and at epoch =  35  the loss is =  0.029988445341587067  and accuracy is =  0.9948\n",
            "At step  20  and at epoch =  36  the loss is =  0.04041928052902222  and accuracy is =  0.9866\n",
            "At step  20  and at epoch =  37  the loss is =  0.03513278439640999  and accuracy is =  0.9596\n",
            "At step  20  and at epoch =  38  the loss is =  0.03464720398187637  and accuracy is =  0.9882\n",
            "At step  20  and at epoch =  39  the loss is =  0.033631954342126846  and accuracy is =  0.9976\n",
            "At step  20  and at epoch =  40  the loss is =  0.03920472040772438  and accuracy is =  0.9938\n",
            "At step  20  and at epoch =  41  the loss is =  0.03270406648516655  and accuracy is =  0.9818\n",
            "At step  20  and at epoch =  42  the loss is =  0.04426614195108414  and accuracy is =  0.9752\n",
            "At step  20  and at epoch =  43  the loss is =  0.029895154759287834  and accuracy is =  0.9808\n",
            "At step  20  and at epoch =  44  the loss is =  0.029518580064177513  and accuracy is =  0.9976\n",
            "At step  20  and at epoch =  45  the loss is =  0.028790220618247986  and accuracy is =  0.9942\n",
            "At step  20  and at epoch =  46  the loss is =  0.023640073835849762  and accuracy is =  0.9992\n",
            "At step  20  and at epoch =  47  the loss is =  0.044682662934064865  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  48  the loss is =  0.04198192059993744  and accuracy is =  0.9986\n",
            "At step  20  and at epoch =  49  the loss is =  0.03855261951684952  and accuracy is =  0.9992\n",
            "At step  20  and at epoch =  50  the loss is =  0.044065020978450775  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  51  the loss is =  0.04395417124032974  and accuracy is =  0.9988\n",
            "At step  20  and at epoch =  52  the loss is =  0.02571137435734272  and accuracy is =  0.9966\n",
            "At step  20  and at epoch =  53  the loss is =  0.035537634044885635  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  54  the loss is =  0.03556148707866669  and accuracy is =  0.999\n",
            "At step  20  and at epoch =  55  the loss is =  0.027386687695980072  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  56  the loss is =  0.02998301200568676  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  57  the loss is =  0.034552719444036484  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  58  the loss is =  0.03216901421546936  and accuracy is =  0.9992\n",
            "At step  20  and at epoch =  59  the loss is =  0.026241786777973175  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  60  the loss is =  0.03367754444479942  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  61  the loss is =  0.042744096368551254  and accuracy is =  0.999\n",
            "At step  20  and at epoch =  62  the loss is =  0.030292361974716187  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  63  the loss is =  0.030746597796678543  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  64  the loss is =  0.020764019340276718  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  65  the loss is =  0.0309413094073534  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  66  the loss is =  0.03206611052155495  and accuracy is =  0.9998\n",
            "At step  20  and at epoch =  67  the loss is =  0.03764089569449425  and accuracy is =  0.9996\n",
            "At step  20  and at epoch =  68  the loss is =  0.027324624359607697  and accuracy is =  1.0\n",
            "At step  20  and at epoch =  69  the loss is =  0.02372758276760578  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "56\n",
            "Validation Loss: 0.05098848417401314 Validation Accuracy : 0.4593333333333333\n",
            "task = 30 \n",
            "train col =  [85 24 98 41 73 58 78 77 70 49]\n",
            "train col =  [[85 24 98 41 73 58 78 77 70 49]]\n",
            "At step  30  and at epoch =  0  the loss is =  0.060515496879816055  and accuracy is =  0.4724\n",
            "At step  30  and at epoch =  1  the loss is =  0.06576985865831375  and accuracy is =  0.6622\n",
            "At step  30  and at epoch =  2  the loss is =  0.059610411524772644  and accuracy is =  0.7312\n",
            "At step  30  and at epoch =  3  the loss is =  0.07294730097055435  and accuracy is =  0.797\n",
            "At step  30  and at epoch =  4  the loss is =  0.05580633133649826  and accuracy is =  0.8386\n",
            "At step  30  and at epoch =  5  the loss is =  0.05139821767807007  and accuracy is =  0.8908\n",
            "At step  30  and at epoch =  6  the loss is =  0.06236252561211586  and accuracy is =  0.9222\n",
            "At step  30  and at epoch =  7  the loss is =  0.06142371892929077  and accuracy is =  0.9154\n",
            "At step  30  and at epoch =  8  the loss is =  0.04602816328406334  and accuracy is =  0.937\n",
            "At step  30  and at epoch =  9  the loss is =  0.049465496093034744  and accuracy is =  0.9666\n",
            "At step  30  and at epoch =  10  the loss is =  0.043962545692920685  and accuracy is =  0.9588\n",
            "At step  30  and at epoch =  11  the loss is =  0.04913293197751045  and accuracy is =  0.97\n",
            "At step  30  and at epoch =  12  the loss is =  0.048905886709690094  and accuracy is =  0.974\n",
            "At step  30  and at epoch =  13  the loss is =  0.04949912801384926  and accuracy is =  0.9864\n",
            "At step  30  and at epoch =  14  the loss is =  0.05730074644088745  and accuracy is =  0.9878\n",
            "At step  30  and at epoch =  15  the loss is =  0.05286937206983566  and accuracy is =  0.9618\n",
            "At step  30  and at epoch =  16  the loss is =  0.05312870815396309  and accuracy is =  0.9752\n",
            "At step  30  and at epoch =  17  the loss is =  0.05040490999817848  and accuracy is =  0.9828\n",
            "At step  30  and at epoch =  18  the loss is =  0.05634002387523651  and accuracy is =  0.9866\n",
            "At step  30  and at epoch =  19  the loss is =  0.0466836616396904  and accuracy is =  0.9556\n",
            "At step  30  and at epoch =  20  the loss is =  0.051261480897665024  and accuracy is =  0.9744\n",
            "At step  30  and at epoch =  21  the loss is =  0.040736354887485504  and accuracy is =  0.9912\n",
            "At step  30  and at epoch =  22  the loss is =  0.04212737828493118  and accuracy is =  0.9968\n",
            "At step  30  and at epoch =  23  the loss is =  0.06920324265956879  and accuracy is =  0.9974\n",
            "At step  30  and at epoch =  24  the loss is =  0.04231330007314682  and accuracy is =  0.9768\n",
            "At step  30  and at epoch =  25  the loss is =  0.04214636608958244  and accuracy is =  0.9976\n",
            "At step  30  and at epoch =  26  the loss is =  0.04055275768041611  and accuracy is =  0.9972\n",
            "At step  30  and at epoch =  27  the loss is =  0.07798173278570175  and accuracy is =  0.9948\n",
            "At step  30  and at epoch =  28  the loss is =  0.053922586143016815  and accuracy is =  0.9746\n",
            "At step  30  and at epoch =  29  the loss is =  0.05447619780898094  and accuracy is =  0.9556\n",
            "At step  30  and at epoch =  30  the loss is =  0.051819875836372375  and accuracy is =  0.9822\n",
            "At step  30  and at epoch =  31  the loss is =  0.06064750999212265  and accuracy is =  0.9884\n",
            "At step  30  and at epoch =  32  the loss is =  0.05155164748430252  and accuracy is =  0.9878\n",
            "At step  30  and at epoch =  33  the loss is =  0.06774330139160156  and accuracy is =  0.9938\n",
            "At step  30  and at epoch =  34  the loss is =  0.0499558262526989  and accuracy is =  0.9526\n",
            "At step  30  and at epoch =  35  the loss is =  0.06040916219353676  and accuracy is =  0.986\n",
            "At step  30  and at epoch =  36  the loss is =  0.04573163762688637  and accuracy is =  0.9784\n",
            "At step  30  and at epoch =  37  the loss is =  0.05590783804655075  and accuracy is =  0.9818\n",
            "At step  30  and at epoch =  38  the loss is =  0.05169970542192459  and accuracy is =  0.9904\n",
            "At step  30  and at epoch =  39  the loss is =  0.049180030822753906  and accuracy is =  0.9918\n",
            "At step  30  and at epoch =  40  the loss is =  0.045302052050828934  and accuracy is =  0.9874\n",
            "At step  30  and at epoch =  41  the loss is =  0.060271501541137695  and accuracy is =  0.9884\n",
            "At step  30  and at epoch =  42  the loss is =  0.04771167039871216  and accuracy is =  0.967\n",
            "At step  30  and at epoch =  43  the loss is =  0.04561770334839821  and accuracy is =  0.9924\n",
            "At step  30  and at epoch =  44  the loss is =  0.04920215532183647  and accuracy is =  0.9974\n",
            "At step  30  and at epoch =  45  the loss is =  0.04487277567386627  and accuracy is =  0.9984\n",
            "At step  30  and at epoch =  46  the loss is =  0.048044126480817795  and accuracy is =  0.999\n",
            "At step  30  and at epoch =  47  the loss is =  0.04273728281259537  and accuracy is =  0.996\n",
            "At step  30  and at epoch =  48  the loss is =  0.046741221100091934  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  49  the loss is =  0.03499041870236397  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  50  the loss is =  0.048192862421274185  and accuracy is =  0.9998\n",
            "At step  30  and at epoch =  51  the loss is =  0.06153332442045212  and accuracy is =  0.9998\n",
            "At step  30  and at epoch =  52  the loss is =  0.04821854457259178  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  53  the loss is =  0.06893865764141083  and accuracy is =  0.9994\n",
            "At step  30  and at epoch =  54  the loss is =  0.048506345599889755  and accuracy is =  0.9992\n",
            "At step  30  and at epoch =  55  the loss is =  0.037560999393463135  and accuracy is =  0.9986\n",
            "At step  30  and at epoch =  56  the loss is =  0.045051898807287216  and accuracy is =  0.9998\n",
            "At step  30  and at epoch =  57  the loss is =  0.04159218817949295  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  58  the loss is =  0.03965913504362106  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  59  the loss is =  0.03915116563439369  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  60  the loss is =  0.04562359303236008  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  61  the loss is =  0.062205810099840164  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  62  the loss is =  0.04771357774734497  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  63  the loss is =  0.045777034014463425  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  64  the loss is =  0.03964168578386307  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  65  the loss is =  0.07036701589822769  and accuracy is =  0.9996\n",
            "At step  30  and at epoch =  66  the loss is =  0.045620907098054886  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  67  the loss is =  0.04550738260149956  and accuracy is =  0.9998\n",
            "At step  30  and at epoch =  68  the loss is =  0.04936521500349045  and accuracy is =  1.0\n",
            "At step  30  and at epoch =  69  the loss is =  0.04423046112060547  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "32\n",
            "Validation Loss: 0.05558355152606964 Validation Accuracy : 0.3635\n",
            "task = 40 \n",
            "train col =  [65 88 36 93 45 10 90 17 32 59]\n",
            "train col =  [[65 88 36 93 45 10 90 17 32 59]]\n",
            "At step  40  and at epoch =  0  the loss is =  0.08523086458444595  and accuracy is =  0.4262\n",
            "At step  40  and at epoch =  1  the loss is =  0.07128289341926575  and accuracy is =  0.5792\n",
            "At step  40  and at epoch =  2  the loss is =  0.06772316247224808  and accuracy is =  0.6658\n",
            "At step  40  and at epoch =  3  the loss is =  0.0749157965183258  and accuracy is =  0.7318\n",
            "At step  40  and at epoch =  4  the loss is =  0.08195171505212784  and accuracy is =  0.7808\n",
            "At step  40  and at epoch =  5  the loss is =  0.07398247718811035  and accuracy is =  0.8236\n",
            "At step  40  and at epoch =  6  the loss is =  0.07516755163669586  and accuracy is =  0.8596\n",
            "At step  40  and at epoch =  7  the loss is =  0.06354974955320358  and accuracy is =  0.8964\n",
            "At step  40  and at epoch =  8  the loss is =  0.07370135933160782  and accuracy is =  0.942\n",
            "At step  40  and at epoch =  9  the loss is =  0.0790141299366951  and accuracy is =  0.9372\n",
            "At step  40  and at epoch =  10  the loss is =  0.0851927101612091  and accuracy is =  0.9038\n",
            "At step  40  and at epoch =  11  the loss is =  0.06767576932907104  and accuracy is =  0.9342\n",
            "At step  40  and at epoch =  12  the loss is =  0.08620639890432358  and accuracy is =  0.9732\n",
            "At step  40  and at epoch =  13  the loss is =  0.0770166888833046  and accuracy is =  0.9362\n",
            "At step  40  and at epoch =  14  the loss is =  0.06589566171169281  and accuracy is =  0.944\n",
            "At step  40  and at epoch =  15  the loss is =  0.06363172829151154  and accuracy is =  0.9746\n",
            "At step  40  and at epoch =  16  the loss is =  0.06935444474220276  and accuracy is =  0.992\n",
            "At step  40  and at epoch =  17  the loss is =  0.06347306817770004  and accuracy is =  0.9732\n",
            "At step  40  and at epoch =  18  the loss is =  0.06682288646697998  and accuracy is =  0.9872\n",
            "At step  40  and at epoch =  19  the loss is =  0.0701613500714302  and accuracy is =  0.968\n",
            "At step  40  and at epoch =  20  the loss is =  0.06974663585424423  and accuracy is =  0.9794\n",
            "At step  40  and at epoch =  21  the loss is =  0.06247637793421745  and accuracy is =  0.9824\n",
            "At step  40  and at epoch =  22  the loss is =  0.08535776287317276  and accuracy is =  0.9924\n",
            "At step  40  and at epoch =  23  the loss is =  0.0772697925567627  and accuracy is =  0.9582\n",
            "At step  40  and at epoch =  24  the loss is =  0.08749815821647644  and accuracy is =  0.9094\n",
            "At step  40  and at epoch =  25  the loss is =  0.058444730937480927  and accuracy is =  0.9258\n",
            "At step  40  and at epoch =  26  the loss is =  0.07139227539300919  and accuracy is =  0.9802\n",
            "At step  40  and at epoch =  27  the loss is =  0.06079785153269768  and accuracy is =  0.9828\n",
            "At step  40  and at epoch =  28  the loss is =  0.06446216255426407  and accuracy is =  0.9958\n",
            "At step  40  and at epoch =  29  the loss is =  0.059611834585666656  and accuracy is =  0.999\n",
            "At step  40  and at epoch =  30  the loss is =  0.06826876848936081  and accuracy is =  0.9866\n",
            "At step  40  and at epoch =  31  the loss is =  0.07712133973836899  and accuracy is =  0.9732\n",
            "At step  40  and at epoch =  32  the loss is =  0.05695267394185066  and accuracy is =  0.989\n",
            "At step  40  and at epoch =  33  the loss is =  0.056670624762773514  and accuracy is =  0.996\n",
            "At step  40  and at epoch =  34  the loss is =  0.051370978355407715  and accuracy is =  0.9994\n",
            "At step  40  and at epoch =  35  the loss is =  0.05680710822343826  and accuracy is =  0.9996\n",
            "At step  40  and at epoch =  36  the loss is =  0.07978895306587219  and accuracy is =  0.999\n",
            "At step  40  and at epoch =  37  the loss is =  0.06891672313213348  and accuracy is =  0.9528\n",
            "At step  40  and at epoch =  38  the loss is =  0.05552351847290993  and accuracy is =  0.9784\n",
            "At step  40  and at epoch =  39  the loss is =  0.07850947976112366  and accuracy is =  0.9984\n",
            "At step  40  and at epoch =  40  the loss is =  0.07136348634958267  and accuracy is =  0.9844\n",
            "At step  40  and at epoch =  41  the loss is =  0.06398718059062958  and accuracy is =  0.9884\n",
            "At step  40  and at epoch =  42  the loss is =  0.06634530425071716  and accuracy is =  0.9874\n",
            "At step  40  and at epoch =  43  the loss is =  0.05624420940876007  and accuracy is =  0.9908\n",
            "At step  40  and at epoch =  44  the loss is =  0.06075211241841316  and accuracy is =  0.9972\n",
            "At step  40  and at epoch =  45  the loss is =  0.06769323348999023  and accuracy is =  0.997\n",
            "At step  40  and at epoch =  46  the loss is =  0.057005081325769424  and accuracy is =  0.9928\n",
            "At step  40  and at epoch =  47  the loss is =  0.06120811775326729  and accuracy is =  0.9992\n",
            "At step  40  and at epoch =  48  the loss is =  0.0668322890996933  and accuracy is =  0.999\n",
            "At step  40  and at epoch =  49  the loss is =  0.05524168536067009  and accuracy is =  0.9992\n",
            "At step  40  and at epoch =  50  the loss is =  0.07619108259677887  and accuracy is =  0.9996\n",
            "At step  40  and at epoch =  51  the loss is =  0.06014367938041687  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  52  the loss is =  0.05674131214618683  and accuracy is =  0.9994\n",
            "At step  40  and at epoch =  53  the loss is =  0.05900087207555771  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  54  the loss is =  0.06236584484577179  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  55  the loss is =  0.056391581892967224  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  56  the loss is =  0.07802474498748779  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  57  the loss is =  0.07508742064237595  and accuracy is =  0.9994\n",
            "At step  40  and at epoch =  58  the loss is =  0.051601629704236984  and accuracy is =  0.9992\n",
            "At step  40  and at epoch =  59  the loss is =  0.05968879908323288  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  60  the loss is =  0.0697646513581276  and accuracy is =  0.9994\n",
            "At step  40  and at epoch =  61  the loss is =  0.06770414859056473  and accuracy is =  0.9992\n",
            "At step  40  and at epoch =  62  the loss is =  0.06006520241498947  and accuracy is =  0.9998\n",
            "At step  40  and at epoch =  63  the loss is =  0.05927812308073044  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  64  the loss is =  0.06768570095300674  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  65  the loss is =  0.05649806559085846  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  66  the loss is =  0.06609490513801575  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  67  the loss is =  0.05438811331987381  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  68  the loss is =  0.05551527813076973  and accuracy is =  1.0\n",
            "At step  40  and at epoch =  69  the loss is =  0.07792509347200394  and accuracy is =  0.9994\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "8\n",
            "Validation Loss: 0.0762692466378212 Validation Accuracy : 0.2876\n",
            "task = 50 \n",
            "train col =  [83 43 53 11 86 19 38 30 40 50]\n",
            "train col =  [[83 43 53 11 86 19 38 30 40 50]]\n",
            "At step  50  and at epoch =  0  the loss is =  0.10487265139818192  and accuracy is =  0.4486\n",
            "At step  50  and at epoch =  1  the loss is =  0.0979301705956459  and accuracy is =  0.6148\n",
            "At step  50  and at epoch =  2  the loss is =  0.11032836884260178  and accuracy is =  0.688\n",
            "At step  50  and at epoch =  3  the loss is =  0.08669277280569077  and accuracy is =  0.7344\n",
            "At step  50  and at epoch =  4  the loss is =  0.09406125545501709  and accuracy is =  0.787\n",
            "At step  50  and at epoch =  5  the loss is =  0.09048499166965485  and accuracy is =  0.8364\n",
            "At step  50  and at epoch =  6  the loss is =  0.09364047646522522  and accuracy is =  0.8644\n",
            "At step  50  and at epoch =  7  the loss is =  0.1009204089641571  and accuracy is =  0.8942\n",
            "At step  50  and at epoch =  8  the loss is =  0.09012826532125473  and accuracy is =  0.8946\n",
            "At step  50  and at epoch =  9  the loss is =  0.10309890657663345  and accuracy is =  0.941\n",
            "At step  50  and at epoch =  10  the loss is =  0.07602332532405853  and accuracy is =  0.9252\n",
            "At step  50  and at epoch =  11  the loss is =  0.08793817460536957  and accuracy is =  0.97\n",
            "At step  50  and at epoch =  12  the loss is =  0.1049584448337555  and accuracy is =  0.9526\n",
            "At step  50  and at epoch =  13  the loss is =  0.0998273715376854  and accuracy is =  0.9236\n",
            "At step  50  and at epoch =  14  the loss is =  0.07833461463451385  and accuracy is =  0.9466\n",
            "At step  50  and at epoch =  15  the loss is =  0.0806126743555069  and accuracy is =  0.9816\n",
            "At step  50  and at epoch =  16  the loss is =  0.08088845014572144  and accuracy is =  0.9906\n",
            "At step  50  and at epoch =  17  the loss is =  0.07829520106315613  and accuracy is =  0.988\n",
            "At step  50  and at epoch =  18  the loss is =  0.08788564801216125  and accuracy is =  0.9872\n",
            "At step  50  and at epoch =  19  the loss is =  0.07895663380622864  and accuracy is =  0.945\n",
            "At step  50  and at epoch =  20  the loss is =  0.09520585089921951  and accuracy is =  0.9818\n",
            "At step  50  and at epoch =  21  the loss is =  0.08163353055715561  and accuracy is =  0.98\n",
            "At step  50  and at epoch =  22  the loss is =  0.09821370989084244  and accuracy is =  0.986\n",
            "At step  50  and at epoch =  23  the loss is =  0.08439841866493225  and accuracy is =  0.9754\n",
            "At step  50  and at epoch =  24  the loss is =  0.07664787769317627  and accuracy is =  0.9816\n",
            "At step  50  and at epoch =  25  the loss is =  0.07747302949428558  and accuracy is =  0.9946\n",
            "At step  50  and at epoch =  26  the loss is =  0.08238880336284637  and accuracy is =  0.9962\n",
            "At step  50  and at epoch =  27  the loss is =  0.08395325392484665  and accuracy is =  0.9832\n",
            "At step  50  and at epoch =  28  the loss is =  0.08380656689405441  and accuracy is =  0.9824\n",
            "At step  50  and at epoch =  29  the loss is =  0.07752513140439987  and accuracy is =  0.997\n",
            "At step  50  and at epoch =  30  the loss is =  0.08090206235647202  and accuracy is =  0.982\n",
            "At step  50  and at epoch =  31  the loss is =  0.09385567158460617  and accuracy is =  0.9912\n",
            "At step  50  and at epoch =  32  the loss is =  0.08278815448284149  and accuracy is =  0.9542\n",
            "At step  50  and at epoch =  33  the loss is =  0.08873187750577927  and accuracy is =  0.9866\n",
            "At step  50  and at epoch =  34  the loss is =  0.07889895141124725  and accuracy is =  0.9772\n",
            "At step  50  and at epoch =  35  the loss is =  0.07763373106718063  and accuracy is =  0.9914\n",
            "At step  50  and at epoch =  36  the loss is =  0.08577756583690643  and accuracy is =  0.9958\n",
            "At step  50  and at epoch =  37  the loss is =  0.0786476582288742  and accuracy is =  0.9966\n",
            "At step  50  and at epoch =  38  the loss is =  0.08379153162240982  and accuracy is =  0.9936\n",
            "At step  50  and at epoch =  39  the loss is =  0.09172879904508591  and accuracy is =  0.9916\n",
            "At step  50  and at epoch =  40  the loss is =  0.09753088653087616  and accuracy is =  0.9788\n",
            "At step  50  and at epoch =  41  the loss is =  0.08076298236846924  and accuracy is =  0.9858\n",
            "At step  50  and at epoch =  42  the loss is =  0.08650299906730652  and accuracy is =  0.9846\n",
            "At step  50  and at epoch =  43  the loss is =  0.08703838288784027  and accuracy is =  0.9922\n",
            "At step  50  and at epoch =  44  the loss is =  0.08071427792310715  and accuracy is =  0.9764\n",
            "At step  50  and at epoch =  45  the loss is =  0.07979931682348251  and accuracy is =  0.988\n",
            "At step  50  and at epoch =  46  the loss is =  0.10441865772008896  and accuracy is =  0.978\n",
            "At step  50  and at epoch =  47  the loss is =  0.07704458385705948  and accuracy is =  0.9354\n",
            "At step  50  and at epoch =  48  the loss is =  0.06845714896917343  and accuracy is =  0.995\n",
            "At step  50  and at epoch =  49  the loss is =  0.0837520956993103  and accuracy is =  0.9986\n",
            "At step  50  and at epoch =  50  the loss is =  0.06859705597162247  and accuracy is =  0.9982\n",
            "At step  50  and at epoch =  51  the loss is =  0.09332451969385147  and accuracy is =  0.9992\n",
            "At step  50  and at epoch =  52  the loss is =  0.10123005509376526  and accuracy is =  0.9988\n",
            "At step  50  and at epoch =  53  the loss is =  0.07428357005119324  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  54  the loss is =  0.07579456269741058  and accuracy is =  0.9994\n",
            "At step  50  and at epoch =  55  the loss is =  0.0760733038187027  and accuracy is =  0.9992\n",
            "At step  50  and at epoch =  56  the loss is =  0.07587671279907227  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  57  the loss is =  0.0767175629734993  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  58  the loss is =  0.0704222247004509  and accuracy is =  0.9994\n",
            "At step  50  and at epoch =  59  the loss is =  0.08038690686225891  and accuracy is =  0.9996\n",
            "At step  50  and at epoch =  60  the loss is =  0.08017151057720184  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  61  the loss is =  0.07889260351657867  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  62  the loss is =  0.06597966700792313  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  63  the loss is =  0.07671337574720383  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  64  the loss is =  0.07840929180383682  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  65  the loss is =  0.07638079673051834  and accuracy is =  0.9996\n",
            "At step  50  and at epoch =  66  the loss is =  0.08958683907985687  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  67  the loss is =  0.07279734313488007  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  68  the loss is =  0.09317468106746674  and accuracy is =  0.9998\n",
            "At step  50  and at epoch =  69  the loss is =  0.08217594027519226  and accuracy is =  0.9998\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "112\n",
            "Validation Loss: 0.071866475045681 Validation Accuracy : 0.253\n",
            "task = 60 \n",
            "train col =  [57 81 12 95 25 47 34 52 44 72]\n",
            "train col =  [[57 81 12 95 25 47 34 52 44 72]]\n",
            "At step  60  and at epoch =  0  the loss is =  0.11953103542327881  and accuracy is =  0.5182\n",
            "At step  60  and at epoch =  1  the loss is =  0.11944063007831573  and accuracy is =  0.653\n",
            "At step  60  and at epoch =  2  the loss is =  0.12911823391914368  and accuracy is =  0.7018\n",
            "At step  60  and at epoch =  3  the loss is =  0.11207011342048645  and accuracy is =  0.7492\n",
            "At step  60  and at epoch =  4  the loss is =  0.10085231810808182  and accuracy is =  0.7926\n",
            "At step  60  and at epoch =  5  the loss is =  0.1007954552769661  and accuracy is =  0.8394\n",
            "At step  60  and at epoch =  6  the loss is =  0.10314539819955826  and accuracy is =  0.8748\n",
            "At step  60  and at epoch =  7  the loss is =  0.09054698795080185  and accuracy is =  0.9064\n",
            "At step  60  and at epoch =  8  the loss is =  0.1101459413766861  and accuracy is =  0.9308\n",
            "At step  60  and at epoch =  9  the loss is =  0.1113288551568985  and accuracy is =  0.9492\n",
            "At step  60  and at epoch =  10  the loss is =  0.1023220419883728  and accuracy is =  0.9478\n",
            "At step  60  and at epoch =  11  the loss is =  0.10150903463363647  and accuracy is =  0.9538\n",
            "At step  60  and at epoch =  12  the loss is =  0.09846232831478119  and accuracy is =  0.9772\n",
            "At step  60  and at epoch =  13  the loss is =  0.10270323604345322  and accuracy is =  0.9828\n",
            "At step  60  and at epoch =  14  the loss is =  0.0987500324845314  and accuracy is =  0.9804\n",
            "At step  60  and at epoch =  15  the loss is =  0.1146167516708374  and accuracy is =  0.9894\n",
            "At step  60  and at epoch =  16  the loss is =  0.10473273694515228  and accuracy is =  0.968\n",
            "At step  60  and at epoch =  17  the loss is =  0.09708151966333389  and accuracy is =  0.9758\n",
            "At step  60  and at epoch =  18  the loss is =  0.10043922066688538  and accuracy is =  0.9864\n",
            "At step  60  and at epoch =  19  the loss is =  0.08764217048883438  and accuracy is =  0.9874\n",
            "At step  60  and at epoch =  20  the loss is =  0.09268569946289062  and accuracy is =  0.9968\n",
            "At step  60  and at epoch =  21  the loss is =  0.11337693780660629  and accuracy is =  0.9942\n",
            "At step  60  and at epoch =  22  the loss is =  0.10481917858123779  and accuracy is =  0.9862\n",
            "At step  60  and at epoch =  23  the loss is =  0.096394844353199  and accuracy is =  0.9876\n",
            "At step  60  and at epoch =  24  the loss is =  0.09046371281147003  and accuracy is =  0.9664\n",
            "At step  60  and at epoch =  25  the loss is =  0.10498856008052826  and accuracy is =  0.9898\n",
            "At step  60  and at epoch =  26  the loss is =  0.11183390021324158  and accuracy is =  0.983\n",
            "At step  60  and at epoch =  27  the loss is =  0.091587595641613  and accuracy is =  0.9802\n",
            "At step  60  and at epoch =  28  the loss is =  0.10032167285680771  and accuracy is =  0.992\n",
            "At step  60  and at epoch =  29  the loss is =  0.09549621492624283  and accuracy is =  0.9892\n",
            "At step  60  and at epoch =  30  the loss is =  0.09705697745084763  and accuracy is =  0.9792\n",
            "At step  60  and at epoch =  31  the loss is =  0.10061511397361755  and accuracy is =  0.9762\n",
            "At step  60  and at epoch =  32  the loss is =  0.09542451798915863  and accuracy is =  0.9684\n",
            "At step  60  and at epoch =  33  the loss is =  0.09828048199415207  and accuracy is =  0.9932\n",
            "At step  60  and at epoch =  34  the loss is =  0.09525319933891296  and accuracy is =  0.9778\n",
            "At step  60  and at epoch =  35  the loss is =  0.11429473757743835  and accuracy is =  0.9912\n",
            "At step  60  and at epoch =  36  the loss is =  0.09708751738071442  and accuracy is =  0.978\n",
            "At step  60  and at epoch =  37  the loss is =  0.08983670175075531  and accuracy is =  0.9874\n",
            "At step  60  and at epoch =  38  the loss is =  0.09402817487716675  and accuracy is =  0.9736\n",
            "At step  60  and at epoch =  39  the loss is =  0.10636875033378601  and accuracy is =  0.9916\n",
            "At step  60  and at epoch =  40  the loss is =  0.11054523289203644  and accuracy is =  0.9724\n",
            "At step  60  and at epoch =  41  the loss is =  0.10853365063667297  and accuracy is =  0.9882\n",
            "At step  60  and at epoch =  42  the loss is =  0.09161358326673508  and accuracy is =  0.9708\n",
            "At step  60  and at epoch =  43  the loss is =  0.09642715007066727  and accuracy is =  0.9924\n",
            "At step  60  and at epoch =  44  the loss is =  0.09239588677883148  and accuracy is =  0.9906\n",
            "At step  60  and at epoch =  45  the loss is =  0.09233789145946503  and accuracy is =  0.999\n",
            "At step  60  and at epoch =  46  the loss is =  0.09702415019273758  and accuracy is =  0.9996\n",
            "At step  60  and at epoch =  47  the loss is =  0.09061340987682343  and accuracy is =  0.9968\n",
            "At step  60  and at epoch =  48  the loss is =  0.09439492970705032  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  49  the loss is =  0.08920817822217941  and accuracy is =  0.9996\n",
            "At step  60  and at epoch =  50  the loss is =  0.10663297772407532  and accuracy is =  0.9996\n",
            "At step  60  and at epoch =  51  the loss is =  0.08185084909200668  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  52  the loss is =  0.11895047873258591  and accuracy is =  0.9994\n",
            "At step  60  and at epoch =  53  the loss is =  0.10404879599809647  and accuracy is =  0.9992\n",
            "At step  60  and at epoch =  54  the loss is =  0.09678581357002258  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  55  the loss is =  0.09450772404670715  and accuracy is =  0.9994\n",
            "At step  60  and at epoch =  56  the loss is =  0.09981714189052582  and accuracy is =  0.9992\n",
            "At step  60  and at epoch =  57  the loss is =  0.08715333789587021  and accuracy is =  0.9994\n",
            "At step  60  and at epoch =  58  the loss is =  0.08554969727993011  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  59  the loss is =  0.09902552515268326  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  60  the loss is =  0.08720852434635162  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  61  the loss is =  0.08811231702566147  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  62  the loss is =  0.09076414257287979  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  63  the loss is =  0.0817032977938652  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  64  the loss is =  0.08958899974822998  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  65  the loss is =  0.09292364120483398  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  66  the loss is =  0.11153123527765274  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  67  the loss is =  0.0897589921951294  and accuracy is =  1.0\n",
            "At step  60  and at epoch =  68  the loss is =  0.0893765315413475  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  69  the loss is =  0.0876503512263298  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "88\n",
            "Validation Loss: 0.08772929757833481 Validation Accuracy : 0.21442857142857144\n",
            "task = 70 \n",
            "train col =  [46 79 20 28  5 71  8 18 33 15]\n",
            "train col =  [[46 79 20 28  5 71  8 18 33 15]]\n",
            "At step  70  and at epoch =  0  the loss is =  0.1373443454504013  and accuracy is =  0.5284\n",
            "At step  70  and at epoch =  1  the loss is =  0.14440137147903442  and accuracy is =  0.698\n",
            "At step  70  and at epoch =  2  the loss is =  0.13198664784431458  and accuracy is =  0.7524\n",
            "At step  70  and at epoch =  3  the loss is =  0.12568800151348114  and accuracy is =  0.798\n",
            "At step  70  and at epoch =  4  the loss is =  0.11821787804365158  and accuracy is =  0.8338\n",
            "At step  70  and at epoch =  5  the loss is =  0.1198253408074379  and accuracy is =  0.8868\n",
            "At step  70  and at epoch =  6  the loss is =  0.11508645117282867  and accuracy is =  0.9134\n",
            "At step  70  and at epoch =  7  the loss is =  0.12575939297676086  and accuracy is =  0.9452\n",
            "At step  70  and at epoch =  8  the loss is =  0.1370254009962082  and accuracy is =  0.9532\n",
            "At step  70  and at epoch =  9  the loss is =  0.12315866351127625  and accuracy is =  0.9546\n",
            "At step  70  and at epoch =  10  the loss is =  0.10830900073051453  and accuracy is =  0.9592\n",
            "At step  70  and at epoch =  11  the loss is =  0.12204089760780334  and accuracy is =  0.9882\n",
            "At step  70  and at epoch =  12  the loss is =  0.10832051932811737  and accuracy is =  0.9798\n",
            "At step  70  and at epoch =  13  the loss is =  0.11557161062955856  and accuracy is =  0.9812\n",
            "At step  70  and at epoch =  14  the loss is =  0.11554825305938721  and accuracy is =  0.9892\n",
            "At step  70  and at epoch =  15  the loss is =  0.1121143326163292  and accuracy is =  0.99\n",
            "At step  70  and at epoch =  16  the loss is =  0.10553883016109467  and accuracy is =  0.9914\n",
            "At step  70  and at epoch =  17  the loss is =  0.1228870376944542  and accuracy is =  0.993\n",
            "At step  70  and at epoch =  18  the loss is =  0.12136590480804443  and accuracy is =  0.9806\n",
            "At step  70  and at epoch =  19  the loss is =  0.12623806297779083  and accuracy is =  0.9786\n",
            "At step  70  and at epoch =  20  the loss is =  0.11866658180952072  and accuracy is =  0.9714\n",
            "At step  70  and at epoch =  21  the loss is =  0.11908076703548431  and accuracy is =  0.9834\n",
            "At step  70  and at epoch =  22  the loss is =  0.12767688930034637  and accuracy is =  0.9882\n",
            "At step  70  and at epoch =  23  the loss is =  0.10512935370206833  and accuracy is =  0.9814\n",
            "At step  70  and at epoch =  24  the loss is =  0.1185288280248642  and accuracy is =  0.993\n",
            "At step  70  and at epoch =  25  the loss is =  0.11039363592863083  and accuracy is =  0.9886\n",
            "At step  70  and at epoch =  26  the loss is =  0.09902603179216385  and accuracy is =  0.996\n",
            "At step  70  and at epoch =  27  the loss is =  0.12149924784898758  and accuracy is =  0.9984\n",
            "At step  70  and at epoch =  28  the loss is =  0.12008263915777206  and accuracy is =  0.9964\n",
            "At step  70  and at epoch =  29  the loss is =  0.12416984140872955  and accuracy is =  0.9966\n",
            "At step  70  and at epoch =  30  the loss is =  0.12366650998592377  and accuracy is =  0.9968\n",
            "At step  70  and at epoch =  31  the loss is =  0.12168832868337631  and accuracy is =  0.9876\n",
            "At step  70  and at epoch =  32  the loss is =  0.11944316327571869  and accuracy is =  0.9934\n",
            "At step  70  and at epoch =  33  the loss is =  0.12265799194574356  and accuracy is =  0.9906\n",
            "At step  70  and at epoch =  34  the loss is =  0.10914402455091476  and accuracy is =  0.9912\n",
            "At step  70  and at epoch =  35  the loss is =  0.1081680878996849  and accuracy is =  0.9988\n",
            "At step  70  and at epoch =  36  the loss is =  0.12425167113542557  and accuracy is =  0.9974\n",
            "At step  70  and at epoch =  37  the loss is =  0.09936317056417465  and accuracy is =  0.9852\n",
            "At step  70  and at epoch =  38  the loss is =  0.10593635588884354  and accuracy is =  0.995\n",
            "At step  70  and at epoch =  39  the loss is =  0.11866527050733566  and accuracy is =  0.9976\n",
            "At step  70  and at epoch =  40  the loss is =  0.11373274773359299  and accuracy is =  0.984\n",
            "At step  70  and at epoch =  41  the loss is =  0.11342641711235046  and accuracy is =  0.9924\n",
            "At step  70  and at epoch =  42  the loss is =  0.1115630716085434  and accuracy is =  0.9968\n",
            "At step  70  and at epoch =  43  the loss is =  0.1098652258515358  and accuracy is =  0.9988\n",
            "At step  70  and at epoch =  44  the loss is =  0.11200274527072906  and accuracy is =  0.9982\n",
            "At step  70  and at epoch =  45  the loss is =  0.10753019899129868  and accuracy is =  0.9994\n",
            "At step  70  and at epoch =  46  the loss is =  0.10570291429758072  and accuracy is =  0.999\n",
            "At step  70  and at epoch =  47  the loss is =  0.1095237210392952  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  48  the loss is =  0.1219736859202385  and accuracy is =  0.9996\n",
            "At step  70  and at epoch =  49  the loss is =  0.11216764897108078  and accuracy is =  0.9984\n",
            "At step  70  and at epoch =  50  the loss is =  0.12259635329246521  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  51  the loss is =  0.11263414472341537  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  52  the loss is =  0.10542581230401993  and accuracy is =  0.9992\n",
            "At step  70  and at epoch =  53  the loss is =  0.11758232861757278  and accuracy is =  0.9996\n",
            "At step  70  and at epoch =  54  the loss is =  0.11040285229682922  and accuracy is =  0.9996\n",
            "At step  70  and at epoch =  55  the loss is =  0.11490826308727264  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  56  the loss is =  0.1041300967335701  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  57  the loss is =  0.10586265474557877  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  58  the loss is =  0.10815191268920898  and accuracy is =  0.9994\n",
            "At step  70  and at epoch =  59  the loss is =  0.1167229637503624  and accuracy is =  0.999\n",
            "At step  70  and at epoch =  60  the loss is =  0.10360836982727051  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  61  the loss is =  0.12344112247228622  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  62  the loss is =  0.11703351885080338  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  63  the loss is =  0.10489145666360855  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  64  the loss is =  0.1276387721300125  and accuracy is =  0.9996\n",
            "At step  70  and at epoch =  65  the loss is =  0.1241939514875412  and accuracy is =  0.9998\n",
            "At step  70  and at epoch =  66  the loss is =  0.09829689562320709  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  67  the loss is =  0.1066071167588234  and accuracy is =  1.0\n",
            "At step  70  and at epoch =  68  the loss is =  0.1349133849143982  and accuracy is =  0.9992\n",
            "At step  70  and at epoch =  69  the loss is =  0.1081794872879982  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "64\n",
            "Validation Loss: 0.10319539904594421 Validation Accuracy : 0.199\n",
            "task = 80 \n",
            "train col =  [55 29 64 31 67  7 13 14 42  6]\n",
            "train col =  [[55 29 64 31 67  7 13 14 42  6]]\n",
            "At step  80  and at epoch =  0  the loss is =  0.16917861998081207  and accuracy is =  0.4386\n",
            "At step  80  and at epoch =  1  the loss is =  0.1459638625383377  and accuracy is =  0.5898\n",
            "At step  80  and at epoch =  2  the loss is =  0.16685275733470917  and accuracy is =  0.6568\n",
            "At step  80  and at epoch =  3  the loss is =  0.1654529571533203  and accuracy is =  0.6996\n",
            "At step  80  and at epoch =  4  the loss is =  0.14106392860412598  and accuracy is =  0.7406\n",
            "At step  80  and at epoch =  5  the loss is =  0.14634501934051514  and accuracy is =  0.8076\n",
            "At step  80  and at epoch =  6  the loss is =  0.15315255522727966  and accuracy is =  0.8542\n",
            "At step  80  and at epoch =  7  the loss is =  0.13881805539131165  and accuracy is =  0.8538\n",
            "At step  80  and at epoch =  8  the loss is =  0.15712767839431763  and accuracy is =  0.8954\n",
            "At step  80  and at epoch =  9  the loss is =  0.12777644395828247  and accuracy is =  0.925\n",
            "At step  80  and at epoch =  10  the loss is =  0.128704234957695  and accuracy is =  0.9578\n",
            "At step  80  and at epoch =  11  the loss is =  0.13710454106330872  and accuracy is =  0.9574\n",
            "At step  80  and at epoch =  12  the loss is =  0.12457072734832764  and accuracy is =  0.9668\n",
            "At step  80  and at epoch =  13  the loss is =  0.14337031543254852  and accuracy is =  0.9792\n",
            "At step  80  and at epoch =  14  the loss is =  0.1317945122718811  and accuracy is =  0.9554\n",
            "At step  80  and at epoch =  15  the loss is =  0.13005280494689941  and accuracy is =  0.976\n",
            "At step  80  and at epoch =  16  the loss is =  0.1208377406001091  and accuracy is =  0.9792\n",
            "At step  80  and at epoch =  17  the loss is =  0.12913566827774048  and accuracy is =  0.996\n",
            "At step  80  and at epoch =  18  the loss is =  0.12677325308322906  and accuracy is =  0.9876\n",
            "At step  80  and at epoch =  19  the loss is =  0.12196841835975647  and accuracy is =  0.9928\n",
            "At step  80  and at epoch =  20  the loss is =  0.13706348836421967  and accuracy is =  0.9978\n",
            "At step  80  and at epoch =  21  the loss is =  0.14813537895679474  and accuracy is =  0.9928\n",
            "At step  80  and at epoch =  22  the loss is =  0.13904987275600433  and accuracy is =  0.9882\n",
            "At step  80  and at epoch =  23  the loss is =  0.13670951128005981  and accuracy is =  0.9644\n",
            "At step  80  and at epoch =  24  the loss is =  0.14607375860214233  and accuracy is =  0.9518\n",
            "At step  80  and at epoch =  25  the loss is =  0.14218291640281677  and accuracy is =  0.9472\n",
            "At step  80  and at epoch =  26  the loss is =  0.13583959639072418  and accuracy is =  0.9668\n",
            "At step  80  and at epoch =  27  the loss is =  0.1326647698879242  and accuracy is =  0.9844\n",
            "At step  80  and at epoch =  28  the loss is =  0.14111746847629547  and accuracy is =  0.9834\n",
            "At step  80  and at epoch =  29  the loss is =  0.13250727951526642  and accuracy is =  0.986\n",
            "At step  80  and at epoch =  30  the loss is =  0.13262298703193665  and accuracy is =  0.9922\n",
            "At step  80  and at epoch =  31  the loss is =  0.13098807632923126  and accuracy is =  0.9956\n",
            "At step  80  and at epoch =  32  the loss is =  0.12428876757621765  and accuracy is =  0.996\n",
            "At step  80  and at epoch =  33  the loss is =  0.12769952416419983  and accuracy is =  0.999\n",
            "At step  80  and at epoch =  34  the loss is =  0.12863579392433167  and accuracy is =  0.9942\n",
            "At step  80  and at epoch =  35  the loss is =  0.133820042014122  and accuracy is =  0.9894\n",
            "At step  80  and at epoch =  36  the loss is =  0.1458153873682022  and accuracy is =  0.99\n",
            "At step  80  and at epoch =  37  the loss is =  0.1575908064842224  and accuracy is =  0.9834\n",
            "At step  80  and at epoch =  38  the loss is =  0.1283801645040512  and accuracy is =  0.9598\n",
            "At step  80  and at epoch =  39  the loss is =  0.14084258675575256  and accuracy is =  0.982\n",
            "At step  80  and at epoch =  40  the loss is =  0.14435932040214539  and accuracy is =  0.9788\n",
            "At step  80  and at epoch =  41  the loss is =  0.12904290854930878  and accuracy is =  0.9912\n",
            "At step  80  and at epoch =  42  the loss is =  0.12811218202114105  and accuracy is =  0.997\n",
            "At step  80  and at epoch =  43  the loss is =  0.13140366971492767  and accuracy is =  0.9946\n",
            "At step  80  and at epoch =  44  the loss is =  0.13975471258163452  and accuracy is =  0.999\n",
            "At step  80  and at epoch =  45  the loss is =  0.12867797911167145  and accuracy is =  0.9844\n",
            "At step  80  and at epoch =  46  the loss is =  0.1364302933216095  and accuracy is =  0.9924\n",
            "At step  80  and at epoch =  47  the loss is =  0.13047245144844055  and accuracy is =  0.9776\n",
            "At step  80  and at epoch =  48  the loss is =  0.1341579258441925  and accuracy is =  0.9992\n",
            "At step  80  and at epoch =  49  the loss is =  0.15674597024917603  and accuracy is =  0.999\n",
            "At step  80  and at epoch =  50  the loss is =  0.11425109952688217  and accuracy is =  0.9992\n",
            "At step  80  and at epoch =  51  the loss is =  0.12728793919086456  and accuracy is =  0.9996\n",
            "At step  80  and at epoch =  52  the loss is =  0.1314213126897812  and accuracy is =  0.9994\n",
            "At step  80  and at epoch =  53  the loss is =  0.13253726065158844  and accuracy is =  0.9988\n",
            "At step  80  and at epoch =  54  the loss is =  0.12955570220947266  and accuracy is =  0.9994\n",
            "At step  80  and at epoch =  55  the loss is =  0.12709102034568787  and accuracy is =  0.9996\n",
            "At step  80  and at epoch =  56  the loss is =  0.12111303210258484  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  57  the loss is =  0.12270385771989822  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  58  the loss is =  0.13478326797485352  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  59  the loss is =  0.13256938755512238  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  60  the loss is =  0.12543772161006927  and accuracy is =  0.9996\n",
            "At step  80  and at epoch =  61  the loss is =  0.12540434300899506  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  62  the loss is =  0.12773969769477844  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  63  the loss is =  0.12803862988948822  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  64  the loss is =  0.12555094063282013  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  65  the loss is =  0.12291759252548218  and accuracy is =  0.9998\n",
            "At step  80  and at epoch =  66  the loss is =  0.11790858954191208  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  67  the loss is =  0.12932831048965454  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  68  the loss is =  0.12888093292713165  and accuracy is =  1.0\n",
            "At step  80  and at epoch =  69  the loss is =  0.12722277641296387  and accuracy is =  0.9998\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "40\n",
            "Validation Loss: 0.09863279014825821 Validation Accuracy : 0.1728888888888889\n",
            "task = 90 \n",
            "train col =  [82  2 27 16 26  3  4  1  9  0]\n",
            "train col =  [[82  2 27 16 26  3  4  1  9  0]]\n",
            "At step  90  and at epoch =  0  the loss is =  0.1675378382205963  and accuracy is =  0.4996\n",
            "At step  90  and at epoch =  1  the loss is =  0.15434670448303223  and accuracy is =  0.6596\n",
            "At step  90  and at epoch =  2  the loss is =  0.15566645562648773  and accuracy is =  0.7284\n",
            "At step  90  and at epoch =  3  the loss is =  0.14957870543003082  and accuracy is =  0.7748\n",
            "At step  90  and at epoch =  4  the loss is =  0.1600807160139084  and accuracy is =  0.8144\n",
            "At step  90  and at epoch =  5  the loss is =  0.15753687918186188  and accuracy is =  0.8552\n",
            "At step  90  and at epoch =  6  the loss is =  0.1454763561487198  and accuracy is =  0.8858\n",
            "At step  90  and at epoch =  7  the loss is =  0.1517830789089203  and accuracy is =  0.9348\n",
            "At step  90  and at epoch =  8  the loss is =  0.15120717883110046  and accuracy is =  0.9526\n",
            "At step  90  and at epoch =  9  the loss is =  0.15009109675884247  and accuracy is =  0.955\n",
            "At step  90  and at epoch =  10  the loss is =  0.15481209754943848  and accuracy is =  0.9634\n",
            "At step  90  and at epoch =  11  the loss is =  0.14573445916175842  and accuracy is =  0.9696\n",
            "At step  90  and at epoch =  12  the loss is =  0.16405481100082397  and accuracy is =  0.9834\n",
            "At step  90  and at epoch =  13  the loss is =  0.15191985666751862  and accuracy is =  0.975\n",
            "At step  90  and at epoch =  14  the loss is =  0.15442360937595367  and accuracy is =  0.9842\n",
            "At step  90  and at epoch =  15  the loss is =  0.144741490483284  and accuracy is =  0.9908\n",
            "At step  90  and at epoch =  16  the loss is =  0.14644135534763336  and accuracy is =  0.989\n",
            "At step  90  and at epoch =  17  the loss is =  0.1663876324892044  and accuracy is =  0.975\n",
            "At step  90  and at epoch =  18  the loss is =  0.14194273948669434  and accuracy is =  0.9676\n",
            "At step  90  and at epoch =  19  the loss is =  0.14773042500019073  and accuracy is =  0.9858\n",
            "At step  90  and at epoch =  20  the loss is =  0.1483539342880249  and accuracy is =  0.984\n",
            "At step  90  and at epoch =  21  the loss is =  0.14439822733402252  and accuracy is =  0.9922\n",
            "At step  90  and at epoch =  22  the loss is =  0.1505478471517563  and accuracy is =  0.9758\n",
            "At step  90  and at epoch =  23  the loss is =  0.14010804891586304  and accuracy is =  0.9874\n",
            "At step  90  and at epoch =  24  the loss is =  0.14873984456062317  and accuracy is =  0.9956\n",
            "At step  90  and at epoch =  25  the loss is =  0.1369466781616211  and accuracy is =  0.9898\n",
            "At step  90  and at epoch =  26  the loss is =  0.14692693948745728  and accuracy is =  0.998\n",
            "At step  90  and at epoch =  27  the loss is =  0.1661534458398819  and accuracy is =  0.9972\n",
            "At step  90  and at epoch =  28  the loss is =  0.15159828960895538  and accuracy is =  0.9866\n",
            "At step  90  and at epoch =  29  the loss is =  0.1472899615764618  and accuracy is =  0.9924\n",
            "At step  90  and at epoch =  30  the loss is =  0.15471374988555908  and accuracy is =  0.9842\n",
            "At step  90  and at epoch =  31  the loss is =  0.1424381285905838  and accuracy is =  0.9832\n",
            "At step  90  and at epoch =  32  the loss is =  0.15890194475650787  and accuracy is =  0.9944\n",
            "At step  90  and at epoch =  33  the loss is =  0.14116619527339935  and accuracy is =  0.9914\n",
            "At step  90  and at epoch =  34  the loss is =  0.14598020911216736  and accuracy is =  0.9986\n",
            "At step  90  and at epoch =  35  the loss is =  0.14478753507137299  and accuracy is =  0.9888\n",
            "At step  90  and at epoch =  36  the loss is =  0.1656678318977356  and accuracy is =  0.9926\n",
            "At step  90  and at epoch =  37  the loss is =  0.1490408331155777  and accuracy is =  0.9854\n",
            "At step  90  and at epoch =  38  the loss is =  0.15859359502792358  and accuracy is =  0.9922\n",
            "At step  90  and at epoch =  39  the loss is =  0.1591535210609436  and accuracy is =  0.9852\n",
            "At step  90  and at epoch =  40  the loss is =  0.1331704705953598  and accuracy is =  0.9774\n",
            "At step  90  and at epoch =  41  the loss is =  0.14339616894721985  and accuracy is =  0.9962\n",
            "At step  90  and at epoch =  42  the loss is =  0.139872744679451  and accuracy is =  0.995\n",
            "At step  90  and at epoch =  43  the loss is =  0.13616959750652313  and accuracy is =  0.9934\n",
            "At step  90  and at epoch =  44  the loss is =  0.1404828280210495  and accuracy is =  0.999\n",
            "At step  90  and at epoch =  45  the loss is =  0.1579972803592682  and accuracy is =  0.9986\n",
            "At step  90  and at epoch =  46  the loss is =  0.1371937096118927  and accuracy is =  0.994\n",
            "At step  90  and at epoch =  47  the loss is =  0.13313454389572144  and accuracy is =  0.9992\n",
            "At step  90  and at epoch =  48  the loss is =  0.1513407826423645  and accuracy is =  0.9996\n",
            "At step  90  and at epoch =  49  the loss is =  0.14467138051986694  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  50  the loss is =  0.14376099407672882  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  51  the loss is =  0.16122612357139587  and accuracy is =  0.9996\n",
            "At step  90  and at epoch =  52  the loss is =  0.15018807351589203  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  53  the loss is =  0.13524968922138214  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  54  the loss is =  0.15792442858219147  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  55  the loss is =  0.15721318125724792  and accuracy is =  0.9996\n",
            "At step  90  and at epoch =  56  the loss is =  0.13554896414279938  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  57  the loss is =  0.1567414402961731  and accuracy is =  0.9994\n",
            "At step  90  and at epoch =  58  the loss is =  0.12992943823337555  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  59  the loss is =  0.13500724732875824  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  60  the loss is =  0.15170496702194214  and accuracy is =  0.9996\n",
            "At step  90  and at epoch =  61  the loss is =  0.1338801383972168  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  62  the loss is =  0.15268215537071228  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  63  the loss is =  0.14678965508937836  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  64  the loss is =  0.14208143949508667  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  65  the loss is =  0.13491468131542206  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  66  the loss is =  0.13511604070663452  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  67  the loss is =  0.15306738018989563  and accuracy is =  0.9998\n",
            "At step  90  and at epoch =  68  the loss is =  0.1302817165851593  and accuracy is =  1.0\n",
            "At step  90  and at epoch =  69  the loss is =  0.12966297566890717  and accuracy is =  1.0\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "16\n",
            "Validation Loss: 0.12836633622646332 Validation Accuracy : 0.1658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptf2qZjFbWNr",
        "colab_type": "code",
        "outputId": "9abd5dfb-a5d7-4919-d9fd-622899e38856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plotTask(pars_tasks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}