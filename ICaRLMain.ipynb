{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ICaRLMain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73dc3af7f87141529ec14c37cdec3a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4603ce9c2388489fa6b1d3f2d39054ed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a0273ce0f64149f3ad0f4f2995432b62",
              "IPY_MODEL_33f21e35404f444a97f782f8b2f3d531"
            ]
          }
        },
        "4603ce9c2388489fa6b1d3f2d39054ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0273ce0f64149f3ad0f4f2995432b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9481f4f5469e4e8a8f9b57e7bdb8e6ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a83a3af12602423ba6b97007b647da7c"
          }
        },
        "33f21e35404f444a97f782f8b2f3d531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f3cb0629ae76489987973757f83f20da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 31863095.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_942e6e2a4e1649ce8b3c63f3b7fa93d5"
          }
        },
        "9481f4f5469e4e8a8f9b57e7bdb8e6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a83a3af12602423ba6b97007b647da7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3cb0629ae76489987973757f83f20da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "942e6e2a4e1649ce8b3c63f3b7fa93d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/newLosses_Lucia/ICaRLMain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLMLAOPyrR1K",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LqwZJLUlcYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVa_FlnxrXIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "935c9317-1512-4cb7-e0be-c921baa8a0cd"
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone -b newLosses_Lucia https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 1280 (delta 116), reused 26 (delta 25), pack-reused 1112\u001b[K\n",
            "Receiving objects: 100% (1280/1280), 837.93 KiB | 2.23 MiB/s, done.\n",
            "Resolving deltas: 100% (816/816), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXQyHMXzrZ5A",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c24pdNxurdv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from DatasetCIFAR.data_set import Dataset \n",
        "from DatasetCIFAR import ResNet\n",
        "from DatasetCIFAR import utils\n",
        "from DatasetCIFAR import params\n",
        "from DatasetCIFAR import ICaRLModel\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "random.seed(params.SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FlAkShyryrf",
        "colab_type": "text"
      },
      "source": [
        "# Define Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP2iR2vl3Wiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transformer = transforms.Compose([transforms.RandomCrop(size = 32, padding=4),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CSNk0NlrvAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "73dc3af7f87141529ec14c37cdec3a9d",
            "4603ce9c2388489fa6b1d3f2d39054ed",
            "a0273ce0f64149f3ad0f4f2995432b62",
            "33f21e35404f444a97f782f8b2f3d531",
            "9481f4f5469e4e8a8f9b57e7bdb8e6ad",
            "a83a3af12602423ba6b97007b647da7c",
            "f3cb0629ae76489987973757f83f20da",
            "942e6e2a4e1649ce8b3c63f3b7fa93d5"
          ]
        },
        "outputId": "bc90e241-f5ba-410a-8fd3-beff29981fc0"
      },
      "source": [
        "trainDS = Dataset(train=True, transform = train_transformer)\n",
        "testDS = Dataset(train=False, transform = test_transformer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73dc3af7f87141529ec14c37cdec3a9d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3ge3VayryJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_splits = trainDS.splits\n",
        "test_splits = testDS.splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzxTlFF_rkfe",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgZtPkiPrmQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ICaRL = ResNet.resnet32(num_classes=100)\n",
        "ICaRL =  ICaRL.to(params.DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI8EyFmpOikN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exemplars = [None]*100\n",
        "\n",
        "test_indexes =  []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcFjbBGrOMz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6da017c-2a13-4204-b842-3868649cb865"
      },
      "source": [
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "  train_indexes = trainDS.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes + testDS.__getIndexesGroups__(task)\n",
        "\n",
        "  train_dataset = Subset(trainDS, train_indexes)\n",
        "  test_dataset = Subset(testDS, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE)\n",
        "  test_loader = DataLoader( test_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE )\n",
        "\n",
        "  ICaRL, exemplars = ICaRLModel.incrementalTrain(task, trainDS, ICaRL, exemplars)\n",
        "\n",
        "  col = []\n",
        "  for i,x in enumerate( train_splits[ :int(task/10) + 1]) : \n",
        "    v = np.array(x)\n",
        "    col = np.concatenate( (col,v), axis = None)\n",
        "    col = col.astype(int)\n",
        "  mean = None\n",
        "  total = 0.0\n",
        "  running_corrects = 0.0\n",
        "  for img, lbl, _ in train_loader:\n",
        "      img = img.float().to(params.DEVICE)\n",
        "      preds, mean = ICaRLModel.classify(img, exemplars, ICaRL, task, trainDS, mean)\n",
        "      preds = preds.to(params.DEVICE)\n",
        "      labels = utils.mapFunction(lbl, col).to(params.DEVICE)\n",
        "      #print(\"preds: \", preds.data)\n",
        "      #print(\"mapped labels: \", labels)\n",
        "      #print(\"labels: \", lbl)\n",
        "      total += len(lbl)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "      #print(running_corrects)\n",
        "  accuracy = float(running_corrects/total)\n",
        "  print(f'task: {task}', f'train accuracy = {accuracy}')\n",
        "\n",
        "  total = 0.0\n",
        "  running_corrects = 0.0\n",
        "  for img, lbl, _ in test_loader:\n",
        "      img = img.float().to(params.DEVICE)\n",
        "      preds, _ = ICaRLModel.classify(img, exemplars, ICaRL, task, trainD0S, mean)\n",
        "      preds = preds.to(params.DEVICE)\n",
        "      labels = utils.mapFunction(lbl, col).to(params.DEVICE)\n",
        "      #print(\"preds: \", preds.data)\n",
        "      #print(\"mapped labels: \", labels)\n",
        "      #print(\"labels: \", lbl)\n",
        "      total += len(lbl)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "      #print(running_corrects)\n",
        "  accuracy = float(running_corrects/total)\n",
        "  print(f'task: {task}', f'test accuracy = {accuracy}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "col =  [94 63 74 21 35 56 91 96 87 48]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  0  and at epoch =  0  the loss is =  0.017890598624944687  and accuracy is =  0.1006\n",
            "At step  0  and at epoch =  1  the loss is =  0.01488502137362957  and accuracy is =  0.1282\n",
            "At step  0  and at epoch =  2  the loss is =  0.009532357566058636  and accuracy is =  0.226\n",
            "At step  0  and at epoch =  3  the loss is =  0.007302583660930395  and accuracy is =  0.3414\n",
            "At step  0  and at epoch =  4  the loss is =  0.00843354407697916  and accuracy is =  0.3916\n",
            "At step  0  and at epoch =  5  the loss is =  0.010823175311088562  and accuracy is =  0.4224\n",
            "At step  0  and at epoch =  6  the loss is =  0.010508752427995205  and accuracy is =  0.4638\n",
            "At step  0  and at epoch =  7  the loss is =  0.007418529596179724  and accuracy is =  0.4786\n",
            "At step  0  and at epoch =  8  the loss is =  0.0068102809600532055  and accuracy is =  0.4928\n",
            "At step  0  and at epoch =  9  the loss is =  0.007457822561264038  and accuracy is =  0.54\n",
            "At step  0  and at epoch =  10  the loss is =  0.008042458444833755  and accuracy is =  0.5612\n",
            "At step  0  and at epoch =  11  the loss is =  0.0060506402514874935  and accuracy is =  0.5914\n",
            "At step  0  and at epoch =  12  the loss is =  0.0075305975042283535  and accuracy is =  0.602\n",
            "At step  0  and at epoch =  13  the loss is =  0.008021809160709381  and accuracy is =  0.6164\n",
            "At step  0  and at epoch =  14  the loss is =  0.0077776238322257996  and accuracy is =  0.64\n",
            "At step  0  and at epoch =  15  the loss is =  0.007804695051163435  and accuracy is =  0.6438\n",
            "At step  0  and at epoch =  16  the loss is =  0.0032003074884414673  and accuracy is =  0.6582\n",
            "At step  0  and at epoch =  17  the loss is =  0.011576369404792786  and accuracy is =  0.699\n",
            "At step  0  and at epoch =  18  the loss is =  0.011713262647390366  and accuracy is =  0.6612\n",
            "At step  0  and at epoch =  19  the loss is =  0.005172616336494684  and accuracy is =  0.6944\n",
            "At step  0  and at epoch =  20  the loss is =  0.005592901725322008  and accuracy is =  0.732\n",
            "At step  0  and at epoch =  21  the loss is =  0.0007591639296151698  and accuracy is =  0.7402\n",
            "At step  0  and at epoch =  22  the loss is =  0.001875454792752862  and accuracy is =  0.7686\n",
            "At step  0  and at epoch =  23  the loss is =  0.008940037339925766  and accuracy is =  0.7604\n",
            "At step  0  and at epoch =  24  the loss is =  0.008137873373925686  and accuracy is =  0.7286\n",
            "At step  0  and at epoch =  25  the loss is =  0.008021973073482513  and accuracy is =  0.6508\n",
            "At step  0  and at epoch =  26  the loss is =  0.004609052091836929  and accuracy is =  0.705\n",
            "At step  0  and at epoch =  27  the loss is =  0.006184739992022514  and accuracy is =  0.769\n",
            "At step  0  and at epoch =  28  the loss is =  0.004467198625206947  and accuracy is =  0.7828\n",
            "At step  0  and at epoch =  29  the loss is =  0.006128228735178709  and accuracy is =  0.8276\n",
            "At step  0  and at epoch =  30  the loss is =  0.012400844134390354  and accuracy is =  0.8102\n",
            "At step  0  and at epoch =  31  the loss is =  0.0055013918317854404  and accuracy is =  0.8134\n",
            "At step  0  and at epoch =  32  the loss is =  0.008005725219845772  and accuracy is =  0.7884\n",
            "At step  0  and at epoch =  33  the loss is =  0.004462655633687973  and accuracy is =  0.8484\n",
            "At step  0  and at epoch =  34  the loss is =  0.003542599268257618  and accuracy is =  0.864\n",
            "At step  0  and at epoch =  35  the loss is =  0.006837289314717054  and accuracy is =  0.8796\n",
            "At step  0  and at epoch =  36  the loss is =  0.0018689443822950125  and accuracy is =  0.8752\n",
            "At step  0  and at epoch =  37  the loss is =  0.001743199536576867  and accuracy is =  0.9034\n",
            "At step  0  and at epoch =  38  the loss is =  0.0015340503305196762  and accuracy is =  0.9274\n",
            "At step  0  and at epoch =  39  the loss is =  0.009439519606530666  and accuracy is =  0.9112\n",
            "At step  0  and at epoch =  40  the loss is =  0.008081724867224693  and accuracy is =  0.717\n",
            "At step  0  and at epoch =  41  the loss is =  0.007494921330362558  and accuracy is =  0.7818\n",
            "At step  0  and at epoch =  42  the loss is =  0.0034785582683980465  and accuracy is =  0.8436\n",
            "At step  0  and at epoch =  43  the loss is =  0.006999858655035496  and accuracy is =  0.8378\n",
            "At step  0  and at epoch =  44  the loss is =  0.008151432499289513  and accuracy is =  0.8418\n",
            "At step  0  and at epoch =  45  the loss is =  0.004095422104001045  and accuracy is =  0.8552\n",
            "At step  0  and at epoch =  46  the loss is =  0.0016068336553871632  and accuracy is =  0.9004\n",
            "At step  0  and at epoch =  47  the loss is =  0.006730703171342611  and accuracy is =  0.933\n",
            "At step  0  and at epoch =  48  the loss is =  0.003312030341476202  and accuracy is =  0.9048\n",
            "At step  0  and at epoch =  49  the loss is =  0.004906277637928724  and accuracy is =  0.9122\n",
            "At step  0  and at epoch =  50  the loss is =  0.009502016007900238  and accuracy is =  0.9152\n",
            "At step  0  and at epoch =  51  the loss is =  0.005857726093381643  and accuracy is =  0.8712\n",
            "At step  0  and at epoch =  52  the loss is =  0.0029592481441795826  and accuracy is =  0.908\n",
            "At step  0  and at epoch =  53  the loss is =  0.007519865874201059  and accuracy is =  0.9262\n",
            "At step  0  and at epoch =  54  the loss is =  0.004150729160755873  and accuracy is =  0.932\n",
            "At step  0  and at epoch =  55  the loss is =  0.0024892124347388744  and accuracy is =  0.9264\n",
            "At step  0  and at epoch =  56  the loss is =  0.03530821204185486  and accuracy is =  0.9268\n",
            "At step  0  and at epoch =  57  the loss is =  0.011851673014461994  and accuracy is =  0.8356\n",
            "At step  0  and at epoch =  58  the loss is =  0.003724097041413188  and accuracy is =  0.8626\n",
            "At step  0  and at epoch =  59  the loss is =  0.006013641599565744  and accuracy is =  0.9204\n",
            "At step  0  and at epoch =  60  the loss is =  0.0027485876344144344  and accuracy is =  0.95\n",
            "At step  0  and at epoch =  61  the loss is =  0.0024422230198979378  and accuracy is =  0.955\n",
            "At step  0  and at epoch =  62  the loss is =  0.002226136391982436  and accuracy is =  0.9778\n",
            "At step  0  and at epoch =  63  the loss is =  0.01166690606623888  and accuracy is =  0.969\n",
            "At step  0  and at epoch =  64  the loss is =  0.004341511521488428  and accuracy is =  0.7446\n",
            "At step  0  and at epoch =  65  the loss is =  0.008014584891498089  and accuracy is =  0.8722\n",
            "At step  0  and at epoch =  66  the loss is =  0.0020256396383047104  and accuracy is =  0.9096\n",
            "At step  0  and at epoch =  67  the loss is =  0.004072002600878477  and accuracy is =  0.9444\n",
            "At step  0  and at epoch =  68  the loss is =  0.006002884823828936  and accuracy is =  0.942\n",
            "At step  0  and at epoch =  69  the loss is =  0.00446354178711772  and accuracy is =  0.9584\n",
            "task: 0 train accuracy = 0.9694\n",
            "task: 0 test accuracy = 0.745\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  10  and at epoch =  0  the loss is =  0.004086821340024471  and accuracy is =  0.2891583452211127\n",
            "At step  10  and at epoch =  1  the loss is =  0.0034792234655469656  and accuracy is =  0.42211126961483597\n",
            "At step  10  and at epoch =  2  the loss is =  0.0036433932837098837  and accuracy is =  0.4854493580599144\n",
            "At step  10  and at epoch =  3  the loss is =  0.0029335159342736006  and accuracy is =  0.5330955777460771\n",
            "At step  10  and at epoch =  4  the loss is =  0.003929220139980316  and accuracy is =  0.564336661911555\n",
            "At step  10  and at epoch =  5  the loss is =  0.0030063712038099766  and accuracy is =  0.5982881597717546\n",
            "At step  10  and at epoch =  6  the loss is =  0.0029002991504967213  and accuracy is =  0.6186875891583452\n",
            "At step  10  and at epoch =  7  the loss is =  0.003052845597267151  and accuracy is =  0.6390870185449358\n",
            "At step  10  and at epoch =  8  the loss is =  0.002947412896901369  and accuracy is =  0.65848787446505\n",
            "At step  10  and at epoch =  9  the loss is =  0.002790124388411641  and accuracy is =  0.6821683309557774\n",
            "At step  10  and at epoch =  10  the loss is =  0.0030844889115542173  and accuracy is =  0.7005706134094151\n",
            "At step  10  and at epoch =  11  the loss is =  0.002729014726355672  and accuracy is =  0.7181169757489301\n",
            "At step  10  and at epoch =  12  the loss is =  0.0023229853250086308  and accuracy is =  0.7405135520684736\n",
            "At step  10  and at epoch =  13  the loss is =  0.0027997903525829315  and accuracy is =  0.7653352353780314\n",
            "At step  10  and at epoch =  14  the loss is =  0.002485742559656501  and accuracy is =  0.7895863052781741\n",
            "At step  10  and at epoch =  15  the loss is =  0.002428969833999872  and accuracy is =  0.8195435092724679\n",
            "At step  10  and at epoch =  16  the loss is =  0.0020950036123394966  and accuracy is =  0.844793152639087\n",
            "At step  10  and at epoch =  17  the loss is =  0.0019055786542594433  and accuracy is =  0.8579172610556348\n",
            "At step  10  and at epoch =  18  the loss is =  0.0015870314091444016  and accuracy is =  0.8808844507845934\n",
            "At step  10  and at epoch =  19  the loss is =  0.0017149518243968487  and accuracy is =  0.8998573466476463\n",
            "At step  10  and at epoch =  20  the loss is =  0.001519029843620956  and accuracy is =  0.9128388017118402\n",
            "At step  10  and at epoch =  21  the loss is =  0.00145035982131958  and accuracy is =  0.935663338088445\n",
            "At step  10  and at epoch =  22  the loss is =  0.0013843677006661892  and accuracy is =  0.9427960057061341\n",
            "At step  10  and at epoch =  23  the loss is =  0.0016030907863751054  and accuracy is =  0.9310984308131242\n",
            "At step  10  and at epoch =  24  the loss is =  0.0017066407017409801  and accuracy is =  0.9388017118402282\n",
            "At step  10  and at epoch =  25  the loss is =  0.0019089081324636936  and accuracy is =  0.9002853067047075\n",
            "At step  10  and at epoch =  26  the loss is =  0.001880101626738906  and accuracy is =  0.8757489300998573\n",
            "At step  10  and at epoch =  27  the loss is =  0.0016075677704066038  and accuracy is =  0.8914407988587731\n",
            "At step  10  and at epoch =  28  the loss is =  0.0013868545647710562  and accuracy is =  0.9366619115549215\n",
            "At step  10  and at epoch =  29  the loss is =  0.0015075725968927145  and accuracy is =  0.94679029957204\n",
            "At step  10  and at epoch =  30  the loss is =  0.001441212254576385  and accuracy is =  0.9542082738944365\n",
            "At step  10  and at epoch =  31  the loss is =  0.0012531051179394126  and accuracy is =  0.9699001426533523\n",
            "At step  10  and at epoch =  32  the loss is =  0.0010723801096901298  and accuracy is =  0.9689015691868759\n",
            "At step  10  and at epoch =  33  the loss is =  0.0017348288092762232  and accuracy is =  0.9661911554921541\n",
            "At step  10  and at epoch =  34  the loss is =  0.0018338020890951157  and accuracy is =  0.9269614835948645\n",
            "At step  10  and at epoch =  35  the loss is =  0.0018766691209748387  and accuracy is =  0.9191155492154066\n",
            "At step  10  and at epoch =  36  the loss is =  0.0016976859187707305  and accuracy is =  0.9222539229671898\n",
            "At step  10  and at epoch =  37  the loss is =  0.0016910911072045565  and accuracy is =  0.9041369472182597\n",
            "At step  10  and at epoch =  38  the loss is =  0.0016302276635542512  and accuracy is =  0.9192582025677604\n",
            "At step  10  and at epoch =  39  the loss is =  0.001263720914721489  and accuracy is =  0.9316690442225393\n",
            "At step  10  and at epoch =  40  the loss is =  0.0010976343182846904  and accuracy is =  0.9490727532097004\n",
            "At step  10  and at epoch =  41  the loss is =  0.0010157806100323796  and accuracy is =  0.9650499286733238\n",
            "At step  10  and at epoch =  42  the loss is =  0.0013107521226629615  and accuracy is =  0.9807417974322397\n",
            "At step  10  and at epoch =  43  the loss is =  0.000793025188613683  and accuracy is =  0.9894436519258203\n",
            "At step  10  and at epoch =  44  the loss is =  0.0009817788377404213  and accuracy is =  0.9948644793152639\n",
            "At step  10  and at epoch =  45  the loss is =  0.0007256208918988705  and accuracy is =  0.9965763195435092\n",
            "At step  10  and at epoch =  46  the loss is =  0.0007746128248982131  and accuracy is =  0.9968616262482168\n",
            "At step  10  and at epoch =  47  the loss is =  0.0007303088204935193  and accuracy is =  0.9978601997146933\n",
            "At step  10  and at epoch =  48  the loss is =  0.0006732393521815538  and accuracy is =  0.9981455064194008\n",
            "At step  10  and at epoch =  49  the loss is =  0.0006967102526687086  and accuracy is =  0.9980028530670471\n",
            "At step  10  and at epoch =  50  the loss is =  0.0006698908982798457  and accuracy is =  0.9977175463623396\n",
            "At step  10  and at epoch =  51  the loss is =  0.0006455071270465851  and accuracy is =  0.9977175463623396\n",
            "At step  10  and at epoch =  52  the loss is =  0.0006899275467731059  and accuracy is =  0.997432239657632\n",
            "At step  10  and at epoch =  53  the loss is =  0.000715971109457314  and accuracy is =  0.9984308131241084\n",
            "At step  10  and at epoch =  54  the loss is =  0.0006497417343780398  and accuracy is =  0.9982881597717547\n",
            "At step  10  and at epoch =  55  the loss is =  0.0008774142479524016  and accuracy is =  0.9975748930099857\n",
            "At step  10  and at epoch =  56  the loss is =  0.0008587592164985836  and accuracy is =  0.9968616262482168\n",
            "At step  10  and at epoch =  57  the loss is =  0.0007161656394600868  and accuracy is =  0.9981455064194008\n",
            "At step  10  and at epoch =  58  the loss is =  0.0006638664635829628  and accuracy is =  0.9980028530670471\n",
            "At step  10  and at epoch =  59  the loss is =  0.0007879248587414622  and accuracy is =  0.997432239657632\n",
            "At step  10  and at epoch =  60  the loss is =  0.0006206055404618382  and accuracy is =  0.9962910128388017\n",
            "At step  10  and at epoch =  61  the loss is =  0.0006863644812256098  and accuracy is =  0.9977175463623396\n",
            "At step  10  and at epoch =  62  the loss is =  0.0007145835552364588  and accuracy is =  0.9970042796005706\n",
            "At step  10  and at epoch =  63  the loss is =  0.0006601245258934796  and accuracy is =  0.9971469329529244\n",
            "At step  10  and at epoch =  64  the loss is =  0.0006828196928836405  and accuracy is =  0.9975748930099857\n",
            "At step  10  and at epoch =  65  the loss is =  0.0006956799770705402  and accuracy is =  0.997432239657632\n",
            "At step  10  and at epoch =  66  the loss is =  0.0005778486374765635  and accuracy is =  0.9965763195435092\n",
            "At step  10  and at epoch =  67  the loss is =  0.0006797357345931232  and accuracy is =  0.9971469329529244\n",
            "At step  10  and at epoch =  68  the loss is =  0.0007994545740075409  and accuracy is =  0.9921540656205421\n",
            "At step  10  and at epoch =  69  the loss is =  0.004265306983143091  and accuracy is =  0.7833095577746078\n",
            "task: 10 train accuracy = 0.409\n",
            "task: 10 test accuracy = 0.397\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76 75 89 23 99\n",
            " 39 66 54 69 84 61]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  20  and at epoch =  0  the loss is =  0.003498511854559183  and accuracy is =  0.1517094017094017\n",
            "At step  20  and at epoch =  1  the loss is =  0.002616887679323554  and accuracy is =  0.22877492877492878\n",
            "At step  20  and at epoch =  2  the loss is =  0.0029624674934893847  and accuracy is =  0.3041310541310541\n",
            "At step  20  and at epoch =  3  the loss is =  0.0029676267877221107  and accuracy is =  0.3901709401709402\n",
            "At step  20  and at epoch =  4  the loss is =  0.0028852801769971848  and accuracy is =  0.4341880341880342\n",
            "At step  20  and at epoch =  5  the loss is =  0.002483486896380782  and accuracy is =  0.4702279202279202\n",
            "At step  20  and at epoch =  6  the loss is =  0.0026808406691998243  and accuracy is =  0.5176638176638176\n",
            "At step  20  and at epoch =  7  the loss is =  0.002278365660458803  and accuracy is =  0.5566951566951567\n",
            "At step  20  and at epoch =  8  the loss is =  0.002515071304515004  and accuracy is =  0.5811965811965812\n",
            "At step  20  and at epoch =  9  the loss is =  0.001996035920456052  and accuracy is =  0.6055555555555555\n",
            "At step  20  and at epoch =  10  the loss is =  0.0018767064902931452  and accuracy is =  0.620940170940171\n",
            "At step  20  and at epoch =  11  the loss is =  0.0018432270735502243  and accuracy is =  0.6346153846153846\n",
            "At step  20  and at epoch =  12  the loss is =  0.001474096905440092  and accuracy is =  0.6354700854700854\n",
            "At step  20  and at epoch =  13  the loss is =  0.0017561553977429867  and accuracy is =  0.6609686609686609\n",
            "At step  20  and at epoch =  14  the loss is =  0.0015585713554173708  and accuracy is =  0.6594017094017094\n",
            "At step  20  and at epoch =  15  the loss is =  0.0018039625138044357  and accuracy is =  0.6582621082621083\n",
            "At step  20  and at epoch =  16  the loss is =  0.0014110073680058122  and accuracy is =  0.6626780626780627\n",
            "At step  20  and at epoch =  17  the loss is =  0.001660241512581706  and accuracy is =  0.6790598290598291\n",
            "At step  20  and at epoch =  18  the loss is =  0.0016048141987994313  and accuracy is =  0.6977207977207978\n",
            "At step  20  and at epoch =  19  the loss is =  0.0017687161453068256  and accuracy is =  0.6874643874643874\n",
            "At step  20  and at epoch =  20  the loss is =  0.0014117228565737605  and accuracy is =  0.7111111111111111\n",
            "At step  20  and at epoch =  21  the loss is =  0.0011210509110242128  and accuracy is =  0.7095441595441595\n",
            "At step  20  and at epoch =  22  the loss is =  0.0013122911332175136  and accuracy is =  0.701994301994302\n",
            "At step  20  and at epoch =  23  the loss is =  0.0016765915788710117  and accuracy is =  0.6972934472934473\n",
            "At step  20  and at epoch =  24  the loss is =  0.0011178040876984596  and accuracy is =  0.7061253561253561\n",
            "At step  20  and at epoch =  25  the loss is =  0.0011014894116669893  and accuracy is =  0.7094017094017094\n",
            "At step  20  and at epoch =  26  the loss is =  0.00087412737775594  and accuracy is =  0.7286324786324786\n",
            "At step  20  and at epoch =  27  the loss is =  0.0014783989172428846  and accuracy is =  0.7292022792022792\n",
            "At step  20  and at epoch =  28  the loss is =  0.001112224766984582  and accuracy is =  0.7237891737891737\n",
            "At step  20  and at epoch =  29  the loss is =  0.0008873960468918085  and accuracy is =  0.732051282051282\n",
            "At step  20  and at epoch =  30  the loss is =  0.000808786484412849  and accuracy is =  0.7354700854700855\n",
            "At step  20  and at epoch =  31  the loss is =  0.0009829713962972164  and accuracy is =  0.7447293447293447\n",
            "At step  20  and at epoch =  32  the loss is =  0.0007820102036930621  and accuracy is =  0.7491452991452991\n",
            "At step  20  and at epoch =  33  the loss is =  0.0011242568725720048  and accuracy is =  0.750997150997151\n",
            "At step  20  and at epoch =  34  the loss is =  0.000857879756949842  and accuracy is =  0.7411680911680911\n",
            "At step  20  and at epoch =  35  the loss is =  0.001163456472568214  and accuracy is =  0.7135327635327635\n",
            "At step  20  and at epoch =  36  the loss is =  0.0011226662900298834  and accuracy is =  0.7235042735042735\n",
            "At step  20  and at epoch =  37  the loss is =  0.0007878815522417426  and accuracy is =  0.7403133903133903\n",
            "At step  20  and at epoch =  38  the loss is =  0.0009199719061143696  and accuracy is =  0.7310541310541311\n",
            "At step  20  and at epoch =  39  the loss is =  0.0012203712249174714  and accuracy is =  0.73005698005698\n",
            "At step  20  and at epoch =  40  the loss is =  0.0009558711317367852  and accuracy is =  0.7425925925925926\n",
            "At step  20  and at epoch =  41  the loss is =  0.0009319154196418822  and accuracy is =  0.7450142450142451\n",
            "At step  20  and at epoch =  42  the loss is =  0.0010351287201046944  and accuracy is =  0.7552706552706553\n",
            "At step  20  and at epoch =  43  the loss is =  0.0010575702181085944  and accuracy is =  0.7434472934472934\n",
            "At step  20  and at epoch =  44  the loss is =  0.0009416710818186402  and accuracy is =  0.7421652421652422\n",
            "At step  20  and at epoch =  45  the loss is =  0.0009112594998441637  and accuracy is =  0.7512820512820513\n",
            "At step  20  and at epoch =  46  the loss is =  0.0006225929246284068  and accuracy is =  0.7457264957264957\n",
            "At step  20  and at epoch =  47  the loss is =  0.0009965221397578716  and accuracy is =  0.752991452991453\n",
            "At step  20  and at epoch =  48  the loss is =  0.0009278216166421771  and accuracy is =  0.7482905982905983\n",
            "At step  20  and at epoch =  49  the loss is =  0.0008707668166607618  and accuracy is =  0.7441595441595441\n",
            "At step  20  and at epoch =  50  the loss is =  0.0010213787900283933  and accuracy is =  0.7461538461538462\n",
            "At step  20  and at epoch =  51  the loss is =  0.0007790921372361481  and accuracy is =  0.7482905982905983\n",
            "At step  20  and at epoch =  52  the loss is =  0.00048572567175142467  and accuracy is =  0.757977207977208\n",
            "At step  20  and at epoch =  53  the loss is =  0.0005408305441960692  and accuracy is =  0.7652421652421653\n",
            "At step  20  and at epoch =  54  the loss is =  0.0005226638750173151  and accuracy is =  0.7675213675213676\n",
            "At step  20  and at epoch =  55  the loss is =  0.0005853524198755622  and accuracy is =  0.7670940170940171\n",
            "At step  20  and at epoch =  56  the loss is =  0.0010580063099041581  and accuracy is =  0.7613960113960114\n",
            "At step  20  and at epoch =  57  the loss is =  0.0007735679973848164  and accuracy is =  0.7462962962962963\n",
            "At step  20  and at epoch =  58  the loss is =  0.0015394507208839059  and accuracy is =  0.7081196581196582\n",
            "At step  20  and at epoch =  59  the loss is =  0.0012578930472955108  and accuracy is =  0.7373219373219373\n",
            "At step  20  and at epoch =  60  the loss is =  0.0010376068530604243  and accuracy is =  0.7443019943019943\n",
            "At step  20  and at epoch =  61  the loss is =  0.0008776893373578787  and accuracy is =  0.7494301994301994\n",
            "At step  20  and at epoch =  62  the loss is =  0.0005891689797863364  and accuracy is =  0.7621082621082621\n",
            "At step  20  and at epoch =  63  the loss is =  0.00042150699300691485  and accuracy is =  0.7643874643874644\n",
            "At step  20  and at epoch =  64  the loss is =  0.0005698252352885902  and accuracy is =  0.7686609686609687\n",
            "At step  20  and at epoch =  65  the loss is =  0.00037154974415898323  and accuracy is =  0.7676638176638176\n",
            "At step  20  and at epoch =  66  the loss is =  0.00046150508569553494  and accuracy is =  0.7719373219373219\n",
            "At step  20  and at epoch =  67  the loss is =  0.0002872415352612734  and accuracy is =  0.7770655270655271\n",
            "At step  20  and at epoch =  68  the loss is =  0.00038360338658094406  and accuracy is =  0.7777777777777778\n",
            "At step  20  and at epoch =  69  the loss is =  0.00030376113136298954  and accuracy is =  0.7784900284900285\n",
            "task: 20 train accuracy = 0.999\n",
            "task: 20 test accuracy = 0.412\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76 75 89 23 99\n",
            " 39 66 54 69 84 61 85 24 98 41 73 58 78 77 70 49]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  30  and at epoch =  0  the loss is =  0.0036453029606491327  and accuracy is =  0.124679029957204\n",
            "At step  30  and at epoch =  1  the loss is =  0.0033432606142014265  and accuracy is =  0.22382310984308132\n",
            "At step  30  and at epoch =  2  the loss is =  0.00365832494571805  and accuracy is =  0.2810271041369472\n",
            "At step  30  and at epoch =  3  the loss is =  0.002658142941072583  and accuracy is =  0.3359486447931526\n",
            "At step  30  and at epoch =  4  the loss is =  0.0030867934692651033  and accuracy is =  0.3825962910128388\n",
            "At step  30  and at epoch =  5  the loss is =  0.0025663580745458603  and accuracy is =  0.42082738944365194\n",
            "At step  30  and at epoch =  6  the loss is =  0.002719710348173976  and accuracy is =  0.4594864479315264\n",
            "At step  30  and at epoch =  7  the loss is =  0.003119507571682334  and accuracy is =  0.4955777460770328\n",
            "At step  30  and at epoch =  8  the loss is =  0.0019365689950063825  and accuracy is =  0.5179743223965764\n",
            "At step  30  and at epoch =  9  the loss is =  0.0020027407445013523  and accuracy is =  0.543509272467903\n",
            "At step  30  and at epoch =  10  the loss is =  0.0021712423767894506  and accuracy is =  0.5726105563480742\n",
            "At step  30  and at epoch =  11  the loss is =  0.0017632548697292805  and accuracy is =  0.5736091298145506\n",
            "At step  30  and at epoch =  12  the loss is =  0.0021637463942170143  and accuracy is =  0.5914407988587732\n",
            "At step  30  and at epoch =  13  the loss is =  0.0019451265688985586  and accuracy is =  0.604564907275321\n",
            "At step  30  and at epoch =  14  the loss is =  0.0016585676930844784  and accuracy is =  0.6089871611982881\n",
            "At step  30  and at epoch =  15  the loss is =  0.001532570575363934  and accuracy is =  0.6323823109843081\n",
            "At step  30  and at epoch =  16  the loss is =  0.0020275511778891087  and accuracy is =  0.6410841654778887\n",
            "At step  30  and at epoch =  17  the loss is =  0.0015160631155595183  and accuracy is =  0.6172610556348074\n",
            "At step  30  and at epoch =  18  the loss is =  0.0019161103991791606  and accuracy is =  0.6281027104136947\n",
            "At step  30  and at epoch =  19  the loss is =  0.001138608786277473  and accuracy is =  0.6415121255349501\n",
            "At step  30  and at epoch =  20  the loss is =  0.001489760004915297  and accuracy is =  0.6706134094151213\n",
            "At step  30  and at epoch =  21  the loss is =  0.0012242085067555308  and accuracy is =  0.6566333808844508\n",
            "At step  30  and at epoch =  22  the loss is =  0.0013359070289880037  and accuracy is =  0.6620542082738944\n",
            "At step  30  and at epoch =  23  the loss is =  0.0016115503385663033  and accuracy is =  0.6676176890156919\n",
            "At step  30  and at epoch =  24  the loss is =  0.0012456579133868217  and accuracy is =  0.6643366619115549\n",
            "At step  30  and at epoch =  25  the loss is =  0.0012123286724090576  and accuracy is =  0.6754636233951498\n",
            "At step  30  and at epoch =  26  the loss is =  0.0009245432447642088  and accuracy is =  0.6767475035663338\n",
            "At step  30  and at epoch =  27  the loss is =  0.0011608710046857595  and accuracy is =  0.684450784593438\n",
            "At step  30  and at epoch =  28  the loss is =  0.001115009537898004  and accuracy is =  0.6835948644793153\n",
            "At step  30  and at epoch =  29  the loss is =  0.0008993573137558997  and accuracy is =  0.6920114122681883\n",
            "At step  30  and at epoch =  30  the loss is =  0.0011258326703682542  and accuracy is =  0.6960057061340942\n",
            "At step  30  and at epoch =  31  the loss is =  0.0017214963445439935  and accuracy is =  0.6800285306704708\n",
            "At step  30  and at epoch =  32  the loss is =  0.0009987743105739355  and accuracy is =  0.6718972895863052\n",
            "At step  30  and at epoch =  33  the loss is =  0.0008428192231804132  and accuracy is =  0.6908701854493581\n",
            "At step  30  and at epoch =  34  the loss is =  0.0009490839438512921  and accuracy is =  0.7042796005706135\n",
            "At step  30  and at epoch =  35  the loss is =  0.001398547668941319  and accuracy is =  0.6998573466476462\n",
            "At step  30  and at epoch =  36  the loss is =  0.0014550199266523123  and accuracy is =  0.6988587731811697\n",
            "At step  30  and at epoch =  37  the loss is =  0.001056619337759912  and accuracy is =  0.6754636233951498\n",
            "At step  30  and at epoch =  38  the loss is =  0.0008358426857739687  and accuracy is =  0.6947218259629101\n",
            "At step  30  and at epoch =  39  the loss is =  0.0005701155751012266  and accuracy is =  0.7104136947218259\n",
            "At step  30  and at epoch =  40  the loss is =  0.00046611251309514046  and accuracy is =  0.7152639087018545\n",
            "At step  30  and at epoch =  41  the loss is =  0.0008723399951122701  and accuracy is =  0.718830242510699\n",
            "At step  30  and at epoch =  42  the loss is =  0.0005872413166798651  and accuracy is =  0.718830242510699\n",
            "At step  30  and at epoch =  43  the loss is =  0.0006767841405235231  and accuracy is =  0.718830242510699\n",
            "At step  30  and at epoch =  44  the loss is =  0.0006693782634101808  and accuracy is =  0.7184022824536377\n",
            "At step  30  and at epoch =  45  the loss is =  0.00170343357603997  and accuracy is =  0.7004279600570613\n",
            "At step  30  and at epoch =  46  the loss is =  0.0015018823323771358  and accuracy is =  0.6536376604850214\n",
            "At step  30  and at epoch =  47  the loss is =  0.0009105524513870478  and accuracy is =  0.6863052781740371\n",
            "At step  30  and at epoch =  48  the loss is =  0.0009907654020935297  and accuracy is =  0.7062767475035663\n",
            "At step  30  and at epoch =  49  the loss is =  0.0011203880421817303  and accuracy is =  0.7118402282453637\n",
            "At step  30  and at epoch =  50  the loss is =  0.0009162077330984175  and accuracy is =  0.7154065620542083\n",
            "At step  30  and at epoch =  51  the loss is =  0.00047971619642339647  and accuracy is =  0.7222539229671897\n",
            "At step  30  and at epoch =  52  the loss is =  0.0004228407924529165  and accuracy is =  0.724679029957204\n",
            "At step  30  and at epoch =  53  the loss is =  0.0006892802775837481  and accuracy is =  0.724679029957204\n",
            "At step  30  and at epoch =  54  the loss is =  0.00045053972280584276  and accuracy is =  0.7265335235378031\n",
            "At step  30  and at epoch =  55  the loss is =  0.0006725702551193535  and accuracy is =  0.7251069900142654\n",
            "At step  30  and at epoch =  56  the loss is =  0.0006099983002059162  and accuracy is =  0.7303851640513552\n",
            "At step  30  and at epoch =  57  the loss is =  0.001455775462090969  and accuracy is =  0.7289586305278174\n",
            "At step  30  and at epoch =  58  the loss is =  0.0008570801583118737  and accuracy is =  0.7179743223965763\n",
            "At step  30  and at epoch =  59  the loss is =  0.001538580865599215  and accuracy is =  0.7029957203994294\n",
            "At step  30  and at epoch =  60  the loss is =  0.0010720179416239262  and accuracy is =  0.6830242510699002\n",
            "At step  30  and at epoch =  61  the loss is =  0.0007978238863870502  and accuracy is =  0.7135520684736091\n",
            "At step  30  and at epoch =  62  the loss is =  0.0007801124593243003  and accuracy is =  0.7077032810271041\n",
            "At step  30  and at epoch =  63  the loss is =  0.0008960039704106748  and accuracy is =  0.7114122681883024\n",
            "At step  30  and at epoch =  64  the loss is =  0.0007555041229352355  and accuracy is =  0.7186875891583452\n",
            "At step  30  and at epoch =  65  the loss is =  0.0006341072730720043  and accuracy is =  0.7222539229671897\n",
            "At step  30  and at epoch =  66  the loss is =  0.0007318491116166115  and accuracy is =  0.7131241084165478\n",
            "At step  30  and at epoch =  67  the loss is =  0.0007683170842938125  and accuracy is =  0.7161198288159771\n",
            "At step  30  and at epoch =  68  the loss is =  0.00043016159906983376  and accuracy is =  0.7216833095577746\n",
            "At step  30  and at epoch =  69  the loss is =  0.000498984765727073  and accuracy is =  0.7281027104136947\n",
            "task: 30 train accuracy = 0.9862\n",
            "task: 30 test accuracy = 0.292\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76 75 89 23 99\n",
            " 39 66 54 69 84 61 85 24 98 41 73 58 78 77 70 49 65 88 36 93 45 10 90 17\n",
            " 32 59]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  40  and at epoch =  0  the loss is =  0.004150181543081999  and accuracy is =  0.07769886363636364\n",
            "At step  40  and at epoch =  1  the loss is =  0.00349306664429605  and accuracy is =  0.13238636363636364\n",
            "At step  40  and at epoch =  2  the loss is =  0.003334860084578395  and accuracy is =  0.165625\n",
            "At step  40  and at epoch =  3  the loss is =  0.0032344250939786434  and accuracy is =  0.20383522727272727\n",
            "At step  40  and at epoch =  4  the loss is =  0.0034536297898739576  and accuracy is =  0.2505681818181818\n",
            "At step  40  and at epoch =  5  the loss is =  0.0027509918436408043  and accuracy is =  0.2890625\n",
            "At step  40  and at epoch =  6  the loss is =  0.0034630894660949707  and accuracy is =  0.33366477272727274\n",
            "At step  40  and at epoch =  7  the loss is =  0.0029897515196353197  and accuracy is =  0.37017045454545455\n",
            "At step  40  and at epoch =  8  the loss is =  0.003300122916698456  and accuracy is =  0.41264204545454547\n",
            "At step  40  and at epoch =  9  the loss is =  0.0026847647968679667  and accuracy is =  0.43707386363636364\n",
            "At step  40  and at epoch =  10  the loss is =  0.0027518372517079115  and accuracy is =  0.46136363636363636\n",
            "At step  40  and at epoch =  11  the loss is =  0.002917761681601405  and accuracy is =  0.4953125\n",
            "At step  40  and at epoch =  12  the loss is =  0.001964479684829712  and accuracy is =  0.4965909090909091\n",
            "At step  40  and at epoch =  13  the loss is =  0.0023476704955101013  and accuracy is =  0.501278409090909\n",
            "At step  40  and at epoch =  14  the loss is =  0.0022425991483032703  and accuracy is =  0.5258522727272728\n",
            "At step  40  and at epoch =  15  the loss is =  0.0016622673720121384  and accuracy is =  0.5400568181818182\n",
            "At step  40  and at epoch =  16  the loss is =  0.002865324029698968  and accuracy is =  0.532528409090909\n",
            "At step  40  and at epoch =  17  the loss is =  0.0021282248198986053  and accuracy is =  0.5526988636363637\n",
            "At step  40  and at epoch =  18  the loss is =  0.001618831418454647  and accuracy is =  0.5566761363636363\n",
            "At step  40  and at epoch =  19  the loss is =  0.0025151383597403765  and accuracy is =  0.5782670454545454\n",
            "At step  40  and at epoch =  20  the loss is =  0.0019623825792223215  and accuracy is =  0.5747159090909091\n",
            "At step  40  and at epoch =  21  the loss is =  0.0016961535438895226  and accuracy is =  0.5826704545454545\n",
            "At step  40  and at epoch =  22  the loss is =  0.0014760401099920273  and accuracy is =  0.6092329545454546\n",
            "At step  40  and at epoch =  23  the loss is =  0.0013835316058248281  and accuracy is =  0.6166193181818181\n",
            "At step  40  and at epoch =  24  the loss is =  0.0018965586787089705  and accuracy is =  0.6254261363636363\n",
            "At step  40  and at epoch =  25  the loss is =  0.0018691932782530785  and accuracy is =  0.6133522727272728\n",
            "At step  40  and at epoch =  26  the loss is =  0.0017849986907094717  and accuracy is =  0.6257102272727273\n",
            "At step  40  and at epoch =  27  the loss is =  0.0013560341903939843  and accuracy is =  0.6316761363636364\n",
            "At step  40  and at epoch =  28  the loss is =  0.001472685020416975  and accuracy is =  0.6288352272727272\n",
            "At step  40  and at epoch =  29  the loss is =  0.0016958432970568538  and accuracy is =  0.6303977272727272\n",
            "At step  40  and at epoch =  30  the loss is =  0.001366684678941965  and accuracy is =  0.6440340909090909\n",
            "At step  40  and at epoch =  31  the loss is =  0.0017871662275865674  and accuracy is =  0.6524147727272728\n",
            "At step  40  and at epoch =  32  the loss is =  0.0014132597716525197  and accuracy is =  0.6579545454545455\n",
            "At step  40  and at epoch =  33  the loss is =  0.0014959812397137284  and accuracy is =  0.6626420454545454\n",
            "At step  40  and at epoch =  34  the loss is =  0.0011304615763947368  and accuracy is =  0.6598011363636364\n",
            "At step  40  and at epoch =  35  the loss is =  0.0015858861152082682  and accuracy is =  0.6545454545454545\n",
            "At step  40  and at epoch =  36  the loss is =  0.0009448438649997115  and accuracy is =  0.6551136363636364\n",
            "At step  40  and at epoch =  37  the loss is =  0.0009173771250061691  and accuracy is =  0.6823863636363636\n",
            "At step  40  and at epoch =  38  the loss is =  0.0012287113349884748  and accuracy is =  0.6833806818181818\n",
            "At step  40  and at epoch =  39  the loss is =  0.0013596193166449666  and accuracy is =  0.6768465909090909\n",
            "At step  40  and at epoch =  40  the loss is =  0.000837358005810529  and accuracy is =  0.6762784090909091\n",
            "At step  40  and at epoch =  41  the loss is =  0.001006326638162136  and accuracy is =  0.6688920454545455\n",
            "At step  40  and at epoch =  42  the loss is =  0.0022763677407056093  and accuracy is =  0.6478693181818181\n",
            "At step  40  and at epoch =  43  the loss is =  0.001222071354277432  and accuracy is =  0.6583806818181818\n",
            "At step  40  and at epoch =  44  the loss is =  0.0012131260009482503  and accuracy is =  0.6713068181818181\n",
            "At step  40  and at epoch =  45  the loss is =  0.0008839912479743361  and accuracy is =  0.6819602272727273\n",
            "At step  40  and at epoch =  46  the loss is =  0.0005420992965810001  and accuracy is =  0.6943181818181818\n",
            "At step  40  and at epoch =  47  the loss is =  0.0006242417148314416  and accuracy is =  0.7002840909090909\n",
            "At step  40  and at epoch =  48  the loss is =  0.0011927346931770444  and accuracy is =  0.6846590909090909\n",
            "At step  40  and at epoch =  49  the loss is =  0.001136498642154038  and accuracy is =  0.6603693181818182\n",
            "At step  40  and at epoch =  50  the loss is =  0.0011017489014193416  and accuracy is =  0.6671875\n",
            "At step  40  and at epoch =  51  the loss is =  0.0008697922457940876  and accuracy is =  0.6838068181818182\n",
            "At step  40  and at epoch =  52  the loss is =  0.0008006503921933472  and accuracy is =  0.6941761363636364\n",
            "At step  40  and at epoch =  53  the loss is =  0.0005973416264168918  and accuracy is =  0.6957386363636363\n",
            "At step  40  and at epoch =  54  the loss is =  0.0006317883380688727  and accuracy is =  0.7001420454545455\n",
            "At step  40  and at epoch =  55  the loss is =  0.0004957857890985906  and accuracy is =  0.7049715909090909\n",
            "At step  40  and at epoch =  56  the loss is =  0.000613272248301655  and accuracy is =  0.7068181818181818\n",
            "At step  40  and at epoch =  57  the loss is =  0.0005680149188265204  and accuracy is =  0.7080965909090909\n",
            "At step  40  and at epoch =  58  the loss is =  0.0008766790269874036  and accuracy is =  0.7039772727272727\n",
            "At step  40  and at epoch =  59  the loss is =  0.0007792390533722937  and accuracy is =  0.7009943181818182\n",
            "At step  40  and at epoch =  60  the loss is =  0.0012811386259272695  and accuracy is =  0.6710227272727273\n",
            "At step  40  and at epoch =  61  the loss is =  0.0014015098568052053  and accuracy is =  0.6325284090909091\n",
            "At step  40  and at epoch =  62  the loss is =  0.001346754259429872  and accuracy is =  0.6678977272727272\n",
            "At step  40  and at epoch =  63  the loss is =  0.0005743270157836378  and accuracy is =  0.6826704545454545\n",
            "At step  40  and at epoch =  64  the loss is =  0.0009648395352996886  and accuracy is =  0.7008522727272727\n",
            "At step  40  and at epoch =  65  the loss is =  0.0005509894108399749  and accuracy is =  0.7051136363636363\n",
            "At step  40  and at epoch =  66  the loss is =  0.0002837400825228542  and accuracy is =  0.7089488636363637\n",
            "At step  40  and at epoch =  67  the loss is =  0.00036362840910442173  and accuracy is =  0.7098011363636364\n",
            "At step  40  and at epoch =  68  the loss is =  0.0007841135375201702  and accuracy is =  0.7092329545454545\n",
            "At step  40  and at epoch =  69  the loss is =  0.0003156806342303753  and accuracy is =  0.709375\n",
            "task: 40 train accuracy = 0.9738\n",
            "task: 40 test accuracy = 0.2038\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76 75 89 23 99\n",
            " 39 66 54 69 84 61 85 24 98 41 73 58 78 77 70 49 65 88 36 93 45 10 90 17\n",
            " 32 59 83 43 53 11 86 19 38 30 40 50]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  50  and at epoch =  0  the loss is =  0.011231753043830395  and accuracy is =  0.06624113475177305\n",
            "At step  50  and at epoch =  1  the loss is =  0.008118492551147938  and accuracy is =  0.12652482269503545\n",
            "At step  50  and at epoch =  2  the loss is =  0.007029511965811253  and accuracy is =  0.2072340425531915\n",
            "At step  50  and at epoch =  3  the loss is =  0.006200412288308144  and accuracy is =  0.26368794326241135\n",
            "At step  50  and at epoch =  4  the loss is =  0.006677205208688974  and accuracy is =  0.30368794326241133\n",
            "At step  50  and at epoch =  5  the loss is =  0.01012773159891367  and accuracy is =  0.329645390070922\n",
            "At step  50  and at epoch =  6  the loss is =  0.0065418086014688015  and accuracy is =  0.34425531914893615\n",
            "At step  50  and at epoch =  7  the loss is =  0.004285069182515144  and accuracy is =  0.38070921985815603\n",
            "At step  50  and at epoch =  8  the loss is =  0.008037022314965725  and accuracy is =  0.39673758865248226\n",
            "At step  50  and at epoch =  9  the loss is =  0.006354907061904669  and accuracy is =  0.40695035460992907\n",
            "At step  50  and at epoch =  10  the loss is =  0.0042990087531507015  and accuracy is =  0.4195744680851064\n",
            "At step  50  and at epoch =  11  the loss is =  0.004317097831517458  and accuracy is =  0.43148936170212765\n",
            "At step  50  and at epoch =  12  the loss is =  0.0035419180057942867  and accuracy is =  0.44156028368794326\n",
            "At step  50  and at epoch =  13  the loss is =  0.005404087249189615  and accuracy is =  0.46709219858156026\n",
            "At step  50  and at epoch =  14  the loss is =  0.004120165947824717  and accuracy is =  0.44666666666666666\n",
            "At step  50  and at epoch =  15  the loss is =  0.0145189194008708  and accuracy is =  0.4748936170212766\n",
            "At step  50  and at epoch =  16  the loss is =  0.0054291668348014355  and accuracy is =  0.40070921985815605\n",
            "At step  50  and at epoch =  17  the loss is =  0.0022873093839734793  and accuracy is =  0.45177304964539006\n",
            "At step  50  and at epoch =  18  the loss is =  0.004546524491161108  and accuracy is =  0.4991489361702128\n",
            "At step  50  and at epoch =  19  the loss is =  0.0034028964582830667  and accuracy is =  0.5224113475177306\n",
            "At step  50  and at epoch =  20  the loss is =  0.004601155407726765  and accuracy is =  0.5280851063829787\n",
            "At step  50  and at epoch =  21  the loss is =  0.005540177691727877  and accuracy is =  0.5113475177304965\n",
            "At step  50  and at epoch =  22  the loss is =  0.0021649934351444244  and accuracy is =  0.5377304964539007\n",
            "At step  50  and at epoch =  23  the loss is =  0.004222104325890541  and accuracy is =  0.5628368794326241\n",
            "At step  50  and at epoch =  24  the loss is =  0.0036077983677387238  and accuracy is =  0.570354609929078\n",
            "At step  50  and at epoch =  25  the loss is =  0.005623265169560909  and accuracy is =  0.5811347517730496\n",
            "At step  50  and at epoch =  26  the loss is =  0.004829249810427427  and accuracy is =  0.5304964539007092\n",
            "At step  50  and at epoch =  27  the loss is =  0.008158071897923946  and accuracy is =  0.5748936170212766\n",
            "At step  50  and at epoch =  28  the loss is =  0.007249540649354458  and accuracy is =  0.47829787234042553\n",
            "At step  50  and at epoch =  29  the loss is =  0.005098732654005289  and accuracy is =  0.5676595744680851\n",
            "At step  50  and at epoch =  30  the loss is =  0.001945632160641253  and accuracy is =  0.5568794326241134\n",
            "At step  50  and at epoch =  31  the loss is =  0.0013523728121072054  and accuracy is =  0.6134751773049646\n",
            "At step  50  and at epoch =  32  the loss is =  0.006501808296889067  and accuracy is =  0.6299290780141844\n",
            "At step  50  and at epoch =  33  the loss is =  0.004453986883163452  and accuracy is =  0.5656737588652482\n",
            "At step  50  and at epoch =  34  the loss is =  0.007264366373419762  and accuracy is =  0.5921985815602837\n",
            "At step  50  and at epoch =  35  the loss is =  0.004963545594364405  and accuracy is =  0.450354609929078\n",
            "At step  50  and at epoch =  36  the loss is =  0.00517801009118557  and accuracy is =  0.569645390070922\n",
            "At step  50  and at epoch =  37  the loss is =  0.004734782502055168  and accuracy is =  0.558581560283688\n",
            "At step  50  and at epoch =  38  the loss is =  0.004384832456707954  and accuracy is =  0.6045390070921985\n",
            "At step  50  and at epoch =  39  the loss is =  0.0018412850331515074  and accuracy is =  0.624822695035461\n",
            "At step  50  and at epoch =  40  the loss is =  0.004275091458112001  and accuracy is =  0.642127659574468\n",
            "At step  50  and at epoch =  41  the loss is =  0.0036252967547625303  and accuracy is =  0.6235460992907801\n",
            "At step  50  and at epoch =  42  the loss is =  0.001768562593497336  and accuracy is =  0.634468085106383\n",
            "At step  50  and at epoch =  43  the loss is =  0.00568030308932066  and accuracy is =  0.6556028368794327\n",
            "At step  50  and at epoch =  44  the loss is =  0.002024770947173238  and accuracy is =  0.624113475177305\n",
            "At step  50  and at epoch =  45  the loss is =  0.004484225530177355  and accuracy is =  0.6612765957446809\n",
            "At step  50  and at epoch =  46  the loss is =  0.0032105669379234314  and accuracy is =  0.6374468085106383\n",
            "At step  50  and at epoch =  47  the loss is =  0.006635153200477362  and accuracy is =  0.6321985815602836\n",
            "At step  50  and at epoch =  48  the loss is =  0.0038156358059495687  and accuracy is =  0.6212765957446809\n",
            "At step  50  and at epoch =  49  the loss is =  0.0014015777269378304  and accuracy is =  0.6719148936170213\n",
            "At step  50  and at epoch =  50  the loss is =  0.0033148829825222492  and accuracy is =  0.6746099290780142\n",
            "At step  50  and at epoch =  51  the loss is =  0.005034490022808313  and accuracy is =  0.5459574468085107\n",
            "At step  50  and at epoch =  52  the loss is =  0.0022159090731292963  and accuracy is =  0.6365957446808511\n",
            "At step  50  and at epoch =  53  the loss is =  0.0024003677535802126  and accuracy is =  0.6676595744680851\n",
            "At step  50  and at epoch =  54  the loss is =  0.005497949663549662  and accuracy is =  0.6750354609929078\n",
            "At step  50  and at epoch =  55  the loss is =  0.002043974120169878  and accuracy is =  0.5981560283687943\n",
            "At step  50  and at epoch =  56  the loss is =  0.011469164863228798  and accuracy is =  0.6672340425531915\n",
            "At step  50  and at epoch =  57  the loss is =  0.004729254171252251  and accuracy is =  0.4975886524822695\n",
            "At step  50  and at epoch =  58  the loss is =  0.008145539090037346  and accuracy is =  0.5923404255319149\n",
            "At step  50  and at epoch =  59  the loss is =  0.004196807276457548  and accuracy is =  0.5707801418439716\n",
            "At step  50  and at epoch =  60  the loss is =  0.0012632394209504128  and accuracy is =  0.6601418439716312\n",
            "At step  50  and at epoch =  61  the loss is =  0.007485846523195505  and accuracy is =  0.6818439716312057\n",
            "At step  50  and at epoch =  62  the loss is =  0.007887700572609901  and accuracy is =  0.6341843971631206\n",
            "At step  50  and at epoch =  63  the loss is =  0.0050471448339521885  and accuracy is =  0.5788652482269504\n",
            "At step  50  and at epoch =  64  the loss is =  0.0013222995912656188  and accuracy is =  0.6609929078014184\n",
            "At step  50  and at epoch =  65  the loss is =  0.004412373527884483  and accuracy is =  0.6687943262411348\n",
            "At step  50  and at epoch =  66  the loss is =  0.008162599988281727  and accuracy is =  0.6719148936170213\n",
            "At step  50  and at epoch =  67  the loss is =  0.01094190962612629  and accuracy is =  0.6533333333333333\n",
            "At step  50  and at epoch =  68  the loss is =  0.005251229740679264  and accuracy is =  0.6514893617021277\n",
            "At step  50  and at epoch =  69  the loss is =  0.0038053232710808516  and accuracy is =  0.5895035460992908\n",
            "task: 50 train accuracy = 0.6324\n",
            "task: 50 test accuracy = 0.1635\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76 75 89 23 99\n",
            " 39 66 54 69 84 61 85 24 98 41 73 58 78 77 70 49 65 88 36 93 45 10 90 17\n",
            " 32 59 83 43 53 11 86 19 38 30 40 50 57 81 12 95 25 47 34 52 44 72]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  60  and at epoch =  0  the loss is =  0.0037292977795004845  and accuracy is =  0.0815340909090909\n",
            "At step  60  and at epoch =  1  the loss is =  0.0032094954513013363  and accuracy is =  0.13877840909090908\n",
            "At step  60  and at epoch =  2  the loss is =  0.0034054224379360676  and accuracy is =  0.24985795454545454\n",
            "At step  60  and at epoch =  3  the loss is =  0.0030558728612959385  and accuracy is =  0.30894886363636365\n",
            "At step  60  and at epoch =  4  the loss is =  0.0030385246500372887  and accuracy is =  0.35255681818181817\n",
            "At step  60  and at epoch =  5  the loss is =  0.003197059500962496  and accuracy is =  0.3951704545454545\n",
            "At step  60  and at epoch =  6  the loss is =  0.002470071194693446  and accuracy is =  0.4122159090909091\n",
            "At step  60  and at epoch =  7  the loss is =  0.002481097588315606  and accuracy is =  0.4330965909090909\n",
            "At step  60  and at epoch =  8  the loss is =  0.002216417109593749  and accuracy is =  0.45795454545454545\n",
            "At step  60  and at epoch =  9  the loss is =  0.0020660334266722202  and accuracy is =  0.4961647727272727\n",
            "At step  60  and at epoch =  10  the loss is =  0.0017326072556897998  and accuracy is =  0.5140625\n",
            "At step  60  and at epoch =  11  the loss is =  0.0020594471134245396  and accuracy is =  0.5384943181818181\n",
            "At step  60  and at epoch =  12  the loss is =  0.00208120490424335  and accuracy is =  0.5551136363636363\n",
            "At step  60  and at epoch =  13  the loss is =  0.001829570159316063  and accuracy is =  0.5774147727272727\n",
            "At step  60  and at epoch =  14  the loss is =  0.0017979004187509418  and accuracy is =  0.6015625\n",
            "At step  60  and at epoch =  15  the loss is =  0.001939370296895504  and accuracy is =  0.5909090909090909\n",
            "At step  60  and at epoch =  16  the loss is =  0.0016537208575755358  and accuracy is =  0.6018465909090909\n",
            "At step  60  and at epoch =  17  the loss is =  0.001479102298617363  and accuracy is =  0.6183238636363636\n",
            "At step  60  and at epoch =  18  the loss is =  0.001648035948164761  and accuracy is =  0.6173295454545454\n",
            "At step  60  and at epoch =  19  the loss is =  0.0013807433424517512  and accuracy is =  0.6335227272727273\n",
            "At step  60  and at epoch =  20  the loss is =  0.0016759997233748436  and accuracy is =  0.6257102272727273\n",
            "At step  60  and at epoch =  21  the loss is =  0.0018299090443179011  and accuracy is =  0.6211647727272728\n",
            "At step  60  and at epoch =  22  the loss is =  0.0016379008302465081  and accuracy is =  0.6348011363636363\n",
            "At step  60  and at epoch =  23  the loss is =  0.001397048239596188  and accuracy is =  0.6440340909090909\n",
            "At step  60  and at epoch =  24  the loss is =  0.0012732448522001505  and accuracy is =  0.6588068181818182\n",
            "At step  60  and at epoch =  25  the loss is =  0.0014352869475260377  and accuracy is =  0.6637784090909091\n",
            "At step  60  and at epoch =  26  the loss is =  0.0009544072672724724  and accuracy is =  0.6626420454545454\n",
            "At step  60  and at epoch =  27  the loss is =  0.002165037440136075  and accuracy is =  0.6488636363636363\n",
            "At step  60  and at epoch =  28  the loss is =  0.0012807538732886314  and accuracy is =  0.6228693181818182\n",
            "At step  60  and at epoch =  29  the loss is =  0.0007693605148233473  and accuracy is =  0.6674715909090909\n",
            "At step  60  and at epoch =  30  the loss is =  0.0008553947554901242  and accuracy is =  0.6873579545454546\n",
            "At step  60  and at epoch =  31  the loss is =  0.0009668139973655343  and accuracy is =  0.6830965909090909\n",
            "At step  60  and at epoch =  32  the loss is =  0.0011737116146832705  and accuracy is =  0.6821022727272728\n",
            "At step  60  and at epoch =  33  the loss is =  0.0008889888995327055  and accuracy is =  0.6605113636363636\n",
            "At step  60  and at epoch =  34  the loss is =  0.0013019597390666604  and accuracy is =  0.6546875\n",
            "At step  60  and at epoch =  35  the loss is =  0.001318395254202187  and accuracy is =  0.6714488636363637\n",
            "At step  60  and at epoch =  36  the loss is =  0.0009778393432497978  and accuracy is =  0.6651988636363636\n",
            "At step  60  and at epoch =  37  the loss is =  0.0008686513756401837  and accuracy is =  0.6848011363636364\n",
            "At step  60  and at epoch =  38  the loss is =  0.0010552009334787726  and accuracy is =  0.6833806818181818\n",
            "At step  60  and at epoch =  39  the loss is =  0.0006699782679788768  and accuracy is =  0.6951704545454546\n",
            "At step  60  and at epoch =  40  the loss is =  0.000803016999270767  and accuracy is =  0.6963068181818182\n",
            "At step  60  and at epoch =  41  the loss is =  0.0006469807703979313  and accuracy is =  0.6947443181818181\n",
            "At step  60  and at epoch =  42  the loss is =  0.000648384157102555  and accuracy is =  0.6947443181818181\n",
            "At step  60  and at epoch =  43  the loss is =  0.0015685204416513443  and accuracy is =  0.671875\n",
            "At step  60  and at epoch =  44  the loss is =  0.0009690274018794298  and accuracy is =  0.6561079545454546\n",
            "At step  60  and at epoch =  45  the loss is =  0.0009186589741148055  and accuracy is =  0.6853693181818182\n",
            "At step  60  and at epoch =  46  the loss is =  0.0007561666425317526  and accuracy is =  0.6953125\n",
            "At step  60  and at epoch =  47  the loss is =  0.000605016655754298  and accuracy is =  0.69375\n",
            "At step  60  and at epoch =  48  the loss is =  0.001183388172648847  and accuracy is =  0.6788352272727273\n",
            "At step  60  and at epoch =  49  the loss is =  0.001281743636354804  and accuracy is =  0.6697443181818182\n",
            "At step  60  and at epoch =  50  the loss is =  0.0007654990768060088  and accuracy is =  0.6920454545454545\n",
            "At step  60  and at epoch =  51  the loss is =  0.000681037490721792  and accuracy is =  0.6992897727272728\n",
            "At step  60  and at epoch =  52  the loss is =  0.0005511338822543621  and accuracy is =  0.6997159090909091\n",
            "At step  60  and at epoch =  53  the loss is =  0.0008117178804241121  and accuracy is =  0.6926136363636364\n",
            "At step  60  and at epoch =  54  the loss is =  0.0009431247017346323  and accuracy is =  0.6876420454545454\n",
            "At step  60  and at epoch =  55  the loss is =  0.0007964540272951126  and accuracy is =  0.688778409090909\n",
            "At step  60  and at epoch =  56  the loss is =  0.00039647347875870764  and accuracy is =  0.6943181818181818\n",
            "At step  60  and at epoch =  57  the loss is =  0.0007563894614577293  and accuracy is =  0.701846590909091\n",
            "At step  60  and at epoch =  58  the loss is =  0.0004058183985762298  and accuracy is =  0.7038352272727273\n",
            "At step  60  and at epoch =  59  the loss is =  0.00047062462545000017  and accuracy is =  0.7089488636363637\n",
            "At step  60  and at epoch =  60  the loss is =  0.0003128607932012528  and accuracy is =  0.709659090909091\n",
            "At step  60  and at epoch =  61  the loss is =  0.00033997284481301904  and accuracy is =  0.709375\n",
            "At step  60  and at epoch =  62  the loss is =  0.00030275480821728706  and accuracy is =  0.709659090909091\n",
            "At step  60  and at epoch =  63  the loss is =  0.00017177268455270678  and accuracy is =  0.7098011363636364\n",
            "At step  60  and at epoch =  64  the loss is =  0.00013246008893474936  and accuracy is =  0.7100852272727273\n",
            "At step  60  and at epoch =  65  the loss is =  0.00017143900913652033  and accuracy is =  0.7100852272727273\n",
            "At step  60  and at epoch =  66  the loss is =  0.0003057905414607376  and accuracy is =  0.7100852272727273\n",
            "At step  60  and at epoch =  67  the loss is =  0.00013298839621711522  and accuracy is =  0.7100852272727273\n",
            "At step  60  and at epoch =  68  the loss is =  0.00024035513342823833  and accuracy is =  0.7102272727272727\n",
            "At step  60  and at epoch =  69  the loss is =  0.0002540148561820388  and accuracy is =  0.7102272727272727\n",
            "task: 60 train accuracy = 1.0\n",
            "task: 60 test accuracy = 0.15142857142857144\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76 75 89 23 99\n",
            " 39 66 54 69 84 61 85 24 98 41 73 58 78 77 70 49 65 88 36 93 45 10 90 17\n",
            " 32 59 83 43 53 11 86 19 38 30 40 50 57 81 12 95 25 47 34 52 44 72 46 79\n",
            " 20 28  5 71  8 18 33 15]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  70  and at epoch =  0  the loss is =  0.003551867324858904  and accuracy is =  0.06244665718349929\n",
            "At step  70  and at epoch =  1  the loss is =  0.0031921141780912876  and accuracy is =  0.16415362731152205\n",
            "At step  70  and at epoch =  2  the loss is =  0.003145311726257205  and accuracy is =  0.22162162162162163\n",
            "At step  70  and at epoch =  3  the loss is =  0.0031190491281449795  and accuracy is =  0.25689900426742535\n",
            "At step  70  and at epoch =  4  the loss is =  0.0029744338244199753  and accuracy is =  0.2839260312944524\n",
            "At step  70  and at epoch =  5  the loss is =  0.0027880026027560234  and accuracy is =  0.31934566145092463\n",
            "At step  70  and at epoch =  6  the loss is =  0.0024672425352036953  and accuracy is =  0.34623044096728306\n",
            "At step  70  and at epoch =  7  the loss is =  0.0027263478841632605  and accuracy is =  0.3879089615931721\n",
            "At step  70  and at epoch =  8  the loss is =  0.0020683505572378635  and accuracy is =  0.410099573257468\n",
            "At step  70  and at epoch =  9  the loss is =  0.0029906437266618013  and accuracy is =  0.4321479374110953\n",
            "At step  70  and at epoch =  10  the loss is =  0.0023859411012381315  and accuracy is =  0.44310099573257467\n",
            "At step  70  and at epoch =  11  the loss is =  0.001969583798199892  and accuracy is =  0.4601706970128023\n",
            "At step  70  and at epoch =  12  the loss is =  0.002499752677977085  and accuracy is =  0.4800853485064011\n",
            "At step  70  and at epoch =  13  the loss is =  0.0018924809992313385  and accuracy is =  0.5024182076813656\n",
            "At step  70  and at epoch =  14  the loss is =  0.001938498578965664  and accuracy is =  0.5071123755334281\n",
            "At step  70  and at epoch =  15  the loss is =  0.0015958001604303718  and accuracy is =  0.5203413940256045\n",
            "At step  70  and at epoch =  16  the loss is =  0.002265210496261716  and accuracy is =  0.5211948790896159\n",
            "At step  70  and at epoch =  17  the loss is =  0.0021234808955341578  and accuracy is =  0.5394025604551921\n",
            "At step  70  and at epoch =  18  the loss is =  0.002077818848192692  and accuracy is =  0.5581792318634424\n",
            "At step  70  and at epoch =  19  the loss is =  0.0016226202715188265  and accuracy is =  0.566145092460882\n",
            "At step  70  and at epoch =  20  the loss is =  0.002045408124104142  and accuracy is =  0.5749644381223329\n",
            "At step  70  and at epoch =  21  the loss is =  0.0014906866708770394  and accuracy is =  0.5738264580369844\n",
            "At step  70  and at epoch =  22  the loss is =  0.0016436593141406775  and accuracy is =  0.5819345661450924\n",
            "At step  70  and at epoch =  23  the loss is =  0.001684179762378335  and accuracy is =  0.5836415362731152\n",
            "At step  70  and at epoch =  24  the loss is =  0.00138151366263628  and accuracy is =  0.5930298719772404\n",
            "At step  70  and at epoch =  25  the loss is =  0.0015273571480065584  and accuracy is =  0.6099573257467994\n",
            "At step  70  and at epoch =  26  the loss is =  0.0016446039080619812  and accuracy is =  0.6076813655761024\n",
            "At step  70  and at epoch =  27  the loss is =  0.0012906297342851758  and accuracy is =  0.6173541963015647\n",
            "At step  70  and at epoch =  28  the loss is =  0.001592453452758491  and accuracy is =  0.630298719772404\n",
            "At step  70  and at epoch =  29  the loss is =  0.0015697487397119403  and accuracy is =  0.6266002844950214\n",
            "At step  70  and at epoch =  30  the loss is =  0.0013098266208544374  and accuracy is =  0.6362731152204837\n",
            "At step  70  and at epoch =  31  the loss is =  0.0013753661187365651  and accuracy is =  0.6335704125177809\n",
            "At step  70  and at epoch =  32  the loss is =  0.001725495676510036  and accuracy is =  0.6396870554765292\n",
            "At step  70  and at epoch =  33  the loss is =  0.0013891822891309857  and accuracy is =  0.6509246088193457\n",
            "At step  70  and at epoch =  34  the loss is =  0.001333239721134305  and accuracy is =  0.6530583214793741\n",
            "At step  70  and at epoch =  35  the loss is =  0.0009988333331421018  and accuracy is =  0.6524893314366998\n",
            "At step  70  and at epoch =  36  the loss is =  0.0009474277612753212  and accuracy is =  0.6667140825035562\n",
            "At step  70  and at epoch =  37  the loss is =  0.0016917068278416991  and accuracy is =  0.6709815078236131\n",
            "At step  70  and at epoch =  38  the loss is =  0.0008627689676359296  and accuracy is =  0.6776671408250355\n",
            "At step  70  and at epoch =  39  the loss is =  0.00112597132101655  and accuracy is =  0.6679943100995732\n",
            "At step  70  and at epoch =  40  the loss is =  0.0015975059941411018  and accuracy is =  0.6578947368421053\n",
            "At step  70  and at epoch =  41  the loss is =  0.000977586372755468  and accuracy is =  0.659601706970128\n",
            "At step  70  and at epoch =  42  the loss is =  0.0009120928007178009  and accuracy is =  0.6809388335704125\n",
            "At step  70  and at epoch =  43  the loss is =  0.0011222598841413856  and accuracy is =  0.6742532005689901\n",
            "At step  70  and at epoch =  44  the loss is =  0.00094278046162799  and accuracy is =  0.6816500711237553\n",
            "At step  70  and at epoch =  45  the loss is =  0.0008872210164554417  and accuracy is =  0.6786628733997155\n",
            "At step  70  and at epoch =  46  the loss is =  0.0009040455333888531  and accuracy is =  0.6773826458036984\n",
            "At step  70  and at epoch =  47  the loss is =  0.0006881614681333303  and accuracy is =  0.6859174964438123\n",
            "At step  70  and at epoch =  48  the loss is =  0.0007798631559126079  and accuracy is =  0.6917496443812233\n",
            "At step  70  and at epoch =  49  the loss is =  0.0006695553311146796  and accuracy is =  0.689900426742532\n",
            "At step  70  and at epoch =  50  the loss is =  0.0007092064479365945  and accuracy is =  0.6935988620199146\n",
            "At step  70  and at epoch =  51  the loss is =  0.0008154973620548844  and accuracy is =  0.6897581792318634\n",
            "At step  70  and at epoch =  52  the loss is =  0.0008498969255015254  and accuracy is =  0.6822190611664296\n",
            "At step  70  and at epoch =  53  the loss is =  0.00099681515712291  and accuracy is =  0.688904694167852\n",
            "At step  70  and at epoch =  54  the loss is =  0.0009139991016127169  and accuracy is =  0.6799431009957326\n",
            "At step  70  and at epoch =  55  the loss is =  0.0008482668199576437  and accuracy is =  0.6809388335704125\n",
            "At step  70  and at epoch =  56  the loss is =  0.0008086652378551662  and accuracy is =  0.6756756756756757\n",
            "At step  70  and at epoch =  57  the loss is =  0.0006352793425321579  and accuracy is =  0.6900426742532005\n",
            "At step  70  and at epoch =  58  the loss is =  0.00048002501716837287  and accuracy is =  0.7019914651493598\n",
            "At step  70  and at epoch =  59  the loss is =  0.000410773151088506  and accuracy is =  0.7066856330014225\n",
            "At step  70  and at epoch =  60  the loss is =  0.0003095125430263579  and accuracy is =  0.7095305832147938\n",
            "At step  70  and at epoch =  61  the loss is =  0.0006361139821819961  and accuracy is =  0.7076813655761024\n",
            "At step  70  and at epoch =  62  the loss is =  0.0010991171002388  and accuracy is =  0.6972972972972973\n",
            "At step  70  and at epoch =  63  the loss is =  0.0012064921902492642  and accuracy is =  0.6530583214793741\n",
            "At step  70  and at epoch =  64  the loss is =  0.000777649343945086  and accuracy is =  0.6761024182076814\n",
            "At step  70  and at epoch =  65  the loss is =  0.000636264740023762  and accuracy is =  0.6931721194879089\n",
            "At step  70  and at epoch =  66  the loss is =  0.00043246379937045276  and accuracy is =  0.7054054054054054\n",
            "At step  70  and at epoch =  67  the loss is =  0.000822135538328439  and accuracy is =  0.7083926031294452\n",
            "At step  70  and at epoch =  68  the loss is =  0.0003009479260072112  and accuracy is =  0.7095305832147938\n",
            "At step  70  and at epoch =  69  the loss is =  0.00026314848219044507  and accuracy is =  0.7089615931721195\n",
            "task: 70 train accuracy = 0.9678\n",
            "task: 70 test accuracy = 0.10725\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76 75 89 23 99\n",
            " 39 66 54 69 84 61 85 24 98 41 73 58 78 77 70 49 65 88 36 93 45 10 90 17\n",
            " 32 59 83 43 53 11 86 19 38 30 40 50 57 81 12 95 25 47 34 52 44 72 46 79\n",
            " 20 28  5 71  8 18 33 15 55 29 64 31 67  7 13 14 42  6]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  80  and at epoch =  0  the loss is =  0.004685691557824612  and accuracy is =  0.06299435028248587\n",
            "At step  80  and at epoch =  1  the loss is =  0.005579990800470114  and accuracy is =  0.10522598870056497\n",
            "At step  80  and at epoch =  2  the loss is =  0.0038383493665605783  and accuracy is =  0.13615819209039548\n",
            "At step  80  and at epoch =  3  the loss is =  0.004536451771855354  and accuracy is =  0.18036723163841809\n",
            "At step  80  and at epoch =  4  the loss is =  0.0040765791200101376  and accuracy is =  0.20423728813559322\n",
            "At step  80  and at epoch =  5  the loss is =  0.00432331208139658  and accuracy is =  0.22598870056497175\n",
            "At step  80  and at epoch =  6  the loss is =  0.003662900999188423  and accuracy is =  0.24632768361581922\n",
            "At step  80  and at epoch =  7  the loss is =  0.0036149455700069666  and accuracy is =  0.2754237288135593\n",
            "At step  80  and at epoch =  8  the loss is =  0.004253114573657513  and accuracy is =  0.2916666666666667\n",
            "At step  80  and at epoch =  9  the loss is =  0.0036037724930793047  and accuracy is =  0.3112994350282486\n",
            "At step  80  and at epoch =  10  the loss is =  0.005033895373344421  and accuracy is =  0.32429378531073444\n",
            "At step  80  and at epoch =  11  the loss is =  0.00404741894453764  and accuracy is =  0.3498587570621469\n",
            "At step  80  and at epoch =  12  the loss is =  0.0033065229654312134  and accuracy is =  0.36468926553672315\n",
            "At step  80  and at epoch =  13  the loss is =  0.004435787443071604  and accuracy is =  0.3755649717514124\n",
            "At step  80  and at epoch =  14  the loss is =  0.002325716195628047  and accuracy is =  0.3919491525423729\n",
            "At step  80  and at epoch =  15  the loss is =  0.0037309371400624514  and accuracy is =  0.3998587570621469\n",
            "At step  80  and at epoch =  16  the loss is =  0.0036370097659528255  and accuracy is =  0.4137005649717514\n",
            "At step  80  and at epoch =  17  the loss is =  0.004091802053153515  and accuracy is =  0.434180790960452\n",
            "At step  80  and at epoch =  18  the loss is =  0.002988608554005623  and accuracy is =  0.4475988700564972\n",
            "At step  80  and at epoch =  19  the loss is =  0.003312933025881648  and accuracy is =  0.46285310734463275\n",
            "At step  80  and at epoch =  20  the loss is =  0.0023297443985939026  and accuracy is =  0.4711864406779661\n",
            "At step  80  and at epoch =  21  the loss is =  0.0025539465714246035  and accuracy is =  0.47824858757062144\n",
            "At step  80  and at epoch =  22  the loss is =  0.002420665929093957  and accuracy is =  0.49392655367231636\n",
            "At step  80  and at epoch =  23  the loss is =  0.0026071651373058558  and accuracy is =  0.5117231638418079\n",
            "At step  80  and at epoch =  24  the loss is =  0.0027283879462629557  and accuracy is =  0.5086158192090395\n",
            "At step  80  and at epoch =  25  the loss is =  0.0016545100370422006  and accuracy is =  0.5072033898305085\n",
            "At step  80  and at epoch =  26  the loss is =  0.003361127572134137  and accuracy is =  0.5323446327683616\n",
            "At step  80  and at epoch =  27  the loss is =  0.0027853960637003183  and accuracy is =  0.5392655367231638\n",
            "At step  80  and at epoch =  28  the loss is =  0.0028136135078966618  and accuracy is =  0.551271186440678\n",
            "At step  80  and at epoch =  29  the loss is =  0.0026723789051175117  and accuracy is =  0.551271186440678\n",
            "At step  80  and at epoch =  30  the loss is =  0.0025162629317492247  and accuracy is =  0.5375706214689265\n",
            "At step  80  and at epoch =  31  the loss is =  0.003289824118837714  and accuracy is =  0.5600282485875706\n",
            "At step  80  and at epoch =  32  the loss is =  0.0019980084616690874  and accuracy is =  0.5572033898305084\n",
            "At step  80  and at epoch =  33  the loss is =  0.0019179466180503368  and accuracy is =  0.5761299435028249\n",
            "At step  80  and at epoch =  34  the loss is =  0.0025562902446836233  and accuracy is =  0.6049435028248588\n",
            "At step  80  and at epoch =  35  the loss is =  0.0025535356253385544  and accuracy is =  0.5939265536723164\n",
            "At step  80  and at epoch =  36  the loss is =  0.002333467360585928  and accuracy is =  0.5875706214689266\n",
            "At step  80  and at epoch =  37  the loss is =  0.0022647602017968893  and accuracy is =  0.5754237288135593\n",
            "At step  80  and at epoch =  38  the loss is =  0.0022755677346140146  and accuracy is =  0.572316384180791\n",
            "At step  80  and at epoch =  39  the loss is =  0.0018810972105711699  and accuracy is =  0.6031073446327684\n",
            "At step  80  and at epoch =  40  the loss is =  0.0022492732387036085  and accuracy is =  0.6115819209039548\n",
            "At step  80  and at epoch =  41  the loss is =  0.0018185999942943454  and accuracy is =  0.6076271186440678\n",
            "At step  80  and at epoch =  42  the loss is =  0.002133656060323119  and accuracy is =  0.6076271186440678\n",
            "At step  80  and at epoch =  43  the loss is =  0.0014332415303215384  and accuracy is =  0.610593220338983\n",
            "At step  80  and at epoch =  44  the loss is =  0.0014190986985340714  and accuracy is =  0.6450564971751412\n",
            "At step  80  and at epoch =  45  the loss is =  0.0017310850089415908  and accuracy is =  0.6583333333333333\n",
            "At step  80  and at epoch =  46  the loss is =  0.0012013119412586093  and accuracy is =  0.6498587570621469\n",
            "At step  80  and at epoch =  47  the loss is =  0.0010339341824874282  and accuracy is =  0.6524011299435029\n",
            "At step  80  and at epoch =  48  the loss is =  0.0018946450436487794  and accuracy is =  0.6478813559322034\n",
            "At step  80  and at epoch =  49  the loss is =  0.0015404446749016643  and accuracy is =  0.586864406779661\n",
            "At step  80  and at epoch =  50  the loss is =  0.0023800020571798086  and accuracy is =  0.6271186440677966\n",
            "At step  80  and at epoch =  51  the loss is =  0.0017450758023187518  and accuracy is =  0.6456214689265537\n",
            "At step  80  and at epoch =  52  the loss is =  0.0017674764385446906  and accuracy is =  0.6314971751412429\n",
            "At step  80  and at epoch =  53  the loss is =  0.0011271398980170488  and accuracy is =  0.643361581920904\n",
            "At step  80  and at epoch =  54  the loss is =  0.000974003691226244  and accuracy is =  0.6641242937853107\n",
            "At step  80  and at epoch =  55  the loss is =  0.001189958886243403  and accuracy is =  0.6738700564971751\n",
            "At step  80  and at epoch =  56  the loss is =  0.0013608613517135382  and accuracy is =  0.6679378531073447\n",
            "At step  80  and at epoch =  57  the loss is =  0.002086121588945389  and accuracy is =  0.6601694915254237\n",
            "At step  80  and at epoch =  58  the loss is =  0.0026058887597173452  and accuracy is =  0.623587570621469\n",
            "At step  80  and at epoch =  59  the loss is =  0.0017030340386554599  and accuracy is =  0.6110169491525423\n",
            "At step  80  and at epoch =  60  the loss is =  0.0010525360703468323  and accuracy is =  0.6699152542372881\n",
            "At step  80  and at epoch =  61  the loss is =  0.001128246309235692  and accuracy is =  0.684322033898305\n",
            "At step  80  and at epoch =  62  the loss is =  0.0013740099966526031  and accuracy is =  0.6901129943502825\n",
            "At step  80  and at epoch =  63  the loss is =  0.0011195087572559714  and accuracy is =  0.6685028248587571\n",
            "At step  80  and at epoch =  64  the loss is =  0.0022189836017787457  and accuracy is =  0.6555084745762711\n",
            "At step  80  and at epoch =  65  the loss is =  0.0010325303301215172  and accuracy is =  0.6588983050847458\n",
            "At step  80  and at epoch =  66  the loss is =  0.0011967428727075458  and accuracy is =  0.6672316384180791\n",
            "At step  80  and at epoch =  67  the loss is =  0.0009083283366635442  and accuracy is =  0.6792372881355933\n",
            "At step  80  and at epoch =  68  the loss is =  0.001402302528731525  and accuracy is =  0.678954802259887\n",
            "At step  80  and at epoch =  69  the loss is =  0.0013360176235437393  and accuracy is =  0.6574858757062146\n",
            "task: 80 train accuracy = 0.5352\n",
            "task: 80 test accuracy = 0.06888888888888889\n",
            "col =  [94 63 74 21 35 56 91 96 87 48 68 80 22 37 60 97 51 62 92 76 75 89 23 99\n",
            " 39 66 54 69 84 61 85 24 98 41 73 58 78 77 70 49 65 88 36 93 45 10 90 17\n",
            " 32 59 83 43 53 11 86 19 38 30 40 50 57 81 12 95 25 47 34 52 44 72 46 79\n",
            " 20 28  5 71  8 18 33 15 55 29 64 31 67  7 13 14 42  6 82  2 27 16 26  3\n",
            "  4  1  9  0]\n",
            "col[:10] [94 63 74 21 35 56 91 96 87 48]\n",
            "At step  90  and at epoch =  0  the loss is =  0.0054120635613799095  and accuracy is =  0.07779349363507779\n",
            "At step  90  and at epoch =  1  the loss is =  0.0035064201802015305  and accuracy is =  0.14101838755304102\n",
            "At step  90  and at epoch =  2  the loss is =  0.004467877559363842  and accuracy is =  0.17171145685997172\n",
            "At step  90  and at epoch =  3  the loss is =  0.003391694277524948  and accuracy is =  0.22135785007072137\n",
            "At step  90  and at epoch =  4  the loss is =  0.0029871673323214054  and accuracy is =  0.26294200848656296\n",
            "At step  90  and at epoch =  5  the loss is =  0.0042422316037118435  and accuracy is =  0.2988684582743989\n",
            "At step  90  and at epoch =  6  the loss is =  0.003633828368037939  and accuracy is =  0.33960396039603963\n",
            "At step  90  and at epoch =  7  the loss is =  0.003985471557825804  and accuracy is =  0.36845827439886847\n",
            "At step  90  and at epoch =  8  the loss is =  0.0035809401888400316  and accuracy is =  0.4016973125884017\n",
            "At step  90  and at epoch =  9  the loss is =  0.002763034775853157  and accuracy is =  0.41796322489391796\n",
            "At step  90  and at epoch =  10  the loss is =  0.0025799747090786695  and accuracy is =  0.4500707213578501\n",
            "At step  90  and at epoch =  11  the loss is =  0.0032544774003326893  and accuracy is =  0.47072135785007074\n",
            "At step  90  and at epoch =  12  the loss is =  0.00276037841103971  and accuracy is =  0.4842998585572843\n",
            "At step  90  and at epoch =  13  the loss is =  0.003941016271710396  and accuracy is =  0.5128712871287129\n",
            "At step  90  and at epoch =  14  the loss is =  0.00401679752394557  and accuracy is =  0.5231966053748232\n",
            "At step  90  and at epoch =  15  the loss is =  0.002692478708922863  and accuracy is =  0.5152758132956152\n",
            "At step  90  and at epoch =  16  the loss is =  0.0026916891802102327  and accuracy is =  0.5574257425742575\n",
            "At step  90  and at epoch =  17  the loss is =  0.002491540042683482  and accuracy is =  0.5452616690240453\n",
            "At step  90  and at epoch =  18  the loss is =  0.001967248972505331  and accuracy is =  0.5780763790664781\n",
            "At step  90  and at epoch =  19  the loss is =  0.0021917440462857485  and accuracy is =  0.5715700141442716\n",
            "At step  90  and at epoch =  20  the loss is =  0.002106492407619953  and accuracy is =  0.6107496463932107\n",
            "At step  90  and at epoch =  21  the loss is =  0.0024413226637989283  and accuracy is =  0.636067892503536\n",
            "At step  90  and at epoch =  22  the loss is =  0.002344478853046894  and accuracy is =  0.6298444130127299\n",
            "At step  90  and at epoch =  23  the loss is =  0.002266643103212118  and accuracy is =  0.5885431400282886\n",
            "At step  90  and at epoch =  24  the loss is =  0.0026233994867652655  and accuracy is =  0.5349363507779349\n",
            "At step  90  and at epoch =  25  the loss is =  0.0035657421685755253  and accuracy is =  0.6022630834512023\n",
            "At step  90  and at epoch =  26  the loss is =  0.0016683395951986313  and accuracy is =  0.6141442715700142\n",
            "At step  90  and at epoch =  27  the loss is =  0.0013107542181387544  and accuracy is =  0.6451202263083451\n",
            "At step  90  and at epoch =  28  the loss is =  0.0033170890528708696  and accuracy is =  0.6533239038189533\n",
            "At step  90  and at epoch =  29  the loss is =  0.0009734382620081306  and accuracy is =  0.6451202263083451\n",
            "At step  90  and at epoch =  30  the loss is =  0.0028097331523895264  and accuracy is =  0.6646393210749646\n",
            "At step  90  and at epoch =  31  the loss is =  0.0022837799042463303  and accuracy is =  0.6043847241867044\n",
            "At step  90  and at epoch =  32  the loss is =  0.003294885391369462  and accuracy is =  0.6304101838755304\n",
            "At step  90  and at epoch =  33  the loss is =  0.0017585385357961059  and accuracy is =  0.6472418670438472\n",
            "At step  90  and at epoch =  34  the loss is =  0.001246461644768715  and accuracy is =  0.6152758132956153\n",
            "At step  90  and at epoch =  35  the loss is =  0.0015105137135833502  and accuracy is =  0.6724186704384724\n",
            "At step  90  and at epoch =  36  the loss is =  0.001547635649330914  and accuracy is =  0.6796322489391796\n",
            "At step  90  and at epoch =  37  the loss is =  0.002092834794893861  and accuracy is =  0.6725601131541725\n",
            "At step  90  and at epoch =  38  the loss is =  0.0020752539858222008  and accuracy is =  0.6626591230551626\n",
            "At step  90  and at epoch =  39  the loss is =  0.001477258512750268  and accuracy is =  0.6217821782178218\n",
            "At step  90  and at epoch =  40  the loss is =  0.001749689574353397  and accuracy is =  0.6695898161244696\n",
            "At step  90  and at epoch =  41  the loss is =  0.001387877739034593  and accuracy is =  0.6865629420084866\n",
            "At step  90  and at epoch =  42  the loss is =  0.0014072494814172387  and accuracy is =  0.6809052333804809\n",
            "At step  90  and at epoch =  43  the loss is =  0.0009714083280414343  and accuracy is =  0.6842998585572843\n",
            "At step  90  and at epoch =  44  the loss is =  0.0019163665128871799  and accuracy is =  0.657001414427157\n",
            "At step  90  and at epoch =  45  the loss is =  0.0013611559988930821  and accuracy is =  0.6667609618104667\n",
            "At step  90  and at epoch =  46  the loss is =  0.0009483867906965315  and accuracy is =  0.6878359264497879\n",
            "At step  90  and at epoch =  47  the loss is =  0.0009945809142664075  and accuracy is =  0.6790664780763791\n",
            "At step  90  and at epoch =  48  the loss is =  0.001775359152816236  and accuracy is =  0.6775106082036775\n",
            "At step  90  and at epoch =  49  the loss is =  0.00232515181414783  and accuracy is =  0.676944837340877\n",
            "At step  90  and at epoch =  50  the loss is =  0.001681914203800261  and accuracy is =  0.6760961810466761\n",
            "At step  90  and at epoch =  51  the loss is =  0.0009631471475586295  and accuracy is =  0.6789250353606789\n",
            "At step  90  and at epoch =  52  the loss is =  0.0012723677791655064  and accuracy is =  0.6872701555869872\n",
            "At step  90  and at epoch =  53  the loss is =  0.003425122471526265  and accuracy is =  0.6858557284299859\n",
            "At step  90  and at epoch =  54  the loss is =  0.0017371587455272675  and accuracy is =  0.6714285714285714\n",
            "At step  90  and at epoch =  55  the loss is =  0.0007582865655422211  and accuracy is =  0.6585572842998586\n",
            "At step  90  and at epoch =  56  the loss is =  0.0011495074722915888  and accuracy is =  0.6809052333804809\n",
            "At step  90  and at epoch =  57  the loss is =  0.0009623391670174897  and accuracy is =  0.6820367751060821\n",
            "At step  90  and at epoch =  58  the loss is =  0.0009791195625439286  and accuracy is =  0.697029702970297\n",
            "At step  90  and at epoch =  59  the loss is =  0.0005069316830486059  and accuracy is =  0.7004243281471004\n",
            "At step  90  and at epoch =  60  the loss is =  0.0006614550948143005  and accuracy is =  0.7001414427157001\n",
            "At step  90  and at epoch =  61  the loss is =  0.0010120138758793473  and accuracy is =  0.7038189533239039\n",
            "At step  90  and at epoch =  62  the loss is =  0.0013656176161020994  and accuracy is =  0.6868458274398869\n",
            "At step  90  and at epoch =  63  the loss is =  0.0007949798600748181  and accuracy is =  0.6688826025459689\n",
            "At step  90  and at epoch =  64  the loss is =  0.000956988544203341  and accuracy is =  0.6765205091937765\n",
            "At step  90  and at epoch =  65  the loss is =  0.001200165948830545  and accuracy is =  0.6876944837340877\n",
            "At step  90  and at epoch =  66  the loss is =  0.0018204353982582688  and accuracy is =  0.6666195190947666\n",
            "At step  90  and at epoch =  67  the loss is =  0.0006059378501959145  and accuracy is =  0.6892503536067892\n",
            "At step  90  and at epoch =  68  the loss is =  0.0014129093615338206  and accuracy is =  0.7008486562942009\n",
            "At step  90  and at epoch =  69  the loss is =  0.000659702520351857  and accuracy is =  0.6974540311173975\n",
            "task: 90 train accuracy = 0.7272\n",
            "task: 90 test accuracy = 0.0723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0C6HE4usvUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = [0.745,  0.397, 0.412, 0.292, 0.2038,0.1635, 0.151, 0.107, 0.068, 0.0723 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1zSoSnlBV_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTask(pars_tasks):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  x_tasks =  np.linspace(10, 100, 10)\n",
        "\n",
        "  plt.plot(x_tasks, pars_tasks, label=['Accuracy'])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.title('Accuracy over tasks')\n",
        "  plt.legend(['Accuracy'])\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIlF3OcfBY9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "21f12658-9afd-4b0d-8e7e-e6e2bf72edb9"
      },
      "source": [
        "plotTask(l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}