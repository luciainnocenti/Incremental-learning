{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ICaRLMain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f401c9d257b34412911c0a25b17b9326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e774c622e2d3478ea6dff82ed9a0ae78",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_470ff911dc9748a8ae5d035c2e851cab",
              "IPY_MODEL_1d32a49822e3414ba0d2f6ebd95d7236"
            ]
          }
        },
        "e774c622e2d3478ea6dff82ed9a0ae78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "470ff911dc9748a8ae5d035c2e851cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bac853c46306404f894db5926c7ed396",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_913dc9f3ade64ef59c411814397901d6"
          }
        },
        "1d32a49822e3414ba0d2f6ebd95d7236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8a5097f799bf4d97be3a87acdf564120",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 32837431.87it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_155befd3eee34e08993bf4cc8af241e2"
          }
        },
        "bac853c46306404f894db5926c7ed396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "913dc9f3ade64ef59c411814397901d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a5097f799bf4d97be3a87acdf564120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "155befd3eee34e08993bf4cc8af241e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/improvement_Lucia/ICaRLMain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLMLAOPyrR1K",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LqwZJLUlcYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVa_FlnxrXIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "7f908756-ef5b-49a3-af84-84fac5c48901"
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone -b improvement_Lucia https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 2468 (delta 20), reused 0 (delta 0), pack-reused 2431\u001b[K\n",
            "Receiving objects: 100% (2468/2468), 2.74 MiB | 2.29 MiB/s, done.\n",
            "Resolving deltas: 100% (1582/1582), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXQyHMXzrZ5A",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c24pdNxurdv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "4b17032c-fc48-4c6f-ae28-b1e254a98965"
      },
      "source": [
        "from DatasetCIFAR.data_set import Dataset \n",
        "from DatasetCIFAR.data_set import Subset\n",
        "from DatasetCIFAR import ResNet\n",
        "from DatasetCIFAR import utils\n",
        "from DatasetCIFAR import params\n",
        "from DatasetCIFAR import ICaRLModel\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "random.seed(params.SEED)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2rkcBbIKfUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e6e16947-6e22-4bb2-8fee-a3714909458e"
      },
      "source": [
        "print(params.SEED)\n",
        "print(params.NUM_WORKERS)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "653\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FlAkShyryrf",
        "colab_type": "text"
      },
      "source": [
        "# Define Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP2iR2vl3Wiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transformer = transforms.Compose([transforms.RandomCrop(size = 32, padding=4),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CSNk0NlrvAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "f401c9d257b34412911c0a25b17b9326",
            "e774c622e2d3478ea6dff82ed9a0ae78",
            "470ff911dc9748a8ae5d035c2e851cab",
            "1d32a49822e3414ba0d2f6ebd95d7236",
            "bac853c46306404f894db5926c7ed396",
            "913dc9f3ade64ef59c411814397901d6",
            "8a5097f799bf4d97be3a87acdf564120",
            "155befd3eee34e08993bf4cc8af241e2"
          ]
        },
        "outputId": "5cbe033f-6107-41ca-978f-28af3a780de1"
      },
      "source": [
        "trainDS = Dataset(train=True)\n",
        "testDS = Dataset(train=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f401c9d257b34412911c0a25b17b9326",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3ge3VayryJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_splits = trainDS.splits\n",
        "test_splits = testDS.splits"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzxTlFF_rkfe",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgZtPkiPrmQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ICaRL = ResNet.resnet32(num_classes=100)\n",
        "ICaRL =  ICaRL.to(params.DEVICE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEAKDqKak-nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BIC = ResNet.BiasLayer()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI8EyFmpOikN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exemplars = [None]*100\n",
        "\n",
        "test_indexes =  []\n",
        "accs = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcFjbBGrOMz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1c9777e-9964-4385-9468-8e43a6075968"
      },
      "source": [
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "  train_indexes = trainDS.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes + testDS.__getIndexesGroups__(task)\n",
        "\n",
        "  train_dataset = Subset(trainDS, train_indexes, transform = train_transformer)\n",
        "  test_dataset = Subset(testDS, test_indexes, transform = test_transformer)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE, shuffle=True)\n",
        "  test_loader = DataLoader( test_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE , shuffle=True )\n",
        "  \n",
        "  ICaRL, exemplars = ICaRLModel.incrementalTrain(task, trainDS, ICaRL, exemplars, train_transformer, BIC = BIC)\n",
        "\n",
        "  col = []\n",
        "  for i,x in enumerate( train_splits[ :int(task/10) + 1]) : \n",
        "    v = np.array(x)\n",
        "    col = np.concatenate( (col,v), axis = None)\n",
        "    col = col.astype(int)\n",
        "  mean = None\n",
        "  total = 0.0\n",
        "  running_corrects = 0.0\n",
        "  for img, lbl, _ in train_loader:\n",
        "    img = img.float().to(params.DEVICE)\n",
        "    lbl = lbl.to(params.DEVICE)\n",
        "    outputs = ICaRL(img)\n",
        "    c = np.array(train_splits[int(task/10)]).astype(int)\n",
        "    cut_outputs = np.take_along_axis(outputs.to(params.DEVICE), c[None,:], axis = 1).to(params.DEVICE)\n",
        "    cut_outputs = BIC(cut_outputs)\n",
        "    outputs[:,c] = cut_outputs\n",
        "    \n",
        "    _ , preds = torch.max(outputs.data, 1)\n",
        "    #labels = utils.mapFunction(lbl, col).to(params.DEVICE)\n",
        "    total += len(lbl)\n",
        "    running_corrects += torch.sum(preds == lbl.data).data.item()\n",
        "  accuracy = float(running_corrects/total)\n",
        "  print(f'task: {task}', f'train accuracy = {accuracy}')\n",
        "  accs.append(accuracy)\n",
        "\n",
        "  total = 0.0\n",
        "  running_corrects = 0.0\n",
        "  #tot_preds = []\n",
        "  #tot_lab = []\n",
        "  for img, lbl, _ in test_loader:\n",
        "    img = img.float().to(params.DEVICE)\n",
        "    lbl = lbl.to(params.DEVICE)\n",
        "    outputs = ICaRL(img)\n",
        "    c = np.array(train_splits[int(task/10)]).astype(int)\n",
        "    cut_outputs = np.take_along_axis(outputs.to(params.DEVICE), c[None,:], axis = 1).to(params.DEVICE)\n",
        "    cut_outputs = BIC(cut_outputs)\n",
        "    outputs[:,c] = cut_outputs\n",
        "\n",
        "    _ , preds = torch.max(outputs.data, 1)\n",
        "    \n",
        "    total += len(lbl)\n",
        "    running_corrects += torch.sum(preds == lbl.data).data.item()\n",
        "\n",
        "  accuracy = float(running_corrects/total)\n",
        "  print(f'task: {task}', f'test accuracy = {accuracy}')\n",
        "  #cf = confusion_matrix(tot_lab, tot_preds)\n",
        "  #df_cm = pd.DataFrame(cf, range(task + params.TASK_SIZE), range(task + params.TASK_SIZE))\n",
        "  #sn.set(font_scale = .5) # for label size\n",
        "  #sn.heatmap(df_cm, annot=False)\n",
        "  #plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "task : 0\n",
            "1.0 0.0\n",
            "At step  0  and at epoch =  0  the loss is =  0.023228462785482407  and accuracy is =  0.1836\n",
            "At step  0  and at epoch =  1  the loss is =  0.02203056775033474  and accuracy is =  0.3716\n",
            "At step  0  and at epoch =  2  the loss is =  0.02631252259016037  and accuracy is =  0.441\n",
            "At step  0  and at epoch =  3  the loss is =  0.02978803589940071  and accuracy is =  0.5206\n",
            "At step  0  and at epoch =  4  the loss is =  0.027100667357444763  and accuracy is =  0.5506\n",
            "At step  0  and at epoch =  5  the loss is =  0.0311138816177845  and accuracy is =  0.5634\n",
            "At step  0  and at epoch =  6  the loss is =  0.0290700551122427  and accuracy is =  0.6164\n",
            "At step  0  and at epoch =  7  the loss is =  0.017687682062387466  and accuracy is =  0.629\n",
            "At step  0  and at epoch =  8  the loss is =  0.01771804690361023  and accuracy is =  0.6842\n",
            "At step  0  and at epoch =  9  the loss is =  0.0234511848539114  and accuracy is =  0.6766\n",
            "At step  0  and at epoch =  10  the loss is =  0.029441488906741142  and accuracy is =  0.6784\n",
            "At step  0  and at epoch =  11  the loss is =  0.017928987741470337  and accuracy is =  0.7096\n",
            "At step  0  and at epoch =  12  the loss is =  0.01670301891863346  and accuracy is =  0.7472\n",
            "At step  0  and at epoch =  13  the loss is =  0.018737580627202988  and accuracy is =  0.7216\n",
            "At step  0  and at epoch =  14  the loss is =  0.008405803702771664  and accuracy is =  0.7618\n",
            "At step  0  and at epoch =  15  the loss is =  0.018987953662872314  and accuracy is =  0.7692\n",
            "At step  0  and at epoch =  16  the loss is =  0.024624446406960487  and accuracy is =  0.7704\n",
            "At step  0  and at epoch =  17  the loss is =  0.0025891805998981  and accuracy is =  0.7846\n",
            "At step  0  and at epoch =  18  the loss is =  0.013378722593188286  and accuracy is =  0.8064\n",
            "At step  0  and at epoch =  19  the loss is =  0.018670573830604553  and accuracy is =  0.7994\n",
            "At step  0  and at epoch =  20  the loss is =  0.019360031932592392  and accuracy is =  0.8134\n",
            "At step  0  and at epoch =  21  the loss is =  0.006189336534589529  and accuracy is =  0.811\n",
            "At step  0  and at epoch =  22  the loss is =  0.010649706237018108  and accuracy is =  0.823\n",
            "At step  0  and at epoch =  23  the loss is =  0.017780009657144547  and accuracy is =  0.797\n",
            "At step  0  and at epoch =  24  the loss is =  0.01466367207467556  and accuracy is =  0.8128\n",
            "At step  0  and at epoch =  25  the loss is =  0.030497955158352852  and accuracy is =  0.8384\n",
            "At step  0  and at epoch =  26  the loss is =  0.006355669349431992  and accuracy is =  0.8182\n",
            "At step  0  and at epoch =  27  the loss is =  0.007070484571158886  and accuracy is =  0.846\n",
            "At step  0  and at epoch =  28  the loss is =  0.01896381378173828  and accuracy is =  0.8484\n",
            "At step  0  and at epoch =  29  the loss is =  0.00862998329102993  and accuracy is =  0.8462\n",
            "At step  0  and at epoch =  30  the loss is =  0.014234323985874653  and accuracy is =  0.863\n",
            "At step  0  and at epoch =  31  the loss is =  0.014992005191743374  and accuracy is =  0.853\n",
            "At step  0  and at epoch =  32  the loss is =  0.03052799589931965  and accuracy is =  0.852\n",
            "At step  0  and at epoch =  33  the loss is =  0.010975784622132778  and accuracy is =  0.8476\n",
            "At step  0  and at epoch =  34  the loss is =  0.022908538579940796  and accuracy is =  0.8784\n",
            "At step  0  and at epoch =  35  the loss is =  0.008225888013839722  and accuracy is =  0.8754\n",
            "At step  0  and at epoch =  36  the loss is =  0.02321523055434227  and accuracy is =  0.874\n",
            "At step  0  and at epoch =  37  the loss is =  0.019400164484977722  and accuracy is =  0.8666\n",
            "At step  0  and at epoch =  38  the loss is =  0.011029945686459541  and accuracy is =  0.8644\n",
            "At step  0  and at epoch =  39  the loss is =  0.01185340154916048  and accuracy is =  0.8818\n",
            "At step  0  and at epoch =  40  the loss is =  0.011343325488269329  and accuracy is =  0.884\n",
            "At step  0  and at epoch =  41  the loss is =  0.007032731547951698  and accuracy is =  0.8828\n",
            "At step  0  and at epoch =  42  the loss is =  0.0036753404419869184  and accuracy is =  0.8696\n",
            "At step  0  and at epoch =  43  the loss is =  0.01458580233156681  and accuracy is =  0.8986\n",
            "At step  0  and at epoch =  44  the loss is =  0.014176969416439533  and accuracy is =  0.8938\n",
            "At step  0  and at epoch =  45  the loss is =  0.007737079635262489  and accuracy is =  0.8936\n",
            "At step  0  and at epoch =  46  the loss is =  0.00972051452845335  and accuracy is =  0.901\n",
            "At step  0  and at epoch =  47  the loss is =  0.011576496995985508  and accuracy is =  0.905\n",
            "At step  0  and at epoch =  48  the loss is =  0.006855646148324013  and accuracy is =  0.9128\n",
            "At step  0  and at epoch =  49  the loss is =  0.0004381552862469107  and accuracy is =  0.9406\n",
            "At step  0  and at epoch =  50  the loss is =  0.013551410287618637  and accuracy is =  0.9498\n",
            "At step  0  and at epoch =  51  the loss is =  0.0013811147073283792  and accuracy is =  0.9526\n",
            "At step  0  and at epoch =  52  the loss is =  0.013330893591046333  and accuracy is =  0.9608\n",
            "At step  0  and at epoch =  53  the loss is =  0.005790506489574909  and accuracy is =  0.948\n",
            "At step  0  and at epoch =  54  the loss is =  0.016318975016474724  and accuracy is =  0.9628\n",
            "At step  0  and at epoch =  55  the loss is =  0.0044522411189973354  and accuracy is =  0.9636\n",
            "At step  0  and at epoch =  56  the loss is =  0.00467916252091527  and accuracy is =  0.9596\n",
            "At step  0  and at epoch =  57  the loss is =  0.006490877829492092  and accuracy is =  0.9634\n",
            "At step  0  and at epoch =  58  the loss is =  0.007503540255129337  and accuracy is =  0.9602\n",
            "At step  0  and at epoch =  59  the loss is =  0.005869461689144373  and accuracy is =  0.9682\n",
            "At step  0  and at epoch =  60  the loss is =  0.007167539559304714  and accuracy is =  0.9682\n",
            "At step  0  and at epoch =  61  the loss is =  0.012098993174731731  and accuracy is =  0.9636\n",
            "At step  0  and at epoch =  62  the loss is =  0.005352039821445942  and accuracy is =  0.9668\n",
            "At step  0  and at epoch =  63  the loss is =  0.0026328079402446747  and accuracy is =  0.9764\n",
            "At step  0  and at epoch =  64  the loss is =  0.0008952337084338069  and accuracy is =  0.9748\n",
            "At step  0  and at epoch =  65  the loss is =  0.006155572831630707  and accuracy is =  0.9768\n",
            "At step  0  and at epoch =  66  the loss is =  0.002648299792781472  and accuracy is =  0.9786\n",
            "At step  0  and at epoch =  67  the loss is =  0.01478280033916235  and accuracy is =  0.9764\n",
            "At step  0  and at epoch =  68  the loss is =  0.006286763586103916  and accuracy is =  0.9768\n",
            "At step  0  and at epoch =  69  the loss is =  0.006873596925288439  and accuracy is =  0.979\n",
            "task: 0 train accuracy = 0.9816\n",
            "task: 0 test accuracy = 0.886\n",
            "task : 10\n",
            "0.8303744792938232 -0.2589843273162842\n",
            "At step  10  and at epoch =  0  the loss is =  0.0349431186914444  and accuracy is =  0.2768181818181818\n",
            "At step  10  and at epoch =  1  the loss is =  0.02978263422846794  and accuracy is =  0.36787878787878786\n",
            "At step  10  and at epoch =  2  the loss is =  0.0225273035466671  and accuracy is =  0.46136363636363636\n",
            "At step  10  and at epoch =  3  the loss is =  0.026099229231476784  and accuracy is =  0.5196969696969697\n",
            "At step  10  and at epoch =  4  the loss is =  0.022333359345793724  and accuracy is =  0.5534848484848485\n",
            "At step  10  and at epoch =  5  the loss is =  0.022314399480819702  and accuracy is =  0.58\n",
            "At step  10  and at epoch =  6  the loss is =  0.02647414430975914  and accuracy is =  0.61\n",
            "At step  10  and at epoch =  7  the loss is =  0.02171296440064907  and accuracy is =  0.6253030303030302\n",
            "At step  10  and at epoch =  8  the loss is =  0.02182062901556492  and accuracy is =  0.6481818181818182\n",
            "At step  10  and at epoch =  9  the loss is =  0.021103886887431145  and accuracy is =  0.6515151515151515\n",
            "At step  10  and at epoch =  10  the loss is =  0.020908983424305916  and accuracy is =  0.6757575757575758\n",
            "At step  10  and at epoch =  11  the loss is =  0.022935325279831886  and accuracy is =  0.6843939393939394\n",
            "At step  10  and at epoch =  12  the loss is =  0.02227833680808544  and accuracy is =  0.701969696969697\n",
            "At step  10  and at epoch =  13  the loss is =  0.01988133415579796  and accuracy is =  0.7159090909090909\n",
            "At step  10  and at epoch =  14  the loss is =  0.023697059601545334  and accuracy is =  0.72\n",
            "At step  10  and at epoch =  15  the loss is =  0.023516114801168442  and accuracy is =  0.7215151515151516\n",
            "At step  10  and at epoch =  16  the loss is =  0.02040516771376133  and accuracy is =  0.7310606060606061\n",
            "At step  10  and at epoch =  17  the loss is =  0.022396523505449295  and accuracy is =  0.7425757575757576\n",
            "At step  10  and at epoch =  18  the loss is =  0.023101354017853737  and accuracy is =  0.7460606060606061\n",
            "At step  10  and at epoch =  19  the loss is =  0.020673243328928947  and accuracy is =  0.7451515151515151\n",
            "At step  10  and at epoch =  20  the loss is =  0.023434489965438843  and accuracy is =  0.7477272727272727\n",
            "At step  10  and at epoch =  21  the loss is =  0.019914034754037857  and accuracy is =  0.7601515151515151\n",
            "At step  10  and at epoch =  22  the loss is =  0.02221750095486641  and accuracy is =  0.7693939393939394\n",
            "At step  10  and at epoch =  23  the loss is =  0.015958748757839203  and accuracy is =  0.7712121212121212\n",
            "At step  10  and at epoch =  24  the loss is =  0.02073269709944725  and accuracy is =  0.7839393939393939\n",
            "At step  10  and at epoch =  25  the loss is =  0.021833468228578568  and accuracy is =  0.7710606060606061\n",
            "At step  10  and at epoch =  26  the loss is =  0.018876610323786736  and accuracy is =  0.7798484848484849\n",
            "At step  10  and at epoch =  27  the loss is =  0.02159685455262661  and accuracy is =  0.7865151515151515\n",
            "At step  10  and at epoch =  28  the loss is =  0.01826409623026848  and accuracy is =  0.7943939393939394\n",
            "At step  10  and at epoch =  29  the loss is =  0.02071668952703476  and accuracy is =  0.7990909090909091\n",
            "At step  10  and at epoch =  30  the loss is =  0.01971856877207756  and accuracy is =  0.803939393939394\n",
            "At step  10  and at epoch =  31  the loss is =  0.01868102140724659  and accuracy is =  0.8024242424242424\n",
            "At step  10  and at epoch =  32  the loss is =  0.018991390243172646  and accuracy is =  0.8054545454545454\n",
            "At step  10  and at epoch =  33  the loss is =  0.020801415666937828  and accuracy is =  0.8093939393939394\n",
            "At step  10  and at epoch =  34  the loss is =  0.01849186420440674  and accuracy is =  0.8031818181818182\n",
            "At step  10  and at epoch =  35  the loss is =  0.015477368608117104  and accuracy is =  0.818939393939394\n",
            "At step  10  and at epoch =  36  the loss is =  0.01691559888422489  and accuracy is =  0.8116666666666666\n",
            "At step  10  and at epoch =  37  the loss is =  0.015503831207752228  and accuracy is =  0.8243939393939393\n",
            "At step  10  and at epoch =  38  the loss is =  0.01875382848083973  and accuracy is =  0.818030303030303\n",
            "At step  10  and at epoch =  39  the loss is =  0.01773841865360737  and accuracy is =  0.8192424242424242\n",
            "At step  10  and at epoch =  40  the loss is =  0.016588589176535606  and accuracy is =  0.8193939393939393\n",
            "At step  10  and at epoch =  41  the loss is =  0.01708875223994255  and accuracy is =  0.823030303030303\n",
            "At step  10  and at epoch =  42  the loss is =  0.015679504722356796  and accuracy is =  0.8374242424242424\n",
            "At step  10  and at epoch =  43  the loss is =  0.019707782194018364  and accuracy is =  0.8342424242424242\n",
            "At step  10  and at epoch =  44  the loss is =  0.017849482595920563  and accuracy is =  0.8327272727272728\n",
            "At step  10  and at epoch =  45  the loss is =  0.018431399017572403  and accuracy is =  0.8396969696969697\n",
            "At step  10  and at epoch =  46  the loss is =  0.015437456779181957  and accuracy is =  0.8374242424242424\n",
            "At step  10  and at epoch =  47  the loss is =  0.018688466399908066  and accuracy is =  0.8371212121212122\n",
            "At step  10  and at epoch =  48  the loss is =  0.01842544600367546  and accuracy is =  0.8436363636363636\n",
            "At step  10  and at epoch =  49  the loss is =  0.01423608884215355  and accuracy is =  0.8743939393939394\n",
            "At step  10  and at epoch =  50  the loss is =  0.016828490421175957  and accuracy is =  0.8916666666666667\n",
            "At step  10  and at epoch =  51  the loss is =  0.013781922869384289  and accuracy is =  0.8946969696969697\n",
            "At step  10  and at epoch =  52  the loss is =  0.014009887352585793  and accuracy is =  0.9015151515151515\n",
            "At step  10  and at epoch =  53  the loss is =  0.011539150029420853  and accuracy is =  0.9042424242424243\n",
            "At step  10  and at epoch =  54  the loss is =  0.01239316537976265  and accuracy is =  0.9106060606060606\n",
            "At step  10  and at epoch =  55  the loss is =  0.012470640242099762  and accuracy is =  0.9092424242424243\n",
            "At step  10  and at epoch =  56  the loss is =  0.016165943816304207  and accuracy is =  0.9039393939393939\n",
            "At step  10  and at epoch =  57  the loss is =  0.011626186780631542  and accuracy is =  0.9131818181818182\n",
            "At step  10  and at epoch =  58  the loss is =  0.01201094314455986  and accuracy is =  0.9089393939393939\n",
            "At step  10  and at epoch =  59  the loss is =  0.015956277027726173  and accuracy is =  0.9183333333333333\n",
            "At step  10  and at epoch =  60  the loss is =  0.015570271760225296  and accuracy is =  0.9153030303030303\n",
            "At step  10  and at epoch =  61  the loss is =  0.013752511702477932  and accuracy is =  0.9187878787878788\n",
            "At step  10  and at epoch =  62  the loss is =  0.013788145035505295  and accuracy is =  0.9148484848484848\n",
            "At step  10  and at epoch =  63  the loss is =  0.011016562581062317  and accuracy is =  0.9277272727272727\n",
            "At step  10  and at epoch =  64  the loss is =  0.012141535989940166  and accuracy is =  0.9253030303030303\n",
            "At step  10  and at epoch =  65  the loss is =  0.012811590917408466  and accuracy is =  0.9245454545454546\n",
            "At step  10  and at epoch =  66  the loss is =  0.012196053750813007  and accuracy is =  0.9248484848484848\n",
            "At step  10  and at epoch =  67  the loss is =  0.01000457163900137  and accuracy is =  0.9304545454545454\n",
            "At step  10  and at epoch =  68  the loss is =  0.0123013099655509  and accuracy is =  0.9307575757575758\n",
            "At step  10  and at epoch =  69  the loss is =  0.012787928804755211  and accuracy is =  0.9304545454545454\n",
            "task: 10 train accuracy = 0.9006\n",
            "task: 10 test accuracy = 0.7655\n",
            "task : 20\n",
            "0.8823722004890442 -0.6561416983604431\n",
            "At step  20  and at epoch =  0  the loss is =  0.040686145424842834  and accuracy is =  0.25636363636363635\n",
            "At step  20  and at epoch =  1  the loss is =  0.04149334505200386  and accuracy is =  0.315\n",
            "At step  20  and at epoch =  2  the loss is =  0.03971113637089729  and accuracy is =  0.4\n",
            "At step  20  and at epoch =  3  the loss is =  0.033087316900491714  and accuracy is =  0.463030303030303\n",
            "At step  20  and at epoch =  4  the loss is =  0.032261960208415985  and accuracy is =  0.5134848484848484\n",
            "At step  20  and at epoch =  5  the loss is =  0.03614512085914612  and accuracy is =  0.5528787878787879\n",
            "At step  20  and at epoch =  6  the loss is =  0.033291157335042953  and accuracy is =  0.5687878787878788\n",
            "At step  20  and at epoch =  7  the loss is =  0.03330984711647034  and accuracy is =  0.6127272727272727\n",
            "At step  20  and at epoch =  8  the loss is =  0.03420835733413696  and accuracy is =  0.6233333333333333\n",
            "At step  20  and at epoch =  9  the loss is =  0.03482682257890701  and accuracy is =  0.6448484848484849\n",
            "At step  20  and at epoch =  10  the loss is =  0.03400599956512451  and accuracy is =  0.6498484848484849\n",
            "At step  20  and at epoch =  11  the loss is =  0.03228706866502762  and accuracy is =  0.6633333333333333\n",
            "At step  20  and at epoch =  12  the loss is =  0.030439967289566994  and accuracy is =  0.6692424242424242\n",
            "At step  20  and at epoch =  13  the loss is =  0.032358065247535706  and accuracy is =  0.683030303030303\n",
            "At step  20  and at epoch =  14  the loss is =  0.028214115649461746  and accuracy is =  0.7034848484848485\n",
            "At step  20  and at epoch =  15  the loss is =  0.031221818178892136  and accuracy is =  0.7022727272727273\n",
            "At step  20  and at epoch =  16  the loss is =  0.03437703847885132  and accuracy is =  0.7142424242424242\n",
            "At step  20  and at epoch =  17  the loss is =  0.03359397500753403  and accuracy is =  0.7186363636363636\n",
            "At step  20  and at epoch =  18  the loss is =  0.028747964650392532  and accuracy is =  0.7277272727272728\n",
            "At step  20  and at epoch =  19  the loss is =  0.034528810530900955  and accuracy is =  0.7256060606060606\n",
            "At step  20  and at epoch =  20  the loss is =  0.03255527839064598  and accuracy is =  0.7357575757575757\n",
            "At step  20  and at epoch =  21  the loss is =  0.027973702177405357  and accuracy is =  0.735909090909091\n",
            "At step  20  and at epoch =  22  the loss is =  0.03127444535493851  and accuracy is =  0.7440909090909091\n",
            "At step  20  and at epoch =  23  the loss is =  0.03005295991897583  and accuracy is =  0.7612121212121212\n",
            "At step  20  and at epoch =  24  the loss is =  0.029714001342654228  and accuracy is =  0.764090909090909\n",
            "At step  20  and at epoch =  25  the loss is =  0.03109203465282917  and accuracy is =  0.7748484848484849\n",
            "At step  20  and at epoch =  26  the loss is =  0.02979292906820774  and accuracy is =  0.7722727272727272\n",
            "At step  20  and at epoch =  27  the loss is =  0.02828247845172882  and accuracy is =  0.7668181818181818\n",
            "At step  20  and at epoch =  28  the loss is =  0.028042620047926903  and accuracy is =  0.7812121212121212\n",
            "At step  20  and at epoch =  29  the loss is =  0.026391712948679924  and accuracy is =  0.7842424242424243\n",
            "At step  20  and at epoch =  30  the loss is =  0.02789725549519062  and accuracy is =  0.7854545454545454\n",
            "At step  20  and at epoch =  31  the loss is =  0.029811663553118706  and accuracy is =  0.7893939393939394\n",
            "At step  20  and at epoch =  32  the loss is =  0.030610639601945877  and accuracy is =  0.7954545454545454\n",
            "At step  20  and at epoch =  33  the loss is =  0.029241830110549927  and accuracy is =  0.7895454545454546\n",
            "At step  20  and at epoch =  34  the loss is =  0.02687993459403515  and accuracy is =  0.79\n",
            "At step  20  and at epoch =  35  the loss is =  0.02629784494638443  and accuracy is =  0.798939393939394\n",
            "At step  20  and at epoch =  36  the loss is =  0.027868445962667465  and accuracy is =  0.8101515151515152\n",
            "At step  20  and at epoch =  37  the loss is =  0.028506016358733177  and accuracy is =  0.8042424242424242\n",
            "At step  20  and at epoch =  38  the loss is =  0.02700459584593773  and accuracy is =  0.8075757575757576\n",
            "At step  20  and at epoch =  39  the loss is =  0.027577562257647514  and accuracy is =  0.8045454545454546\n",
            "At step  20  and at epoch =  40  the loss is =  0.027983473613858223  and accuracy is =  0.8128787878787879\n",
            "At step  20  and at epoch =  41  the loss is =  0.028558537364006042  and accuracy is =  0.8174242424242424\n",
            "At step  20  and at epoch =  42  the loss is =  0.026053451001644135  and accuracy is =  0.8098484848484848\n",
            "At step  20  and at epoch =  43  the loss is =  0.03010019287467003  and accuracy is =  0.8151515151515152\n",
            "At step  20  and at epoch =  44  the loss is =  0.028583843261003494  and accuracy is =  0.8003030303030303\n",
            "At step  20  and at epoch =  45  the loss is =  0.03357051685452461  and accuracy is =  0.8209090909090909\n",
            "At step  20  and at epoch =  46  the loss is =  0.02995879016816616  and accuracy is =  0.8257575757575758\n",
            "At step  20  and at epoch =  47  the loss is =  0.028751971200108528  and accuracy is =  0.8175757575757576\n",
            "At step  20  and at epoch =  48  the loss is =  0.028667332604527473  and accuracy is =  0.803030303030303\n",
            "At step  20  and at epoch =  49  the loss is =  0.024258730933070183  and accuracy is =  0.8568181818181818\n",
            "At step  20  and at epoch =  50  the loss is =  0.023246167227625847  and accuracy is =  0.8701515151515151\n",
            "At step  20  and at epoch =  51  the loss is =  0.019884750247001648  and accuracy is =  0.8725757575757576\n",
            "At step  20  and at epoch =  52  the loss is =  0.021552277728915215  and accuracy is =  0.8810606060606061\n",
            "At step  20  and at epoch =  53  the loss is =  0.022967344149947166  and accuracy is =  0.876969696969697\n",
            "At step  20  and at epoch =  54  the loss is =  0.024543695151805878  and accuracy is =  0.8845454545454545\n",
            "At step  20  and at epoch =  55  the loss is =  0.026099836453795433  and accuracy is =  0.8834848484848485\n",
            "At step  20  and at epoch =  56  the loss is =  0.023878583684563637  and accuracy is =  0.8937878787878788\n",
            "At step  20  and at epoch =  57  the loss is =  0.02558310329914093  and accuracy is =  0.8860606060606061\n",
            "At step  20  and at epoch =  58  the loss is =  0.02599063515663147  and accuracy is =  0.8895454545454545\n",
            "At step  20  and at epoch =  59  the loss is =  0.02243613637983799  and accuracy is =  0.8892424242424243\n",
            "At step  20  and at epoch =  60  the loss is =  0.02408014051616192  and accuracy is =  0.8883333333333333\n",
            "At step  20  and at epoch =  61  the loss is =  0.02632482349872589  and accuracy is =  0.8880303030303031\n",
            "At step  20  and at epoch =  62  the loss is =  0.025503506883978844  and accuracy is =  0.8993939393939394\n",
            "At step  20  and at epoch =  63  the loss is =  0.023789143189787865  and accuracy is =  0.8992424242424243\n",
            "At step  20  and at epoch =  64  the loss is =  0.02057570219039917  and accuracy is =  0.8966666666666666\n",
            "At step  20  and at epoch =  65  the loss is =  0.021275537088513374  and accuracy is =  0.8934848484848484\n",
            "At step  20  and at epoch =  66  the loss is =  0.02191862091422081  and accuracy is =  0.9015151515151515\n",
            "At step  20  and at epoch =  67  the loss is =  0.023715408518910408  and accuracy is =  0.8945454545454545\n",
            "At step  20  and at epoch =  68  the loss is =  0.02354712411761284  and accuracy is =  0.8995454545454545\n",
            "At step  20  and at epoch =  69  the loss is =  0.02336489036679268  and accuracy is =  0.9007575757575758\n",
            "task: 20 train accuracy = 0.849\n",
            "task: 20 test accuracy = 0.713\n",
            "task : 30\n",
            "0.8598207235336304 -1.1549206972122192\n",
            "At step  30  and at epoch =  0  the loss is =  0.04594004526734352  and accuracy is =  0.2598496240601504\n",
            "At step  30  and at epoch =  1  the loss is =  0.0413537472486496  and accuracy is =  0.3377443609022556\n",
            "At step  30  and at epoch =  2  the loss is =  0.04242563620209694  and accuracy is =  0.4111278195488722\n",
            "At step  30  and at epoch =  3  the loss is =  0.041190195828676224  and accuracy is =  0.4667669172932331\n",
            "At step  30  and at epoch =  4  the loss is =  0.04256916046142578  and accuracy is =  0.5085714285714286\n",
            "At step  30  and at epoch =  5  the loss is =  0.04165160283446312  and accuracy is =  0.5445112781954887\n",
            "At step  30  and at epoch =  6  the loss is =  0.04265730455517769  and accuracy is =  0.5502255639097744\n",
            "At step  30  and at epoch =  7  the loss is =  0.04177642986178398  and accuracy is =  0.5795488721804511\n",
            "At step  30  and at epoch =  8  the loss is =  0.04110763967037201  and accuracy is =  0.5900751879699249\n",
            "At step  30  and at epoch =  9  the loss is =  0.041264329105615616  and accuracy is =  0.6037593984962406\n",
            "At step  30  and at epoch =  10  the loss is =  0.038154035806655884  and accuracy is =  0.6254135338345864\n",
            "At step  30  and at epoch =  11  the loss is =  0.042419493198394775  and accuracy is =  0.6273684210526316\n",
            "At step  30  and at epoch =  12  the loss is =  0.041621290147304535  and accuracy is =  0.6356390977443609\n",
            "At step  30  and at epoch =  13  the loss is =  0.03886958211660385  and accuracy is =  0.6497744360902256\n",
            "At step  30  and at epoch =  14  the loss is =  0.03782721981406212  and accuracy is =  0.6655639097744361\n",
            "At step  30  and at epoch =  15  the loss is =  0.03945552185177803  and accuracy is =  0.673984962406015\n",
            "At step  30  and at epoch =  16  the loss is =  0.04118669778108597  and accuracy is =  0.6801503759398496\n",
            "At step  30  and at epoch =  17  the loss is =  0.039893556386232376  and accuracy is =  0.6888721804511279\n",
            "At step  30  and at epoch =  18  the loss is =  0.03951193392276764  and accuracy is =  0.7019548872180451\n",
            "At step  30  and at epoch =  19  the loss is =  0.0388515330851078  and accuracy is =  0.6995488721804511\n",
            "At step  30  and at epoch =  20  the loss is =  0.039670299738645554  and accuracy is =  0.6972932330827067\n",
            "At step  30  and at epoch =  21  the loss is =  0.04104061424732208  and accuracy is =  0.708421052631579\n",
            "At step  30  and at epoch =  22  the loss is =  0.03857145085930824  and accuracy is =  0.7263157894736842\n",
            "At step  30  and at epoch =  23  the loss is =  0.03686729446053505  and accuracy is =  0.7154887218045113\n",
            "At step  30  and at epoch =  24  the loss is =  0.03633385896682739  and accuracy is =  0.7287218045112782\n",
            "At step  30  and at epoch =  25  the loss is =  0.03759319707751274  and accuracy is =  0.7224060150375939\n",
            "At step  30  and at epoch =  26  the loss is =  0.03888519108295441  and accuracy is =  0.7353383458646616\n",
            "At step  30  and at epoch =  27  the loss is =  0.036593928933143616  and accuracy is =  0.7419548872180451\n",
            "At step  30  and at epoch =  28  the loss is =  0.038146086037158966  and accuracy is =  0.7362406015037594\n",
            "At step  30  and at epoch =  29  the loss is =  0.03642557933926582  and accuracy is =  0.7436090225563909\n",
            "At step  30  and at epoch =  30  the loss is =  0.03929482772946358  and accuracy is =  0.752781954887218\n",
            "At step  30  and at epoch =  31  the loss is =  0.03678049147129059  and accuracy is =  0.7577443609022556\n",
            "At step  30  and at epoch =  32  the loss is =  0.037609606981277466  and accuracy is =  0.7554887218045113\n",
            "At step  30  and at epoch =  33  the loss is =  0.03728920966386795  and accuracy is =  0.7581954887218045\n",
            "At step  30  and at epoch =  34  the loss is =  0.03646058961749077  and accuracy is =  0.7688721804511278\n",
            "At step  30  and at epoch =  35  the loss is =  0.034413840621709824  and accuracy is =  0.7666165413533834\n",
            "At step  30  and at epoch =  36  the loss is =  0.03760417550802231  and accuracy is =  0.770375939849624\n",
            "At step  30  and at epoch =  37  the loss is =  0.0392322763800621  and accuracy is =  0.7777443609022556\n",
            "At step  30  and at epoch =  38  the loss is =  0.03811069577932358  and accuracy is =  0.7763909774436091\n",
            "At step  30  and at epoch =  39  the loss is =  0.036123886704444885  and accuracy is =  0.775187969924812\n",
            "At step  30  and at epoch =  40  the loss is =  0.036636028438806534  and accuracy is =  0.7887218045112782\n",
            "At step  30  and at epoch =  41  the loss is =  0.034506604075431824  and accuracy is =  0.784812030075188\n",
            "At step  30  and at epoch =  42  the loss is =  0.03567487746477127  and accuracy is =  0.7893233082706766\n",
            "At step  30  and at epoch =  43  the loss is =  0.03396448493003845  and accuracy is =  0.7923308270676692\n",
            "At step  30  and at epoch =  44  the loss is =  0.0352255143225193  and accuracy is =  0.7950375939849624\n",
            "At step  30  and at epoch =  45  the loss is =  0.03551690652966499  and accuracy is =  0.7936842105263158\n",
            "At step  30  and at epoch =  46  the loss is =  0.03721548244357109  and accuracy is =  0.7933834586466165\n",
            "At step  30  and at epoch =  47  the loss is =  0.037668291479349136  and accuracy is =  0.7941353383458647\n",
            "At step  30  and at epoch =  48  the loss is =  0.03492782264947891  and accuracy is =  0.7957894736842105\n",
            "At step  30  and at epoch =  49  the loss is =  0.031205449253320694  and accuracy is =  0.8359398496240602\n",
            "At step  30  and at epoch =  50  the loss is =  0.029978463426232338  and accuracy is =  0.8494736842105263\n",
            "At step  30  and at epoch =  51  the loss is =  0.03222158923745155  and accuracy is =  0.8547368421052631\n",
            "At step  30  and at epoch =  52  the loss is =  0.033502526581287384  and accuracy is =  0.8560902255639098\n",
            "At step  30  and at epoch =  53  the loss is =  0.03212571144104004  and accuracy is =  0.855187969924812\n",
            "At step  30  and at epoch =  54  the loss is =  0.03125143423676491  and accuracy is =  0.86\n",
            "At step  30  and at epoch =  55  the loss is =  0.0316058024764061  and accuracy is =  0.8622556390977444\n",
            "At step  30  and at epoch =  56  the loss is =  0.035692114382982254  and accuracy is =  0.8625563909774436\n",
            "At step  30  and at epoch =  57  the loss is =  0.03259611874818802  and accuracy is =  0.8678195488721805\n",
            "At step  30  and at epoch =  58  the loss is =  0.03142645210027695  and accuracy is =  0.8654135338345864\n",
            "At step  30  and at epoch =  59  the loss is =  0.03146115317940712  and accuracy is =  0.8721804511278195\n",
            "At step  30  and at epoch =  60  the loss is =  0.031773891299963  and accuracy is =  0.8718796992481203\n",
            "At step  30  and at epoch =  61  the loss is =  0.033117666840553284  and accuracy is =  0.8666165413533835\n",
            "At step  30  and at epoch =  62  the loss is =  0.032303355634212494  and accuracy is =  0.870375939849624\n",
            "At step  30  and at epoch =  63  the loss is =  0.03534236550331116  and accuracy is =  0.8804511278195488\n",
            "At step  30  and at epoch =  64  the loss is =  0.03107035718858242  and accuracy is =  0.8806015037593985\n",
            "At step  30  and at epoch =  65  the loss is =  0.03158850595355034  and accuracy is =  0.8732330827067669\n",
            "At step  30  and at epoch =  66  the loss is =  0.03159563988447189  and accuracy is =  0.8825563909774436\n",
            "At step  30  and at epoch =  67  the loss is =  0.03274570032954216  and accuracy is =  0.8813533834586467\n",
            "At step  30  and at epoch =  68  the loss is =  0.0317666120827198  and accuracy is =  0.8751879699248121\n",
            "At step  30  and at epoch =  69  the loss is =  0.0315612331032753  and accuracy is =  0.8789473684210526\n",
            "task: 30 train accuracy = 0.7618\n",
            "task: 30 test accuracy = 0.64825\n",
            "task : 40\n",
            "0.9908292293548584 -1.4947891235351562\n",
            "At step  40  and at epoch =  0  the loss is =  0.05952459201216698  and accuracy is =  0.24439393939393939\n",
            "At step  40  and at epoch =  1  the loss is =  0.05795074626803398  and accuracy is =  0.30757575757575756\n",
            "At step  40  and at epoch =  2  the loss is =  0.06322187930345535  and accuracy is =  0.35348484848484846\n",
            "At step  40  and at epoch =  3  the loss is =  0.06220158934593201  and accuracy is =  0.3978787878787879\n",
            "At step  40  and at epoch =  4  the loss is =  0.05510064959526062  and accuracy is =  0.43636363636363634\n",
            "At step  40  and at epoch =  5  the loss is =  0.05762460455298424  and accuracy is =  0.4596969696969697\n",
            "At step  40  and at epoch =  6  the loss is =  0.06145811080932617  and accuracy is =  0.48424242424242425\n",
            "At step  40  and at epoch =  7  the loss is =  0.06002055108547211  and accuracy is =  0.503030303030303\n",
            "At step  40  and at epoch =  8  the loss is =  0.04996388778090477  and accuracy is =  0.5266666666666666\n",
            "At step  40  and at epoch =  9  the loss is =  0.05398247018456459  and accuracy is =  0.5368181818181819\n",
            "At step  40  and at epoch =  10  the loss is =  0.055120471864938736  and accuracy is =  0.5483333333333333\n",
            "At step  40  and at epoch =  11  the loss is =  0.057246915996074677  and accuracy is =  0.5716666666666667\n",
            "At step  40  and at epoch =  12  the loss is =  0.05422939360141754  and accuracy is =  0.5857575757575758\n",
            "At step  40  and at epoch =  13  the loss is =  0.05530824512243271  and accuracy is =  0.6006060606060606\n",
            "At step  40  and at epoch =  14  the loss is =  0.05657386779785156  and accuracy is =  0.6042424242424242\n",
            "At step  40  and at epoch =  15  the loss is =  0.051557984203100204  and accuracy is =  0.6253030303030302\n",
            "At step  40  and at epoch =  16  the loss is =  0.056984614580869675  and accuracy is =  0.6322727272727273\n",
            "At step  40  and at epoch =  17  the loss is =  0.04859597980976105  and accuracy is =  0.6327272727272727\n",
            "At step  40  and at epoch =  18  the loss is =  0.05237237364053726  and accuracy is =  0.64\n",
            "At step  40  and at epoch =  19  the loss is =  0.050162333995103836  and accuracy is =  0.6584848484848485\n",
            "At step  40  and at epoch =  20  the loss is =  0.05005998536944389  and accuracy is =  0.6581818181818182\n",
            "At step  40  and at epoch =  21  the loss is =  0.05337657779455185  and accuracy is =  0.666969696969697\n",
            "At step  40  and at epoch =  22  the loss is =  0.0513349324464798  and accuracy is =  0.6793939393939394\n",
            "At step  40  and at epoch =  23  the loss is =  0.05131516978144646  and accuracy is =  0.6822727272727273\n",
            "At step  40  and at epoch =  24  the loss is =  0.051787737756967545  and accuracy is =  0.6854545454545454\n",
            "At step  40  and at epoch =  25  the loss is =  0.056211940944194794  and accuracy is =  0.6906060606060606\n",
            "At step  40  and at epoch =  26  the loss is =  0.04882727935910225  and accuracy is =  0.7006060606060606\n",
            "At step  40  and at epoch =  27  the loss is =  0.04902860149741173  and accuracy is =  0.7\n",
            "At step  40  and at epoch =  28  the loss is =  0.051892176270484924  and accuracy is =  0.713939393939394\n",
            "At step  40  and at epoch =  29  the loss is =  0.05073694884777069  and accuracy is =  0.7110606060606061\n",
            "At step  40  and at epoch =  30  the loss is =  0.051919907331466675  and accuracy is =  0.7159090909090909\n",
            "At step  40  and at epoch =  31  the loss is =  0.052217524498701096  and accuracy is =  0.7196969696969697\n",
            "At step  40  and at epoch =  32  the loss is =  0.05484569072723389  and accuracy is =  0.7281818181818182\n",
            "At step  40  and at epoch =  33  the loss is =  0.05312544107437134  and accuracy is =  0.7142424242424242\n",
            "At step  40  and at epoch =  34  the loss is =  0.04788263887166977  and accuracy is =  0.7337878787878788\n",
            "At step  40  and at epoch =  35  the loss is =  0.053706616163253784  and accuracy is =  0.7375757575757576\n",
            "At step  40  and at epoch =  36  the loss is =  0.052724916487932205  and accuracy is =  0.7268181818181818\n",
            "At step  40  and at epoch =  37  the loss is =  0.050440192222595215  and accuracy is =  0.7337878787878788\n",
            "At step  40  and at epoch =  38  the loss is =  0.05404644459486008  and accuracy is =  0.7466666666666667\n",
            "At step  40  and at epoch =  39  the loss is =  0.04851222783327103  and accuracy is =  0.7456060606060606\n",
            "At step  40  and at epoch =  40  the loss is =  0.050398483872413635  and accuracy is =  0.7575757575757576\n",
            "At step  40  and at epoch =  41  the loss is =  0.04911688342690468  and accuracy is =  0.7562121212121212\n",
            "At step  40  and at epoch =  42  the loss is =  0.05225067213177681  and accuracy is =  0.7603030303030303\n",
            "At step  40  and at epoch =  43  the loss is =  0.04984869435429573  and accuracy is =  0.7601515151515151\n",
            "At step  40  and at epoch =  44  the loss is =  0.05507425591349602  and accuracy is =  0.759090909090909\n",
            "At step  40  and at epoch =  45  the loss is =  0.049472250044345856  and accuracy is =  0.7610606060606061\n",
            "At step  40  and at epoch =  46  the loss is =  0.05027596652507782  and accuracy is =  0.7698484848484849\n",
            "At step  40  and at epoch =  47  the loss is =  0.052859336137771606  and accuracy is =  0.7654545454545455\n",
            "At step  40  and at epoch =  48  the loss is =  0.049313534051179886  and accuracy is =  0.7646969696969697\n",
            "At step  40  and at epoch =  49  the loss is =  0.042500339448451996  and accuracy is =  0.8025757575757576\n",
            "At step  40  and at epoch =  50  the loss is =  0.04769571125507355  and accuracy is =  0.8265151515151515\n",
            "At step  40  and at epoch =  51  the loss is =  0.04143417999148369  and accuracy is =  0.8433333333333334\n",
            "At step  40  and at epoch =  52  the loss is =  0.045529287308454514  and accuracy is =  0.8342424242424242\n",
            "At step  40  and at epoch =  53  the loss is =  0.05015111342072487  and accuracy is =  0.8443939393939394\n",
            "At step  40  and at epoch =  54  the loss is =  0.04283083975315094  and accuracy is =  0.8431818181818181\n",
            "At step  40  and at epoch =  55  the loss is =  0.04722724109888077  and accuracy is =  0.845\n",
            "At step  40  and at epoch =  56  the loss is =  0.045514583587646484  and accuracy is =  0.8537878787878788\n",
            "At step  40  and at epoch =  57  the loss is =  0.048714008182287216  and accuracy is =  0.8571212121212122\n",
            "At step  40  and at epoch =  58  the loss is =  0.04513798654079437  and accuracy is =  0.8472727272727273\n",
            "At step  40  and at epoch =  59  the loss is =  0.04276632145047188  and accuracy is =  0.8575757575757575\n",
            "At step  40  and at epoch =  60  the loss is =  0.04286174476146698  and accuracy is =  0.8571212121212122\n",
            "At step  40  and at epoch =  61  the loss is =  0.04292687028646469  and accuracy is =  0.8586363636363636\n",
            "At step  40  and at epoch =  62  the loss is =  0.04189412295818329  and accuracy is =  0.8603030303030303\n",
            "At step  40  and at epoch =  63  the loss is =  0.043086566030979156  and accuracy is =  0.8683333333333333\n",
            "At step  40  and at epoch =  64  the loss is =  0.04145008325576782  and accuracy is =  0.866969696969697\n",
            "At step  40  and at epoch =  65  the loss is =  0.04229011759161949  and accuracy is =  0.871969696969697\n",
            "At step  40  and at epoch =  66  the loss is =  0.04174434393644333  and accuracy is =  0.87\n",
            "At step  40  and at epoch =  67  the loss is =  0.04483426362276077  and accuracy is =  0.8678787878787879\n",
            "At step  40  and at epoch =  68  the loss is =  0.04127437248826027  and accuracy is =  0.8686363636363637\n",
            "At step  40  and at epoch =  69  the loss is =  0.04396456480026245  and accuracy is =  0.8695454545454545\n",
            "task: 40 train accuracy = 0.7914\n",
            "task: 40 test accuracy = 0.59\n",
            "task : 50\n",
            "0.8942753076553345 -2.0412166118621826\n",
            "At step  50  and at epoch =  0  the loss is =  0.07133876532316208  and accuracy is =  0.245\n",
            "At step  50  and at epoch =  1  the loss is =  0.0708327442407608  and accuracy is =  0.3259090909090909\n",
            "At step  50  and at epoch =  2  the loss is =  0.07318828999996185  and accuracy is =  0.39621212121212124\n",
            "At step  50  and at epoch =  3  the loss is =  0.0673937126994133  and accuracy is =  0.4312121212121212\n",
            "At step  50  and at epoch =  4  the loss is =  0.06605733186006546  and accuracy is =  0.46454545454545454\n",
            "At step  50  and at epoch =  5  the loss is =  0.06868509203195572  and accuracy is =  0.48878787878787877\n",
            "At step  50  and at epoch =  6  the loss is =  0.06320440769195557  and accuracy is =  0.5125757575757576\n",
            "At step  50  and at epoch =  7  the loss is =  0.0626201257109642  and accuracy is =  0.5348484848484848\n",
            "At step  50  and at epoch =  8  the loss is =  0.06431975960731506  and accuracy is =  0.558939393939394\n",
            "At step  50  and at epoch =  9  the loss is =  0.06337908655405045  and accuracy is =  0.5722727272727273\n",
            "At step  50  and at epoch =  10  the loss is =  0.05979268625378609  and accuracy is =  0.5854545454545454\n",
            "At step  50  and at epoch =  11  the loss is =  0.05826234072446823  and accuracy is =  0.600909090909091\n",
            "At step  50  and at epoch =  12  the loss is =  0.06156327575445175  and accuracy is =  0.5998484848484849\n",
            "At step  50  and at epoch =  13  the loss is =  0.062498535960912704  and accuracy is =  0.613030303030303\n",
            "At step  50  and at epoch =  14  the loss is =  0.06510501354932785  and accuracy is =  0.639090909090909\n",
            "At step  50  and at epoch =  15  the loss is =  0.06259644776582718  and accuracy is =  0.6309090909090909\n",
            "At step  50  and at epoch =  16  the loss is =  0.06249294802546501  and accuracy is =  0.6501515151515151\n",
            "At step  50  and at epoch =  17  the loss is =  0.06430532783269882  and accuracy is =  0.6642424242424242\n",
            "At step  50  and at epoch =  18  the loss is =  0.06448442488908768  and accuracy is =  0.6704545454545454\n",
            "At step  50  and at epoch =  19  the loss is =  0.06407755613327026  and accuracy is =  0.6683333333333333\n",
            "At step  50  and at epoch =  20  the loss is =  0.06429417431354523  and accuracy is =  0.6804545454545454\n",
            "At step  50  and at epoch =  21  the loss is =  0.064206063747406  and accuracy is =  0.6843939393939394\n",
            "At step  50  and at epoch =  22  the loss is =  0.06367436796426773  and accuracy is =  0.6904545454545454\n",
            "At step  50  and at epoch =  23  the loss is =  0.06548798084259033  and accuracy is =  0.6957575757575758\n",
            "At step  50  and at epoch =  24  the loss is =  0.06351808458566666  and accuracy is =  0.7083333333333334\n",
            "At step  50  and at epoch =  25  the loss is =  0.061983466148376465  and accuracy is =  0.7065151515151515\n",
            "At step  50  and at epoch =  26  the loss is =  0.06283164024353027  and accuracy is =  0.7110606060606061\n",
            "At step  50  and at epoch =  27  the loss is =  0.06502209603786469  and accuracy is =  0.7110606060606061\n",
            "At step  50  and at epoch =  28  the loss is =  0.06061983108520508  and accuracy is =  0.7203030303030303\n",
            "At step  50  and at epoch =  29  the loss is =  0.0611736923456192  and accuracy is =  0.7159090909090909\n",
            "At step  50  and at epoch =  30  the loss is =  0.0600895956158638  and accuracy is =  0.7221212121212122\n",
            "At step  50  and at epoch =  31  the loss is =  0.05802381783723831  and accuracy is =  0.7286363636363636\n",
            "At step  50  and at epoch =  32  the loss is =  0.06134479120373726  and accuracy is =  0.7315151515151516\n",
            "At step  50  and at epoch =  33  the loss is =  0.06603643298149109  and accuracy is =  0.7339393939393939\n",
            "At step  50  and at epoch =  34  the loss is =  0.06001927703619003  and accuracy is =  0.7336363636363636\n",
            "At step  50  and at epoch =  35  the loss is =  0.05999093875288963  and accuracy is =  0.7346969696969697\n",
            "At step  50  and at epoch =  36  the loss is =  0.06292899698019028  and accuracy is =  0.7474242424242424\n",
            "At step  50  and at epoch =  37  the loss is =  0.056419894099235535  and accuracy is =  0.7478787878787879\n",
            "At step  50  and at epoch =  38  the loss is =  0.06267186254262924  and accuracy is =  0.7496969696969698\n",
            "At step  50  and at epoch =  39  the loss is =  0.05764032155275345  and accuracy is =  0.7636363636363637\n",
            "At step  50  and at epoch =  40  the loss is =  0.06148651987314224  and accuracy is =  0.7596969696969696\n",
            "At step  50  and at epoch =  41  the loss is =  0.05877457931637764  and accuracy is =  0.7559090909090909\n",
            "At step  50  and at epoch =  42  the loss is =  0.06242906674742699  and accuracy is =  0.7692424242424243\n",
            "At step  50  and at epoch =  43  the loss is =  0.059606198221445084  and accuracy is =  0.7686363636363637\n",
            "At step  50  and at epoch =  44  the loss is =  0.06255942583084106  and accuracy is =  0.7607575757575757\n",
            "At step  50  and at epoch =  45  the loss is =  0.06245458871126175  and accuracy is =  0.7618181818181818\n",
            "At step  50  and at epoch =  46  the loss is =  0.05981413647532463  and accuracy is =  0.7706060606060606\n",
            "At step  50  and at epoch =  47  the loss is =  0.06449029594659805  and accuracy is =  0.7765151515151515\n",
            "At step  50  and at epoch =  48  the loss is =  0.060494113713502884  and accuracy is =  0.7806060606060606\n",
            "At step  50  and at epoch =  49  the loss is =  0.05353858321905136  and accuracy is =  0.81\n",
            "At step  50  and at epoch =  50  the loss is =  0.05633910372853279  and accuracy is =  0.8342424242424242\n",
            "At step  50  and at epoch =  51  the loss is =  0.05451801419258118  and accuracy is =  0.8374242424242424\n",
            "At step  50  and at epoch =  52  the loss is =  0.057582560926675797  and accuracy is =  0.8462121212121212\n",
            "At step  50  and at epoch =  53  the loss is =  0.05679882690310478  and accuracy is =  0.8431818181818181\n",
            "At step  50  and at epoch =  54  the loss is =  0.057038627564907074  and accuracy is =  0.8503030303030303\n",
            "At step  50  and at epoch =  55  the loss is =  0.05395224317908287  and accuracy is =  0.8513636363636363\n",
            "At step  50  and at epoch =  56  the loss is =  0.05761703476309776  and accuracy is =  0.8459090909090909\n",
            "At step  50  and at epoch =  57  the loss is =  0.05488794296979904  and accuracy is =  0.8533333333333334\n",
            "At step  50  and at epoch =  58  the loss is =  0.051211122423410416  and accuracy is =  0.8498484848484849\n",
            "At step  50  and at epoch =  59  the loss is =  0.05389301851391792  and accuracy is =  0.8562121212121212\n",
            "At step  50  and at epoch =  60  the loss is =  0.055532101541757584  and accuracy is =  0.8531818181818182\n",
            "At step  50  and at epoch =  61  the loss is =  0.055116668343544006  and accuracy is =  0.8512121212121212\n",
            "At step  50  and at epoch =  62  the loss is =  0.059752512723207474  and accuracy is =  0.8506060606060606\n",
            "At step  50  and at epoch =  63  the loss is =  0.057121939957141876  and accuracy is =  0.8633333333333333\n",
            "At step  50  and at epoch =  64  the loss is =  0.050785016268491745  and accuracy is =  0.8633333333333333\n",
            "At step  50  and at epoch =  65  the loss is =  0.0518389455974102  and accuracy is =  0.8590909090909091\n",
            "At step  50  and at epoch =  66  the loss is =  0.050177328288555145  and accuracy is =  0.866969696969697\n",
            "At step  50  and at epoch =  67  the loss is =  0.054691437631845474  and accuracy is =  0.8616666666666667\n",
            "At step  50  and at epoch =  68  the loss is =  0.053895942866802216  and accuracy is =  0.8596969696969697\n",
            "At step  50  and at epoch =  69  the loss is =  0.05403640866279602  and accuracy is =  0.8574242424242424\n",
            "task: 50 train accuracy = 0.691\n",
            "task: 50 test accuracy = 0.5313333333333333\n",
            "task : 60\n",
            "1.0056061744689941 -2.3915185928344727\n",
            "At step  60  and at epoch =  0  the loss is =  0.07153243571519852  and accuracy is =  0.21978851963746224\n",
            "At step  60  and at epoch =  1  the loss is =  0.07158113270998001  and accuracy is =  0.2916918429003021\n",
            "At step  60  and at epoch =  2  the loss is =  0.07672230899333954  and accuracy is =  0.345619335347432\n",
            "At step  60  and at epoch =  3  the loss is =  0.07462330907583237  and accuracy is =  0.37930513595166165\n",
            "At step  60  and at epoch =  4  the loss is =  0.07133937627077103  and accuracy is =  0.4080060422960725\n",
            "At step  60  and at epoch =  5  the loss is =  0.07247667759656906  and accuracy is =  0.433987915407855\n",
            "At step  60  and at epoch =  6  the loss is =  0.06970261037349701  and accuracy is =  0.4536253776435045\n",
            "At step  60  and at epoch =  7  the loss is =  0.07260124385356903  and accuracy is =  0.4651057401812689\n",
            "At step  60  and at epoch =  8  the loss is =  0.07178760319948196  and accuracy is =  0.48474320241691843\n",
            "At step  60  and at epoch =  9  the loss is =  0.0736260861158371  and accuracy is =  0.49758308157099695\n",
            "At step  60  and at epoch =  10  the loss is =  0.06903707981109619  and accuracy is =  0.5099697885196375\n",
            "At step  60  and at epoch =  11  the loss is =  0.07939351350069046  and accuracy is =  0.5309667673716012\n",
            "At step  60  and at epoch =  12  the loss is =  0.0752909928560257  and accuracy is =  0.5359516616314199\n",
            "At step  60  and at epoch =  13  the loss is =  0.07104451209306717  and accuracy is =  0.5457703927492447\n",
            "At step  60  and at epoch =  14  the loss is =  0.0766989141702652  and accuracy is =  0.5566465256797583\n",
            "At step  60  and at epoch =  15  the loss is =  0.07872267812490463  and accuracy is =  0.5663141993957704\n",
            "At step  60  and at epoch =  16  the loss is =  0.06886535882949829  and accuracy is =  0.5761329305135952\n",
            "At step  60  and at epoch =  17  the loss is =  0.06909652054309845  and accuracy is =  0.5832326283987915\n",
            "At step  60  and at epoch =  18  the loss is =  0.07028937339782715  and accuracy is =  0.5895770392749244\n",
            "At step  60  and at epoch =  19  the loss is =  0.06622591614723206  and accuracy is =  0.5966767371601208\n",
            "At step  60  and at epoch =  20  the loss is =  0.07520569115877151  and accuracy is =  0.6066465256797583\n",
            "At step  60  and at epoch =  21  the loss is =  0.0639679878950119  and accuracy is =  0.6134441087613293\n",
            "At step  60  and at epoch =  22  the loss is =  0.073935367166996  and accuracy is =  0.6200906344410876\n",
            "At step  60  and at epoch =  23  the loss is =  0.06628543883562088  and accuracy is =  0.6173716012084592\n",
            "At step  60  and at epoch =  24  the loss is =  0.06965363770723343  and accuracy is =  0.6253776435045317\n",
            "At step  60  and at epoch =  25  the loss is =  0.06838317960500717  and accuracy is =  0.6359516616314199\n",
            "At step  60  and at epoch =  26  the loss is =  0.0689982995390892  and accuracy is =  0.638821752265861\n",
            "At step  60  and at epoch =  27  the loss is =  0.07280020415782928  and accuracy is =  0.6439577039274924\n",
            "At step  60  and at epoch =  28  the loss is =  0.07138528674840927  and accuracy is =  0.6459214501510574\n",
            "At step  60  and at epoch =  29  the loss is =  0.07463594526052475  and accuracy is =  0.6540785498489426\n",
            "At step  60  and at epoch =  30  the loss is =  0.06535458564758301  and accuracy is =  0.658761329305136\n",
            "At step  60  and at epoch =  31  the loss is =  0.06998606771230698  and accuracy is =  0.6539274924471299\n",
            "At step  60  and at epoch =  32  the loss is =  0.07012506574392319  and accuracy is =  0.6626888217522658\n",
            "At step  60  and at epoch =  33  the loss is =  0.07127731293439865  and accuracy is =  0.6676737160120846\n",
            "At step  60  and at epoch =  34  the loss is =  0.0677950531244278  and accuracy is =  0.6756797583081571\n",
            "At step  60  and at epoch =  35  the loss is =  0.0689726248383522  and accuracy is =  0.6729607250755287\n",
            "At step  60  and at epoch =  36  the loss is =  0.07034372538328171  and accuracy is =  0.6740181268882175\n",
            "At step  60  and at epoch =  37  the loss is =  0.0667600929737091  and accuracy is =  0.6782477341389728\n",
            "At step  60  and at epoch =  38  the loss is =  0.06449813395738602  and accuracy is =  0.6800604229607251\n",
            "At step  60  and at epoch =  39  the loss is =  0.0729035884141922  and accuracy is =  0.6785498489425982\n",
            "At step  60  and at epoch =  40  the loss is =  0.06887081265449524  and accuracy is =  0.677190332326284\n",
            "At step  60  and at epoch =  41  the loss is =  0.06416045129299164  and accuracy is =  0.6918429003021148\n",
            "At step  60  and at epoch =  42  the loss is =  0.07187698036432266  and accuracy is =  0.68595166163142\n",
            "At step  60  and at epoch =  43  the loss is =  0.07212961465120316  and accuracy is =  0.6939577039274925\n",
            "At step  60  and at epoch =  44  the loss is =  0.06628519296646118  and accuracy is =  0.7\n",
            "At step  60  and at epoch =  45  the loss is =  0.06939855217933655  and accuracy is =  0.6993957703927492\n",
            "At step  60  and at epoch =  46  the loss is =  0.07029470801353455  and accuracy is =  0.6944108761329305\n",
            "At step  60  and at epoch =  47  the loss is =  0.0670672059059143  and accuracy is =  0.7060422960725076\n",
            "At step  60  and at epoch =  48  the loss is =  0.07428624480962753  and accuracy is =  0.7074018126888217\n",
            "At step  60  and at epoch =  49  the loss is =  0.06988058239221573  and accuracy is =  0.7415407854984895\n",
            "At step  60  and at epoch =  50  the loss is =  0.059636279940605164  and accuracy is =  0.7716012084592145\n",
            "At step  60  and at epoch =  51  the loss is =  0.06600788235664368  and accuracy is =  0.7728096676737161\n",
            "At step  60  and at epoch =  52  the loss is =  0.06064392253756523  and accuracy is =  0.777190332326284\n",
            "At step  60  and at epoch =  53  the loss is =  0.06356941163539886  and accuracy is =  0.7820241691842901\n",
            "At step  60  and at epoch =  54  the loss is =  0.06605525314807892  and accuracy is =  0.7867069486404834\n",
            "At step  60  and at epoch =  55  the loss is =  0.06580878049135208  and accuracy is =  0.7845921450151058\n",
            "At step  60  and at epoch =  56  the loss is =  0.059291303157806396  and accuracy is =  0.7950151057401813\n",
            "At step  60  and at epoch =  57  the loss is =  0.06713572889566422  and accuracy is =  0.7992447129909366\n",
            "At step  60  and at epoch =  58  the loss is =  0.06173284351825714  and accuracy is =  0.7933534743202417\n",
            "At step  60  and at epoch =  59  the loss is =  0.06117672100663185  and accuracy is =  0.802870090634441\n",
            "At step  60  and at epoch =  60  the loss is =  0.06517382711172104  and accuracy is =  0.7929003021148037\n",
            "At step  60  and at epoch =  61  the loss is =  0.06246860697865486  and accuracy is =  0.797583081570997\n",
            "At step  60  and at epoch =  62  the loss is =  0.061679769307374954  and accuracy is =  0.804380664652568\n",
            "At step  60  and at epoch =  63  the loss is =  0.06196761503815651  and accuracy is =  0.8010574018126888\n",
            "At step  60  and at epoch =  64  the loss is =  0.06409308314323425  and accuracy is =  0.8096676737160121\n",
            "At step  60  and at epoch =  65  the loss is =  0.05802392214536667  and accuracy is =  0.8099697885196374\n",
            "At step  60  and at epoch =  66  the loss is =  0.05812695249915123  and accuracy is =  0.8148036253776435\n",
            "At step  60  and at epoch =  67  the loss is =  0.06554108113050461  and accuracy is =  0.8117824773413898\n",
            "At step  60  and at epoch =  68  the loss is =  0.0626044049859047  and accuracy is =  0.8125377643504532\n",
            "At step  60  and at epoch =  69  the loss is =  0.05972836911678314  and accuracy is =  0.8126888217522659\n",
            "task: 60 train accuracy = 0.6188\n",
            "task: 60 test accuracy = 0.48214285714285715\n",
            "task : 70\n",
            "1.0963696241378784 -2.7117116451263428\n",
            "At step  70  and at epoch =  0  the loss is =  0.0891605019569397  and accuracy is =  0.24444444444444444\n",
            "At step  70  and at epoch =  1  the loss is =  0.0881151482462883  and accuracy is =  0.37037037037037035\n",
            "At step  70  and at epoch =  2  the loss is =  0.08484535664319992  and accuracy is =  0.4223703703703704\n",
            "At step  70  and at epoch =  3  the loss is =  0.08315854519605637  and accuracy is =  0.4697777777777778\n",
            "At step  70  and at epoch =  4  the loss is =  0.08890540897846222  and accuracy is =  0.5085925925925926\n",
            "At step  70  and at epoch =  5  the loss is =  0.083272285759449  and accuracy is =  0.5306666666666666\n",
            "At step  70  and at epoch =  6  the loss is =  0.08035258203744888  and accuracy is =  0.5502222222222222\n",
            "At step  70  and at epoch =  7  the loss is =  0.08275693655014038  and accuracy is =  0.5703703703703704\n",
            "At step  70  and at epoch =  8  the loss is =  0.08224392682313919  and accuracy is =  0.5949629629629629\n",
            "At step  70  and at epoch =  9  the loss is =  0.08243434131145477  and accuracy is =  0.5971851851851852\n",
            "At step  70  and at epoch =  10  the loss is =  0.0828891173005104  and accuracy is =  0.6194074074074074\n",
            "At step  70  and at epoch =  11  the loss is =  0.08057703822851181  and accuracy is =  0.6373333333333333\n",
            "At step  70  and at epoch =  12  the loss is =  0.07747823745012283  and accuracy is =  0.6405925925925926\n",
            "At step  70  and at epoch =  13  the loss is =  0.07946415990591049  and accuracy is =  0.6444444444444445\n",
            "At step  70  and at epoch =  14  the loss is =  0.07957533746957779  and accuracy is =  0.6579259259259259\n",
            "At step  70  and at epoch =  15  the loss is =  0.08061531186103821  and accuracy is =  0.6702222222222223\n",
            "At step  70  and at epoch =  16  the loss is =  0.07799102365970612  and accuracy is =  0.6705185185185185\n",
            "At step  70  and at epoch =  17  the loss is =  0.08029434084892273  and accuracy is =  0.6845925925925926\n",
            "At step  70  and at epoch =  18  the loss is =  0.08102726936340332  and accuracy is =  0.6817777777777778\n",
            "At step  70  and at epoch =  19  the loss is =  0.07680283486843109  and accuracy is =  0.701037037037037\n",
            "At step  70  and at epoch =  20  the loss is =  0.0812523141503334  and accuracy is =  0.6961481481481482\n",
            "At step  70  and at epoch =  21  the loss is =  0.08005896955728531  and accuracy is =  0.7082962962962963\n",
            "At step  70  and at epoch =  22  the loss is =  0.0786290094256401  and accuracy is =  0.7032592592592593\n",
            "At step  70  and at epoch =  23  the loss is =  0.07660939544439316  and accuracy is =  0.7131851851851851\n",
            "At step  70  and at epoch =  24  the loss is =  0.07784226536750793  and accuracy is =  0.7140740740740741\n",
            "At step  70  and at epoch =  25  the loss is =  0.08191778510808945  and accuracy is =  0.7228148148148148\n",
            "At step  70  and at epoch =  26  the loss is =  0.0839909240603447  and accuracy is =  0.7186666666666667\n",
            "At step  70  and at epoch =  27  the loss is =  0.08163564652204514  and accuracy is =  0.7244444444444444\n",
            "At step  70  and at epoch =  28  the loss is =  0.08565962314605713  and accuracy is =  0.7293333333333333\n",
            "At step  70  and at epoch =  29  the loss is =  0.07810453325510025  and accuracy is =  0.7460740740740741\n",
            "At step  70  and at epoch =  30  the loss is =  0.07925013452768326  and accuracy is =  0.7337777777777778\n",
            "At step  70  and at epoch =  31  the loss is =  0.08001159131526947  and accuracy is =  0.744\n",
            "At step  70  and at epoch =  32  the loss is =  0.08021567761898041  and accuracy is =  0.7417777777777778\n",
            "At step  70  and at epoch =  33  the loss is =  0.08285409957170486  and accuracy is =  0.7451851851851852\n",
            "At step  70  and at epoch =  34  the loss is =  0.07970453053712845  and accuracy is =  0.7451851851851852\n",
            "At step  70  and at epoch =  35  the loss is =  0.0749836266040802  and accuracy is =  0.7488888888888889\n",
            "At step  70  and at epoch =  36  the loss is =  0.08334461599588394  and accuracy is =  0.7494814814814815\n",
            "At step  70  and at epoch =  37  the loss is =  0.07750796526670456  and accuracy is =  0.7586666666666667\n",
            "At step  70  and at epoch =  38  the loss is =  0.08115745335817337  and accuracy is =  0.7594074074074074\n",
            "At step  70  and at epoch =  39  the loss is =  0.07384161651134491  and accuracy is =  0.7543703703703704\n",
            "At step  70  and at epoch =  40  the loss is =  0.07787390053272247  and accuracy is =  0.7508148148148148\n",
            "At step  70  and at epoch =  41  the loss is =  0.08208563178777695  and accuracy is =  0.7557037037037037\n",
            "At step  70  and at epoch =  42  the loss is =  0.08348200470209122  and accuracy is =  0.7540740740740741\n",
            "At step  70  and at epoch =  43  the loss is =  0.0826021060347557  and accuracy is =  0.7677037037037037\n",
            "At step  70  and at epoch =  44  the loss is =  0.07575641572475433  and accuracy is =  0.7687407407407407\n",
            "At step  70  and at epoch =  45  the loss is =  0.08211686462163925  and accuracy is =  0.7687407407407407\n",
            "At step  70  and at epoch =  46  the loss is =  0.08054632693529129  and accuracy is =  0.7712592592592593\n",
            "At step  70  and at epoch =  47  the loss is =  0.077378049492836  and accuracy is =  0.7757037037037037\n",
            "At step  70  and at epoch =  48  the loss is =  0.08200391381978989  and accuracy is =  0.7742222222222223\n",
            "At step  70  and at epoch =  49  the loss is =  0.07706882059574127  and accuracy is =  0.806074074074074\n",
            "At step  70  and at epoch =  50  the loss is =  0.0793391689658165  and accuracy is =  0.8207407407407408\n",
            "At step  70  and at epoch =  51  the loss is =  0.07424366474151611  and accuracy is =  0.8237037037037037\n",
            "At step  70  and at epoch =  52  the loss is =  0.07622581720352173  and accuracy is =  0.8291851851851851\n",
            "At step  70  and at epoch =  53  the loss is =  0.07593227177858353  and accuracy is =  0.8321481481481482\n",
            "At step  70  and at epoch =  54  the loss is =  0.07394685596227646  and accuracy is =  0.830962962962963\n",
            "At step  70  and at epoch =  55  the loss is =  0.0718894973397255  and accuracy is =  0.8380740740740741\n",
            "At step  70  and at epoch =  56  the loss is =  0.0738031193614006  and accuracy is =  0.84\n",
            "At step  70  and at epoch =  57  the loss is =  0.07611498236656189  and accuracy is =  0.8373333333333334\n",
            "At step  70  and at epoch =  58  the loss is =  0.0765782818198204  and accuracy is =  0.8425185185185186\n",
            "At step  70  and at epoch =  59  the loss is =  0.08001726865768433  and accuracy is =  0.8358518518518518\n",
            "At step  70  and at epoch =  60  the loss is =  0.07719063013792038  and accuracy is =  0.845925925925926\n",
            "At step  70  and at epoch =  61  the loss is =  0.07404627650976181  and accuracy is =  0.8420740740740741\n",
            "At step  70  and at epoch =  62  the loss is =  0.07411632686853409  and accuracy is =  0.8432592592592593\n",
            "At step  70  and at epoch =  63  the loss is =  0.07236692309379578  and accuracy is =  0.8423703703703703\n",
            "At step  70  and at epoch =  64  the loss is =  0.06986269354820251  and accuracy is =  0.8444444444444444\n",
            "At step  70  and at epoch =  65  the loss is =  0.07176718860864639  and accuracy is =  0.8524444444444444\n",
            "At step  70  and at epoch =  66  the loss is =  0.07098483294248581  and accuracy is =  0.8514074074074074\n",
            "At step  70  and at epoch =  67  the loss is =  0.07453925162553787  and accuracy is =  0.845037037037037\n",
            "At step  70  and at epoch =  68  the loss is =  0.07143259048461914  and accuracy is =  0.8531851851851852\n",
            "At step  70  and at epoch =  69  the loss is =  0.07784993946552277  and accuracy is =  0.8484444444444444\n",
            "task: 70 train accuracy = 0.7216\n",
            "task: 70 test accuracy = 0.454\n",
            "task : 80\n",
            "0.967544436454773 -3.011828899383545\n",
            "At step  80  and at epoch =  0  the loss is =  0.10208359360694885  and accuracy is =  0.23652694610778444\n",
            "At step  80  and at epoch =  1  the loss is =  0.10331039130687714  and accuracy is =  0.33967065868263474\n",
            "At step  80  and at epoch =  2  the loss is =  0.11240410059690475  and accuracy is =  0.3941616766467066\n",
            "At step  80  and at epoch =  3  the loss is =  0.09495702385902405  and accuracy is =  0.4276946107784431\n",
            "At step  80  and at epoch =  4  the loss is =  0.0966273620724678  and accuracy is =  0.4510479041916168\n",
            "At step  80  and at epoch =  5  the loss is =  0.11229570209980011  and accuracy is =  0.47724550898203594\n",
            "At step  80  and at epoch =  6  the loss is =  0.09359245747327805  and accuracy is =  0.49610778443113773\n",
            "At step  80  and at epoch =  7  the loss is =  0.1118125319480896  and accuracy is =  0.5149700598802395\n",
            "At step  80  and at epoch =  8  the loss is =  0.09798157960176468  and accuracy is =  0.5288922155688622\n",
            "At step  80  and at epoch =  9  the loss is =  0.10403501242399216  and accuracy is =  0.5399700598802395\n",
            "At step  80  and at epoch =  10  the loss is =  0.11036176234483719  and accuracy is =  0.5607784431137725\n",
            "At step  80  and at epoch =  11  the loss is =  0.09551233053207397  and accuracy is =  0.5844311377245509\n",
            "At step  80  and at epoch =  12  the loss is =  0.10072515904903412  and accuracy is =  0.5854790419161676\n",
            "At step  80  and at epoch =  13  the loss is =  0.11228787899017334  and accuracy is =  0.5971556886227545\n",
            "At step  80  and at epoch =  14  the loss is =  0.10958239436149597  and accuracy is =  0.6059880239520958\n",
            "At step  80  and at epoch =  15  the loss is =  0.11105936765670776  and accuracy is =  0.6059880239520958\n",
            "At step  80  and at epoch =  16  the loss is =  0.1001870259642601  and accuracy is =  0.6091317365269461\n",
            "At step  80  and at epoch =  17  the loss is =  0.08785218000411987  and accuracy is =  0.6149700598802396\n",
            "At step  80  and at epoch =  18  the loss is =  0.08572842925786972  and accuracy is =  0.6305389221556886\n",
            "At step  80  and at epoch =  19  the loss is =  0.10032540559768677  and accuracy is =  0.63937125748503\n",
            "At step  80  and at epoch =  20  the loss is =  0.10404899716377258  and accuracy is =  0.6543413173652695\n",
            "At step  80  and at epoch =  21  the loss is =  0.1073252260684967  and accuracy is =  0.6341317365269461\n",
            "At step  80  and at epoch =  22  the loss is =  0.10887797176837921  and accuracy is =  0.6473053892215569\n",
            "At step  80  and at epoch =  23  the loss is =  0.09793820977210999  and accuracy is =  0.6570359281437126\n",
            "At step  80  and at epoch =  24  the loss is =  0.10286571830511093  and accuracy is =  0.6609281437125748\n",
            "At step  80  and at epoch =  25  the loss is =  0.1047925129532814  and accuracy is =  0.6741017964071856\n",
            "At step  80  and at epoch =  26  the loss is =  0.1060362383723259  and accuracy is =  0.6697604790419162\n",
            "At step  80  and at epoch =  27  the loss is =  0.10476133972406387  and accuracy is =  0.6785928143712575\n",
            "At step  80  and at epoch =  28  the loss is =  0.10855073481798172  and accuracy is =  0.6905688622754491\n",
            "At step  80  and at epoch =  29  the loss is =  0.11084926873445511  and accuracy is =  0.6818862275449101\n",
            "At step  80  and at epoch =  30  the loss is =  0.1024026945233345  and accuracy is =  0.6832335329341317\n",
            "At step  80  and at epoch =  31  the loss is =  0.11072331666946411  and accuracy is =  0.6947604790419162\n",
            "At step  80  and at epoch =  32  the loss is =  0.0984695553779602  and accuracy is =  0.7035928143712575\n",
            "At step  80  and at epoch =  33  the loss is =  0.09430593997240067  and accuracy is =  0.7001497005988024\n",
            "At step  80  and at epoch =  34  the loss is =  0.1148880273103714  and accuracy is =  0.7005988023952096\n",
            "At step  80  and at epoch =  35  the loss is =  0.10664401948451996  and accuracy is =  0.7004491017964072\n",
            "At step  80  and at epoch =  36  the loss is =  0.10425475239753723  and accuracy is =  0.7112275449101796\n",
            "At step  80  and at epoch =  37  the loss is =  0.09535335004329681  and accuracy is =  0.7107784431137725\n",
            "At step  80  and at epoch =  38  the loss is =  0.11543676257133484  and accuracy is =  0.7073353293413174\n",
            "At step  80  and at epoch =  39  the loss is =  0.11158113181591034  and accuracy is =  0.7001497005988024\n",
            "At step  80  and at epoch =  40  the loss is =  0.09529425203800201  and accuracy is =  0.7175149700598802\n",
            "At step  80  and at epoch =  41  the loss is =  0.10129410028457642  and accuracy is =  0.7157185628742515\n",
            "At step  80  and at epoch =  42  the loss is =  0.10080897063016891  and accuracy is =  0.7215568862275449\n",
            "At step  80  and at epoch =  43  the loss is =  0.09631814807653427  and accuracy is =  0.7232035928143713\n",
            "At step  80  and at epoch =  44  the loss is =  0.10286369919776917  and accuracy is =  0.7377245508982035\n",
            "At step  80  and at epoch =  45  the loss is =  0.10536987334489822  and accuracy is =  0.718562874251497\n",
            "At step  80  and at epoch =  46  the loss is =  0.09835342317819595  and accuracy is =  0.7164670658682635\n",
            "At step  80  and at epoch =  47  the loss is =  0.09171951562166214  and accuracy is =  0.7244011976047904\n",
            "At step  80  and at epoch =  48  the loss is =  0.10506026446819305  and accuracy is =  0.7383233532934131\n",
            "At step  80  and at epoch =  49  the loss is =  0.10221678763628006  and accuracy is =  0.7809880239520958\n",
            "At step  80  and at epoch =  50  the loss is =  0.09259218722581863  and accuracy is =  0.7988023952095809\n",
            "At step  80  and at epoch =  51  the loss is =  0.09706498682498932  and accuracy is =  0.8041916167664671\n",
            "At step  80  and at epoch =  52  the loss is =  0.09870787709951401  and accuracy is =  0.8067365269461078\n",
            "At step  80  and at epoch =  53  the loss is =  0.09816256910562515  and accuracy is =  0.8076347305389222\n",
            "At step  80  and at epoch =  54  the loss is =  0.09011783450841904  and accuracy is =  0.8175149700598803\n",
            "At step  80  and at epoch =  55  the loss is =  0.10699595510959625  and accuracy is =  0.8160179640718563\n",
            "At step  80  and at epoch =  56  the loss is =  0.0947888046503067  and accuracy is =  0.8202095808383234\n",
            "At step  80  and at epoch =  57  the loss is =  0.09845459461212158  and accuracy is =  0.8199101796407186\n",
            "At step  80  and at epoch =  58  the loss is =  0.08973333984613419  and accuracy is =  0.8161676646706587\n",
            "At step  80  and at epoch =  59  the loss is =  0.10214678198099136  and accuracy is =  0.8197604790419162\n",
            "At step  80  and at epoch =  60  the loss is =  0.08409123122692108  and accuracy is =  0.8166167664670658\n",
            "At step  80  and at epoch =  61  the loss is =  0.09129205346107483  and accuracy is =  0.8140718562874252\n",
            "At step  80  and at epoch =  62  the loss is =  0.09231750667095184  and accuracy is =  0.8221556886227545\n",
            "At step  80  and at epoch =  63  the loss is =  0.09436140954494476  and accuracy is =  0.8254491017964072\n",
            "At step  80  and at epoch =  64  the loss is =  0.0996275469660759  and accuracy is =  0.8279940119760479\n",
            "At step  80  and at epoch =  65  the loss is =  0.09017776697874069  and accuracy is =  0.8308383233532934\n",
            "At step  80  and at epoch =  66  the loss is =  0.10557521134614944  and accuracy is =  0.8357784431137725\n",
            "At step  80  and at epoch =  67  the loss is =  0.08905476331710815  and accuracy is =  0.8351796407185629\n",
            "At step  80  and at epoch =  68  the loss is =  0.08207666873931885  and accuracy is =  0.837125748502994\n",
            "At step  80  and at epoch =  69  the loss is =  0.0887921005487442  and accuracy is =  0.8330838323353293\n",
            "task: 80 train accuracy = 0.6048\n",
            "task: 80 test accuracy = 0.411\n",
            "task : 90\n",
            "0.8942586779594421 -3.4890379905700684\n",
            "At step  90  and at epoch =  0  the loss is =  0.11695105582475662  and accuracy is =  0.23595166163141995\n",
            "At step  90  and at epoch =  1  the loss is =  0.11127891391515732  and accuracy is =  0.37190332326283987\n",
            "At step  90  and at epoch =  2  the loss is =  0.10377281159162521  and accuracy is =  0.42220543806646527\n",
            "At step  90  and at epoch =  3  the loss is =  0.11316576600074768  and accuracy is =  0.46299093655589124\n",
            "At step  90  and at epoch =  4  the loss is =  0.10919267684221268  and accuracy is =  0.49259818731117827\n",
            "At step  90  and at epoch =  5  the loss is =  0.10398457199335098  and accuracy is =  0.5123867069486405\n",
            "At step  90  and at epoch =  6  the loss is =  0.11137208342552185  and accuracy is =  0.5225075528700907\n",
            "At step  90  and at epoch =  7  the loss is =  0.11482063680887222  and accuracy is =  0.5557401812688821\n",
            "At step  90  and at epoch =  8  the loss is =  0.1041332259774208  and accuracy is =  0.5589123867069486\n",
            "At step  90  and at epoch =  9  the loss is =  0.1080838292837143  and accuracy is =  0.579607250755287\n",
            "At step  90  and at epoch =  10  the loss is =  0.10284824669361115  and accuracy is =  0.5897280966767372\n",
            "At step  90  and at epoch =  11  the loss is =  0.11434934288263321  and accuracy is =  0.5894259818731118\n",
            "At step  90  and at epoch =  12  the loss is =  0.1042528748512268  and accuracy is =  0.6217522658610272\n",
            "At step  90  and at epoch =  13  the loss is =  0.1073598638176918  and accuracy is =  0.6196374622356495\n",
            "At step  90  and at epoch =  14  the loss is =  0.10925832390785217  and accuracy is =  0.627190332326284\n",
            "At step  90  and at epoch =  15  the loss is =  0.11059611290693283  and accuracy is =  0.6318731117824773\n",
            "At step  90  and at epoch =  16  the loss is =  0.10654496401548386  and accuracy is =  0.6472809667673716\n",
            "At step  90  and at epoch =  17  the loss is =  0.10352656245231628  and accuracy is =  0.6539274924471299\n",
            "At step  90  and at epoch =  18  the loss is =  0.1028146967291832  and accuracy is =  0.6666163141993958\n",
            "At step  90  and at epoch =  19  the loss is =  0.11101244390010834  and accuracy is =  0.6673716012084592\n",
            "At step  90  and at epoch =  20  the loss is =  0.10206025838851929  and accuracy is =  0.6744712990936556\n",
            "At step  90  and at epoch =  21  the loss is =  0.10229519754648209  and accuracy is =  0.6830815709969789\n",
            "At step  90  and at epoch =  22  the loss is =  0.10825292021036148  and accuracy is =  0.6838368580060423\n",
            "At step  90  and at epoch =  23  the loss is =  0.11095421761274338  and accuracy is =  0.6924471299093655\n",
            "At step  90  and at epoch =  24  the loss is =  0.11356411129236221  and accuracy is =  0.6921450151057402\n",
            "At step  90  and at epoch =  25  the loss is =  0.10339758545160294  and accuracy is =  0.6962235649546827\n",
            "At step  90  and at epoch =  26  the loss is =  0.10609211772680283  and accuracy is =  0.7003021148036254\n",
            "At step  90  and at epoch =  27  the loss is =  0.10290869325399399  and accuracy is =  0.7061933534743202\n",
            "At step  90  and at epoch =  28  the loss is =  0.10696232318878174  and accuracy is =  0.706344410876133\n",
            "At step  90  and at epoch =  29  the loss is =  0.10091136395931244  and accuracy is =  0.7141993957703927\n",
            "At step  90  and at epoch =  30  the loss is =  0.0987689271569252  and accuracy is =  0.7161631419939577\n",
            "At step  90  and at epoch =  31  the loss is =  0.10457724332809448  and accuracy is =  0.7128398791540785\n",
            "At step  90  and at epoch =  32  the loss is =  0.10219017416238785  and accuracy is =  0.7264350453172206\n",
            "At step  90  and at epoch =  33  the loss is =  0.10035283863544464  and accuracy is =  0.722809667673716\n",
            "At step  90  and at epoch =  34  the loss is =  0.10473314672708511  and accuracy is =  0.7271903323262839\n",
            "At step  90  and at epoch =  35  the loss is =  0.10009223222732544  and accuracy is =  0.7419939577039275\n",
            "At step  90  and at epoch =  36  the loss is =  0.10184802860021591  and accuracy is =  0.7374622356495468\n",
            "At step  90  and at epoch =  37  the loss is =  0.1017770767211914  and accuracy is =  0.7344410876132931\n",
            "At step  90  and at epoch =  38  the loss is =  0.1078452318906784  and accuracy is =  0.7365558912386707\n",
            "At step  90  and at epoch =  39  the loss is =  0.10618626326322556  and accuracy is =  0.7373111782477342\n",
            "At step  90  and at epoch =  40  the loss is =  0.10337971895933151  and accuracy is =  0.7404833836858006\n",
            "At step  90  and at epoch =  41  the loss is =  0.10499020665884018  and accuracy is =  0.7442598187311178\n",
            "At step  90  and at epoch =  42  the loss is =  0.10443375259637833  and accuracy is =  0.7401812688821753\n",
            "At step  90  and at epoch =  43  the loss is =  0.10774493962526321  and accuracy is =  0.7525679758308157\n",
            "At step  90  and at epoch =  44  the loss is =  0.10227524489164352  and accuracy is =  0.7465256797583082\n",
            "At step  90  and at epoch =  45  the loss is =  0.10189938545227051  and accuracy is =  0.7429003021148036\n",
            "At step  90  and at epoch =  46  the loss is =  0.10113166272640228  and accuracy is =  0.7422960725075529\n",
            "At step  90  and at epoch =  47  the loss is =  0.09835625439882278  and accuracy is =  0.752416918429003\n",
            "At step  90  and at epoch =  48  the loss is =  0.09823189675807953  and accuracy is =  0.7534743202416918\n",
            "At step  90  and at epoch =  49  the loss is =  0.10001585632562637  and accuracy is =  0.8012084592145015\n",
            "At step  90  and at epoch =  50  the loss is =  0.09867345541715622  and accuracy is =  0.8074018126888217\n",
            "At step  90  and at epoch =  51  the loss is =  0.09599605202674866  and accuracy is =  0.808761329305136\n",
            "At step  90  and at epoch =  52  the loss is =  0.0974978357553482  and accuracy is =  0.8039274924471299\n",
            "At step  90  and at epoch =  53  the loss is =  0.09966658800840378  and accuracy is =  0.8158610271903324\n",
            "At step  90  and at epoch =  54  the loss is =  0.10142258554697037  and accuracy is =  0.8200906344410877\n",
            "At step  90  and at epoch =  55  the loss is =  0.10716592520475388  and accuracy is =  0.8200906344410877\n",
            "At step  90  and at epoch =  56  the loss is =  0.09762280434370041  and accuracy is =  0.8169184290030211\n",
            "At step  90  and at epoch =  57  the loss is =  0.09870969504117966  and accuracy is =  0.8223564954682779\n",
            "At step  90  and at epoch =  58  the loss is =  0.09347956627607346  and accuracy is =  0.8231117824773414\n",
            "At step  90  and at epoch =  59  the loss is =  0.09726623445749283  and accuracy is =  0.8196374622356496\n",
            "At step  90  and at epoch =  60  the loss is =  0.10045057535171509  and accuracy is =  0.8200906344410877\n",
            "At step  90  and at epoch =  61  the loss is =  0.10156963765621185  and accuracy is =  0.8268882175226586\n",
            "At step  90  and at epoch =  62  the loss is =  0.1013786643743515  and accuracy is =  0.8238670694864049\n",
            "At step  90  and at epoch =  63  the loss is =  0.10436753928661346  and accuracy is =  0.8259818731117825\n",
            "At step  90  and at epoch =  64  the loss is =  0.0968642383813858  and accuracy is =  0.8321752265861028\n",
            "At step  90  and at epoch =  65  the loss is =  0.1001373678445816  and accuracy is =  0.831570996978852\n",
            "At step  90  and at epoch =  66  the loss is =  0.09939813613891602  and accuracy is =  0.8326283987915408\n",
            "At step  90  and at epoch =  67  the loss is =  0.09503202885389328  and accuracy is =  0.8386706948640483\n",
            "At step  90  and at epoch =  68  the loss is =  0.09821672737598419  and accuracy is =  0.8347432024169185\n",
            "At step  90  and at epoch =  69  the loss is =  0.09774686396121979  and accuracy is =  0.8297583081570997\n",
            "task: 90 train accuracy = 0.5182\n",
            "task: 90 test accuracy = 0.3821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4_a4Ut-Or2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accs = [.89,.76,.71,.64,.59,.53,.48,.45,.41,.40, .38]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAayIE_aPFWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "def plotTask(pars_tasks):\n",
        "\n",
        "  x_tasks =  np.linspace(10, 100, 10)\n",
        "\n",
        "  plt.plot(x_tasks, pars_tasks, label=['Accuracy'])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylim(0,1)\n",
        "  plt.title('Accuracy over tasks')\n",
        "  plt.legend(['Accuracy'])\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RnKOCdaPM9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6bac0b23-25cf-4161-e77e-2918af8cfc6e"
      },
      "source": [
        "plotTask(accs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}