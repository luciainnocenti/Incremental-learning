{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "616b6dd35d7248c59da4ced8a00fc4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_86683320f6ca46a49f10125a3cef20f9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dbf0cd422ed444b2a932891be614cea1",
              "IPY_MODEL_6887de48376c41ca8697eb7e01838973"
            ]
          }
        },
        "86683320f6ca46a49f10125a3cef20f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbf0cd422ed444b2a932891be614cea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aff81bb71f7a4281afb79f80a225f70f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34af509eecc441eaaccf2ee5b44d6477"
          }
        },
        "6887de48376c41ca8697eb7e01838973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_af9924cd548c4e3e98dff87d7f9ea995",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:30&lt;00:00, 14031502.45it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_900101264cfc40b7a32a64a5b8d1f1f8"
          }
        },
        "aff81bb71f7a4281afb79f80a225f70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34af509eecc441eaaccf2ee5b44d6477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af9924cd548c4e3e98dff87d7f9ea995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "900101264cfc40b7a32a64a5b8d1f1f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciainnocenti/IncrementalLearning/blob/master/FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCO7owW4O0jb",
        "colab_type": "text"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01VjtslGOvI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSC6q-24O8Yr",
        "colab_type": "code",
        "outputId": "08ba1ed1-5479-4471-f84f-08ed329264db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "if not os.path.isdir('./DatasetCIFAR'):\n",
        "  !git clone https://github.com/luciainnocenti/IncrementalLearning.git\n",
        "  !mv 'IncrementalLearning' 'DatasetCIFAR'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 726 (delta 120), reused 0 (delta 0), pack-reused 532\u001b[K\n",
            "Receiving objects: 100% (726/726), 467.43 KiB | 547.00 KiB/s, done.\n",
            "Resolving deltas: 100% (449/449), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q5sdRysO9-F",
        "colab_type": "text"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3AwEK1IPEDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from DatasetCIFAR.data_set import Dataset \n",
        "from DatasetCIFAR import ResNet\n",
        "from DatasetCIFAR import utils\n",
        "from DatasetCIFAR import params\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "random.seed(params.SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU8_JQV3O_c5",
        "colab_type": "text"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SN6IDz3PJUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resNet = ResNet.resnet32(num_classes=100)\n",
        "resNet = resNet.to(params.DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXGgft0iPLVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_transformer = transforms.Compose([transforms.RandomCrop(size = 32, padding=4),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ldJmjNkPNiG",
        "colab_type": "text"
      },
      "source": [
        "# Define DataSets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czGrTi8VPRoB",
        "colab_type": "code",
        "outputId": "5629e56d-f678-42f1-d30d-95a8c2e36b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "616b6dd35d7248c59da4ced8a00fc4ab",
            "86683320f6ca46a49f10125a3cef20f9",
            "dbf0cd422ed444b2a932891be614cea1",
            "6887de48376c41ca8697eb7e01838973",
            "aff81bb71f7a4281afb79f80a225f70f",
            "34af509eecc441eaaccf2ee5b44d6477",
            "af9924cd548c4e3e98dff87d7f9ea995",
            "900101264cfc40b7a32a64a5b8d1f1f8"
          ]
        }
      },
      "source": [
        "trainDS = Dataset(train=True, transform = train_transformer)\n",
        "testDS = Dataset(train=False, transform = test_transformer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "616b6dd35d7248c59da4ced8a00fc4ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn8XrSWqPUiN",
        "colab_type": "code",
        "outputId": "105afb71-77aa-4ef7-ef87-198f169c15de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "train_splits = trainDS.splits\n",
        "test_splits = testDS.splits\n",
        "print(train_splits)\n",
        "print(test_splits)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[94.0, 63.0, 74.0, 21.0, 35.0, 56.0, 91.0, 96.0, 87.0, 48.0], [68.0, 80.0, 22.0, 37.0, 60.0, 97.0, 51.0, 62.0, 92.0, 76.0], [75.0, 89.0, 23.0, 99.0, 39.0, 66.0, 54.0, 69.0, 84.0, 61.0], [85.0, 24.0, 98.0, 41.0, 73.0, 58.0, 78.0, 77.0, 70.0, 49.0], [65.0, 88.0, 36.0, 93.0, 45.0, 10.0, 90.0, 17.0, 32.0, 59.0], [83.0, 43.0, 53.0, 11.0, 86.0, 19.0, 38.0, 30.0, 40.0, 50.0], [57.0, 81.0, 12.0, 95.0, 25.0, 47.0, 34.0, 52.0, 44.0, 72.0], [46.0, 79.0, 20.0, 28.0, 5.0, 71.0, 8.0, 18.0, 33.0, 15.0], [55.0, 29.0, 64.0, 31.0, 67.0, 7.0, 13.0, 14.0, 42.0, 6.0], [82.0, 2.0, 27.0, 16.0, 26.0, 3.0, 4.0, 1.0, 9.0, 0.0]]\n",
            "[[94.0, 63.0, 74.0, 21.0, 35.0, 56.0, 91.0, 96.0, 87.0, 48.0], [68.0, 80.0, 22.0, 37.0, 60.0, 97.0, 51.0, 62.0, 92.0, 76.0], [75.0, 89.0, 23.0, 99.0, 39.0, 66.0, 54.0, 69.0, 84.0, 61.0], [85.0, 24.0, 98.0, 41.0, 73.0, 58.0, 78.0, 77.0, 70.0, 49.0], [65.0, 88.0, 36.0, 93.0, 45.0, 10.0, 90.0, 17.0, 32.0, 59.0], [83.0, 43.0, 53.0, 11.0, 86.0, 19.0, 38.0, 30.0, 40.0, 50.0], [57.0, 81.0, 12.0, 95.0, 25.0, 47.0, 34.0, 52.0, 44.0, 72.0], [46.0, 79.0, 20.0, 28.0, 5.0, 71.0, 8.0, 18.0, 33.0, 15.0], [55.0, 29.0, 64.0, 31.0, 67.0, 7.0, 13.0, 14.0, 42.0, 6.0], [82.0, 2.0, 27.0, 16.0, 26.0, 3.0, 4.0, 1.0, 9.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc8WMLe3PQvI",
        "colab_type": "text"
      },
      "source": [
        "# Useful plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQBZdyyrPW_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTask(pars_tasks):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  x_tasks =  np.linspace(10, 100, 10)\n",
        "\n",
        "  plt.plot(x_tasks, pars_tasks, label=['Accuracy', 'Loss'])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.title('Accuracy over tasks')\n",
        "  plt.legend(['Accuracy', 'Loss'])\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RzunPCcSfip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapFunction(labels, splits):\n",
        "\tm_l = []\n",
        "\tl_splits = list(splits)\n",
        "\tfor el in labels:\n",
        "\t\tm_l.append( l_splits.index(el) )\n",
        "\treturn torch.LongTensor(m_l).to(params.DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPWcz2ISRN6E",
        "colab_type": "text"
      },
      "source": [
        "# Train and evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs6I_lmjRLYo",
        "colab_type": "code",
        "outputId": "c7b93f92-4477-45f0-bc8f-dc9a9d267ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pars_tasks = []\n",
        "test_indexes = []\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "  pars_tasks.insert(task, 0)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(resNet.parameters(), lr=params.LR, momentum=params.MOMENTUM, weight_decay=params.WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, params.STEP_SIZE, gamma=params.GAMMA) #allow to change the LR at predefined epochs\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "\n",
        "for task in range(0, 100, params.TASK_SIZE):\n",
        "\n",
        "  train_indexes = trainDS.__getIndexesGroups__(task)\n",
        "  test_indexes = test_indexes + testDS.__getIndexesGroups__(task)\n",
        "\n",
        "  train_dataset = Subset(trainDS, train_indexes)\n",
        "  test_dataset = Subset(testDS, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE, shuffle=True)\n",
        "  test_loader = DataLoader( test_dataset, num_workers=params.NUM_WORKERS, batch_size=params.BATCH_SIZE, shuffle=True )\n",
        "  \n",
        "  col = np.array(train_splits[int(task/10)]).astype(int)\n",
        "  \n",
        "  for epoch in range(params.NUM_EPOCHS):\n",
        "    lenght = 0\n",
        "    scheduler.step() #update the learning rate\n",
        "    running_corrects = 0    \n",
        "    for images, labels, _ in train_loader:\n",
        "      images = images.float().to(params.DEVICE)\n",
        "      labels = labels.to(params.DEVICE)\n",
        "      mappedLabels = mapFunction(labels, col)   \n",
        "      onehot_labels = torch.eye(100)[labels].to(params.DEVICE)#it creates the one-hot-encoding list for the labels; needed for BCELoss    \n",
        "      optimizer.zero_grad() # Zero-ing the gradients\n",
        "      outputs = resNet(images)\n",
        "      loss = criterion(outputs,onehot_labels)   \n",
        "      cut_outputs = np.take_along_axis(outputs.to(params.DEVICE), col[None, :], axis = 1).to(params.DEVICE)\n",
        "      _, preds = torch.max(cut_outputs.data, 1)\n",
        "      running_corrects += torch.sum(preds == mappedLabels.data).data.item()   \n",
        "      loss.backward()   \n",
        "      optimizer.step()    \n",
        "      current_step += 1\n",
        "      lenght += len(images)   \n",
        "    accuracy = running_corrects / float(lenght)\n",
        "    print(\"At step \", str(task), \" and at epoch = \", epoch, \" the loss is = \", loss.item(), \" and accuracy is = \", accuracy)\n",
        "  \n",
        "  resNet = resNet.eval()\n",
        "\n",
        "  t_l = 0\n",
        "  running_corrects = 0\n",
        "  col = []\n",
        "  for i,x in enumerate( test_splits[ :int(task/10) + 1]):\n",
        "  \tv = np.array(x)\n",
        "  \tcol = np.concatenate( (col,v), axis = None)\n",
        "  col = col.astype(int)\n",
        "\n",
        "  for images, labels, _ in test_loader:\n",
        "    images = images.float().to(params.DEVICE)\n",
        "    labels = labels.to(params.DEVICE)   \n",
        "    mappedLabels = mapFunction(labels, col)\n",
        "\n",
        "    onehot_labels = torch.eye(100)[labels].to(params.DEVICE)\n",
        "\n",
        "    outputs = resNet(images).to(params.DEVICE)\n",
        "    cut_outputs = np.take_along_axis(outputs, col[None, :], axis = 1)\n",
        "    cut_outputs = cut_outputs.to(params.DEVICE)\n",
        "    _, preds = torch.max(cut_outputs.data, 1)\n",
        "\t\t# Update Corrects\n",
        "    running_corrects += torch.sum(preds == mappedLabels.data).data.item()\n",
        "    print(len(images))\n",
        "    t_l += len(images)\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / float(t_l)\n",
        "  loss = criterion(outputs,onehot_labels)\n",
        "  print('Validation Loss: {} Validation Accuracy : {}'.format(loss.item(),accuracy) )\n",
        "  pars_tasks[int(task/10)] = (accuracy, loss.item())\n",
        "  resNet = resNet.train(True)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "At step  0  and at epoch =  0  the loss is =  0.03383159637451172  and accuracy is =  0.1828\n",
            "At step  0  and at epoch =  1  the loss is =  0.03398982435464859  and accuracy is =  0.405\n",
            "At step  0  and at epoch =  2  the loss is =  0.021684983745217323  and accuracy is =  0.4906\n",
            "At step  0  and at epoch =  3  the loss is =  0.01981181651353836  and accuracy is =  0.5746\n",
            "At step  0  and at epoch =  4  the loss is =  0.01945577561855316  and accuracy is =  0.6316\n",
            "At step  0  and at epoch =  5  the loss is =  0.011442254297435284  and accuracy is =  0.6778\n",
            "At step  0  and at epoch =  6  the loss is =  0.010256149806082249  and accuracy is =  0.7132\n",
            "At step  0  and at epoch =  7  the loss is =  0.030698062852025032  and accuracy is =  0.7704\n",
            "At step  0  and at epoch =  8  the loss is =  0.011082748882472515  and accuracy is =  0.778\n",
            "At step  0  and at epoch =  9  the loss is =  0.0066450051963329315  and accuracy is =  0.7794\n",
            "At step  0  and at epoch =  10  the loss is =  0.01784353330731392  and accuracy is =  0.8408\n",
            "At step  0  and at epoch =  11  the loss is =  0.014761514030396938  and accuracy is =  0.8202\n",
            "At step  0  and at epoch =  12  the loss is =  0.01804336905479431  and accuracy is =  0.8388\n",
            "At step  0  and at epoch =  13  the loss is =  0.009062509052455425  and accuracy is =  0.8678\n",
            "At step  0  and at epoch =  14  the loss is =  0.015080040320754051  and accuracy is =  0.8826\n",
            "At step  0  and at epoch =  15  the loss is =  0.01810400001704693  and accuracy is =  0.908\n",
            "At step  0  and at epoch =  16  the loss is =  0.015546480193734169  and accuracy is =  0.8238\n",
            "At step  0  and at epoch =  17  the loss is =  0.011631831526756287  and accuracy is =  0.9076\n",
            "At step  0  and at epoch =  18  the loss is =  0.011645093560218811  and accuracy is =  0.9244\n",
            "At step  0  and at epoch =  19  the loss is =  0.005126747768372297  and accuracy is =  0.9544\n",
            "At step  0  and at epoch =  20  the loss is =  0.02681983821094036  and accuracy is =  0.964\n",
            "At step  0  and at epoch =  21  the loss is =  0.008655036799609661  and accuracy is =  0.8626\n",
            "At step  0  and at epoch =  22  the loss is =  0.03468931093811989  and accuracy is =  0.9258\n",
            "At step  0  and at epoch =  23  the loss is =  0.03174929693341255  and accuracy is =  0.8548\n",
            "At step  0  and at epoch =  24  the loss is =  0.009447230026125908  and accuracy is =  0.8686\n",
            "At step  0  and at epoch =  25  the loss is =  0.002771389205008745  and accuracy is =  0.9158\n",
            "At step  0  and at epoch =  26  the loss is =  0.0031229096930474043  and accuracy is =  0.976\n",
            "At step  0  and at epoch =  27  the loss is =  0.007864594459533691  and accuracy is =  0.9772\n",
            "At step  0  and at epoch =  28  the loss is =  0.013360051438212395  and accuracy is =  0.9362\n",
            "At step  0  and at epoch =  29  the loss is =  0.0006320506217889488  and accuracy is =  0.9038\n",
            "At step  0  and at epoch =  30  the loss is =  0.0002091934293275699  and accuracy is =  0.9792\n",
            "At step  0  and at epoch =  31  the loss is =  0.02052943967282772  and accuracy is =  0.9944\n",
            "At step  0  and at epoch =  32  the loss is =  0.024419955909252167  and accuracy is =  0.9426\n",
            "At step  0  and at epoch =  33  the loss is =  0.011665153317153454  and accuracy is =  0.9332\n",
            "At step  0  and at epoch =  34  the loss is =  0.016441969200968742  and accuracy is =  0.9682\n",
            "At step  0  and at epoch =  35  the loss is =  0.02178257703781128  and accuracy is =  0.9478\n",
            "At step  0  and at epoch =  36  the loss is =  0.0012551320251077414  and accuracy is =  0.944\n",
            "At step  0  and at epoch =  37  the loss is =  0.0028667491860687733  and accuracy is =  0.989\n",
            "At step  0  and at epoch =  38  the loss is =  0.00046513296547345817  and accuracy is =  0.9884\n",
            "At step  0  and at epoch =  39  the loss is =  0.011983704753220081  and accuracy is =  0.9952\n",
            "At step  0  and at epoch =  40  the loss is =  0.01899230293929577  and accuracy is =  0.9696\n",
            "At step  0  and at epoch =  41  the loss is =  0.007618504110723734  and accuracy is =  0.9644\n",
            "At step  0  and at epoch =  42  the loss is =  0.016233772039413452  and accuracy is =  0.9236\n",
            "At step  0  and at epoch =  43  the loss is =  0.019612736999988556  and accuracy is =  0.9504\n",
            "At step  0  and at epoch =  44  the loss is =  0.01648879423737526  and accuracy is =  0.931\n",
            "At step  0  and at epoch =  45  the loss is =  0.008146466687321663  and accuracy is =  0.9488\n",
            "At step  0  and at epoch =  46  the loss is =  0.0017621887382119894  and accuracy is =  0.9782\n",
            "At step  0  and at epoch =  47  the loss is =  0.001622688490897417  and accuracy is =  0.9932\n",
            "At step  0  and at epoch =  48  the loss is =  0.0003130741242785007  and accuracy is =  0.998\n",
            "At step  0  and at epoch =  49  the loss is =  0.004051678813993931  and accuracy is =  0.9992\n",
            "At step  0  and at epoch =  50  the loss is =  0.005965748801827431  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  51  the loss is =  0.022659650072455406  and accuracy is =  0.9992\n",
            "At step  0  and at epoch =  52  the loss is =  0.010439506731927395  and accuracy is =  0.998\n",
            "At step  0  and at epoch =  53  the loss is =  0.007019254844635725  and accuracy is =  0.9994\n",
            "At step  0  and at epoch =  54  the loss is =  0.008473929949104786  and accuracy is =  0.9992\n",
            "At step  0  and at epoch =  55  the loss is =  0.009218676947057247  and accuracy is =  0.9976\n",
            "At step  0  and at epoch =  56  the loss is =  0.0041580842807888985  and accuracy is =  0.9982\n",
            "At step  0  and at epoch =  57  the loss is =  0.00561616662889719  and accuracy is =  0.9992\n",
            "At step  0  and at epoch =  58  the loss is =  0.0031700092367827892  and accuracy is =  0.999\n",
            "At step  0  and at epoch =  59  the loss is =  0.009241227060556412  and accuracy is =  0.9994\n",
            "At step  0  and at epoch =  60  the loss is =  0.001779333339072764  and accuracy is =  0.998\n",
            "At step  0  and at epoch =  61  the loss is =  0.002486025681719184  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  62  the loss is =  0.00017380873032379895  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  63  the loss is =  0.0037464473862200975  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  64  the loss is =  0.01071276143193245  and accuracy is =  0.9994\n",
            "At step  0  and at epoch =  65  the loss is =  0.004453020170331001  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  66  the loss is =  0.022696861997246742  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  67  the loss is =  0.003142101690173149  and accuracy is =  0.9996\n",
            "At step  0  and at epoch =  68  the loss is =  0.0015806859591975808  and accuracy is =  0.9998\n",
            "At step  0  and at epoch =  69  the loss is =  0.01034005731344223  and accuracy is =  0.9996\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "104\n",
            "Validation Loss: 0.008301270194351673 Validation Accuracy : 0.787\n",
            "At step  10  and at epoch =  0  the loss is =  0.08082979172468185  and accuracy is =  0.1088\n",
            "At step  10  and at epoch =  1  the loss is =  0.04833368584513664  and accuracy is =  0.2138\n",
            "At step  10  and at epoch =  2  the loss is =  0.03239177167415619  and accuracy is =  0.273\n",
            "At step  10  and at epoch =  3  the loss is =  0.02815590240061283  and accuracy is =  0.3056\n",
            "At step  10  and at epoch =  4  the loss is =  0.03481333330273628  and accuracy is =  0.3408\n",
            "At step  10  and at epoch =  5  the loss is =  0.029855873435735703  and accuracy is =  0.382\n",
            "At step  10  and at epoch =  6  the loss is =  0.02294522151350975  and accuracy is =  0.428\n",
            "At step  10  and at epoch =  7  the loss is =  0.026018766686320305  and accuracy is =  0.456\n",
            "At step  10  and at epoch =  8  the loss is =  0.028784865513443947  and accuracy is =  0.4696\n",
            "At step  10  and at epoch =  9  the loss is =  0.019941000267863274  and accuracy is =  0.4876\n",
            "At step  10  and at epoch =  10  the loss is =  0.026976391673088074  and accuracy is =  0.5072\n",
            "At step  10  and at epoch =  11  the loss is =  0.021178368479013443  and accuracy is =  0.5322\n",
            "At step  10  and at epoch =  12  the loss is =  0.02105765789747238  and accuracy is =  0.5466\n",
            "At step  10  and at epoch =  13  the loss is =  0.022912131622433662  and accuracy is =  0.5612\n",
            "At step  10  and at epoch =  14  the loss is =  0.024496864527463913  and accuracy is =  0.58\n",
            "At step  10  and at epoch =  15  the loss is =  0.022758135572075844  and accuracy is =  0.5886\n",
            "At step  10  and at epoch =  16  the loss is =  0.021521972492337227  and accuracy is =  0.6028\n",
            "At step  10  and at epoch =  17  the loss is =  0.01999635621905327  and accuracy is =  0.6092\n",
            "At step  10  and at epoch =  18  the loss is =  0.015221970155835152  and accuracy is =  0.6258\n",
            "At step  10  and at epoch =  19  the loss is =  0.020815184339880943  and accuracy is =  0.6342\n",
            "At step  10  and at epoch =  20  the loss is =  0.02524730935692787  and accuracy is =  0.6428\n",
            "At step  10  and at epoch =  21  the loss is =  0.018062233924865723  and accuracy is =  0.6492\n",
            "At step  10  and at epoch =  22  the loss is =  0.01574655994772911  and accuracy is =  0.6548\n",
            "At step  10  and at epoch =  23  the loss is =  0.01685941591858864  and accuracy is =  0.6654\n",
            "At step  10  and at epoch =  24  the loss is =  0.02659325860440731  and accuracy is =  0.676\n",
            "At step  10  and at epoch =  25  the loss is =  0.016583271324634552  and accuracy is =  0.6846\n",
            "At step  10  and at epoch =  26  the loss is =  0.023472346365451813  and accuracy is =  0.6904\n",
            "At step  10  and at epoch =  27  the loss is =  0.017565065994858742  and accuracy is =  0.702\n",
            "At step  10  and at epoch =  28  the loss is =  0.02441524900496006  and accuracy is =  0.7046\n",
            "At step  10  and at epoch =  29  the loss is =  0.01952822133898735  and accuracy is =  0.7132\n",
            "At step  10  and at epoch =  30  the loss is =  0.01863805204629898  and accuracy is =  0.7226\n",
            "At step  10  and at epoch =  31  the loss is =  0.0172758586704731  and accuracy is =  0.7238\n",
            "At step  10  and at epoch =  32  the loss is =  0.019476497545838356  and accuracy is =  0.733\n",
            "At step  10  and at epoch =  33  the loss is =  0.021147973835468292  and accuracy is =  0.7392\n",
            "At step  10  and at epoch =  34  the loss is =  0.013052639551460743  and accuracy is =  0.7478\n",
            "At step  10  and at epoch =  35  the loss is =  0.013883459381759167  and accuracy is =  0.7468\n",
            "At step  10  and at epoch =  36  the loss is =  0.015492896549403667  and accuracy is =  0.7526\n",
            "At step  10  and at epoch =  37  the loss is =  0.007435828913003206  and accuracy is =  0.7626\n",
            "At step  10  and at epoch =  38  the loss is =  0.0258076973259449  and accuracy is =  0.771\n",
            "At step  10  and at epoch =  39  the loss is =  0.023606151342391968  and accuracy is =  0.773\n",
            "At step  10  and at epoch =  40  the loss is =  0.009821997955441475  and accuracy is =  0.7872\n",
            "At step  10  and at epoch =  41  the loss is =  0.019349174574017525  and accuracy is =  0.7878\n",
            "At step  10  and at epoch =  42  the loss is =  0.02249416708946228  and accuracy is =  0.7918\n",
            "At step  10  and at epoch =  43  the loss is =  0.009009337052702904  and accuracy is =  0.8042\n",
            "At step  10  and at epoch =  44  the loss is =  0.008170192129909992  and accuracy is =  0.8072\n",
            "At step  10  and at epoch =  45  the loss is =  0.014571539126336575  and accuracy is =  0.8112\n",
            "At step  10  and at epoch =  46  the loss is =  0.020639963448047638  and accuracy is =  0.8174\n",
            "At step  10  and at epoch =  47  the loss is =  0.01731359399855137  and accuracy is =  0.8224\n",
            "At step  10  and at epoch =  48  the loss is =  0.00750277005136013  and accuracy is =  0.8368\n",
            "At step  10  and at epoch =  49  the loss is =  0.0060971020720899105  and accuracy is =  0.8394\n",
            "At step  10  and at epoch =  50  the loss is =  0.014700555242598057  and accuracy is =  0.8454\n",
            "At step  10  and at epoch =  51  the loss is =  0.023014230653643608  and accuracy is =  0.8458\n",
            "At step  10  and at epoch =  52  the loss is =  0.011760836467146873  and accuracy is =  0.8542\n",
            "At step  10  and at epoch =  53  the loss is =  0.006034493446350098  and accuracy is =  0.8574\n",
            "At step  10  and at epoch =  54  the loss is =  0.022621506825089455  and accuracy is =  0.8678\n",
            "At step  10  and at epoch =  55  the loss is =  0.01255936361849308  and accuracy is =  0.8776\n",
            "At step  10  and at epoch =  56  the loss is =  0.02886858396232128  and accuracy is =  0.8814\n",
            "At step  10  and at epoch =  57  the loss is =  0.015144919976592064  and accuracy is =  0.8852\n",
            "At step  10  and at epoch =  58  the loss is =  0.017838401719927788  and accuracy is =  0.8936\n",
            "At step  10  and at epoch =  59  the loss is =  0.009412387385964394  and accuracy is =  0.9018\n",
            "At step  10  and at epoch =  60  the loss is =  0.020965971052646637  and accuracy is =  0.9038\n",
            "At step  10  and at epoch =  61  the loss is =  0.015403952449560165  and accuracy is =  0.9076\n",
            "At step  10  and at epoch =  62  the loss is =  0.01433834619820118  and accuracy is =  0.9116\n",
            "At step  10  and at epoch =  63  the loss is =  0.015743352472782135  and accuracy is =  0.9196\n",
            "At step  10  and at epoch =  64  the loss is =  0.010280456393957138  and accuracy is =  0.9258\n",
            "At step  10  and at epoch =  65  the loss is =  0.023147277534008026  and accuracy is =  0.9302\n",
            "At step  10  and at epoch =  66  the loss is =  0.010617482475936413  and accuracy is =  0.9384\n",
            "At step  10  and at epoch =  67  the loss is =  0.0034257040824741125  and accuracy is =  0.9424\n",
            "At step  10  and at epoch =  68  the loss is =  0.019120704382658005  and accuracy is =  0.9466\n",
            "At step  10  and at epoch =  69  the loss is =  0.00668705627322197  and accuracy is =  0.9448\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "80\n",
            "Validation Loss: 0.0652499571442604 Validation Accuracy : 0.3385\n",
            "At step  20  and at epoch =  0  the loss is =  0.08192544430494308  and accuracy is =  0.0772\n",
            "At step  20  and at epoch =  1  the loss is =  0.05720790848135948  and accuracy is =  0.1862\n",
            "At step  20  and at epoch =  2  the loss is =  0.035868268460035324  and accuracy is =  0.2402\n",
            "At step  20  and at epoch =  3  the loss is =  0.027389125898480415  and accuracy is =  0.3418\n",
            "At step  20  and at epoch =  4  the loss is =  0.024951700121164322  and accuracy is =  0.4084\n",
            "At step  20  and at epoch =  5  the loss is =  0.02223694697022438  and accuracy is =  0.441\n",
            "At step  20  and at epoch =  6  the loss is =  0.027923578396439552  and accuracy is =  0.4692\n",
            "At step  20  and at epoch =  7  the loss is =  0.021166276186704636  and accuracy is =  0.488\n",
            "At step  20  and at epoch =  8  the loss is =  0.020603744313120842  and accuracy is =  0.5088\n",
            "At step  20  and at epoch =  9  the loss is =  0.022449476644396782  and accuracy is =  0.5302\n",
            "At step  20  and at epoch =  10  the loss is =  0.018973268568515778  and accuracy is =  0.5448\n",
            "At step  20  and at epoch =  11  the loss is =  0.021734656766057014  and accuracy is =  0.5628\n",
            "At step  20  and at epoch =  12  the loss is =  0.029677532613277435  and accuracy is =  0.5858\n",
            "At step  20  and at epoch =  13  the loss is =  0.01873791590332985  and accuracy is =  0.5972\n",
            "At step  20  and at epoch =  14  the loss is =  0.018729107454419136  and accuracy is =  0.6124\n",
            "At step  20  and at epoch =  15  the loss is =  0.01850300095975399  and accuracy is =  0.6356\n",
            "At step  20  and at epoch =  16  the loss is =  0.018911199644207954  and accuracy is =  0.6474\n",
            "At step  20  and at epoch =  17  the loss is =  0.02334601990878582  and accuracy is =  0.6518\n",
            "At step  20  and at epoch =  18  the loss is =  0.017752323299646378  and accuracy is =  0.6694\n",
            "At step  20  and at epoch =  19  the loss is =  0.016980521380901337  and accuracy is =  0.6864\n",
            "At step  20  and at epoch =  20  the loss is =  0.02016259916126728  and accuracy is =  0.7008\n",
            "At step  20  and at epoch =  21  the loss is =  0.020643621683120728  and accuracy is =  0.7122\n",
            "At step  20  and at epoch =  22  the loss is =  0.016118504106998444  and accuracy is =  0.7282\n",
            "At step  20  and at epoch =  23  the loss is =  0.01634160801768303  and accuracy is =  0.7376\n",
            "At step  20  and at epoch =  24  the loss is =  0.020859116688370705  and accuracy is =  0.747\n",
            "At step  20  and at epoch =  25  the loss is =  0.019598107784986496  and accuracy is =  0.7596\n",
            "At step  20  and at epoch =  26  the loss is =  0.01390232052654028  and accuracy is =  0.771\n",
            "At step  20  and at epoch =  27  the loss is =  0.0321248434484005  and accuracy is =  0.7812\n",
            "At step  20  and at epoch =  28  the loss is =  0.018005626276135445  and accuracy is =  0.7928\n",
            "At step  20  and at epoch =  29  the loss is =  0.015345358289778233  and accuracy is =  0.8066\n",
            "At step  20  and at epoch =  30  the loss is =  0.01978936791419983  and accuracy is =  0.8186\n",
            "At step  20  and at epoch =  31  the loss is =  0.021210772916674614  and accuracy is =  0.8232\n",
            "At step  20  and at epoch =  32  the loss is =  0.012385374866425991  and accuracy is =  0.8308\n",
            "At step  20  and at epoch =  33  the loss is =  0.014651179313659668  and accuracy is =  0.8442\n",
            "At step  20  and at epoch =  34  the loss is =  0.01784423552453518  and accuracy is =  0.8514\n",
            "At step  20  and at epoch =  35  the loss is =  0.013974480330944061  and accuracy is =  0.8638\n",
            "At step  20  and at epoch =  36  the loss is =  0.013357196003198624  and accuracy is =  0.8692\n",
            "At step  20  and at epoch =  37  the loss is =  0.019116248935461044  and accuracy is =  0.8812\n",
            "At step  20  and at epoch =  38  the loss is =  0.014447178691625595  and accuracy is =  0.8834\n",
            "At step  20  and at epoch =  39  the loss is =  0.02081690914928913  and accuracy is =  0.8934\n",
            "At step  20  and at epoch =  40  the loss is =  0.013538066297769547  and accuracy is =  0.9034\n",
            "At step  20  and at epoch =  41  the loss is =  0.008758338168263435  and accuracy is =  0.9066\n",
            "At step  20  and at epoch =  42  the loss is =  0.012047814205288887  and accuracy is =  0.914\n",
            "At step  20  and at epoch =  43  the loss is =  0.010946881957352161  and accuracy is =  0.9244\n",
            "At step  20  and at epoch =  44  the loss is =  0.012918311171233654  and accuracy is =  0.926\n",
            "At step  20  and at epoch =  45  the loss is =  0.014036938548088074  and accuracy is =  0.9314\n",
            "At step  20  and at epoch =  46  the loss is =  0.012725917622447014  and accuracy is =  0.9372\n",
            "At step  20  and at epoch =  47  the loss is =  0.007077718153595924  and accuracy is =  0.9438\n",
            "At step  20  and at epoch =  48  the loss is =  0.012851532548666  and accuracy is =  0.951\n",
            "At step  20  and at epoch =  49  the loss is =  0.012881726957857609  and accuracy is =  0.9496\n",
            "At step  20  and at epoch =  50  the loss is =  0.009293374605476856  and accuracy is =  0.9576\n",
            "At step  20  and at epoch =  51  the loss is =  0.01643609069287777  and accuracy is =  0.9608\n",
            "At step  20  and at epoch =  52  the loss is =  0.009595521725714207  and accuracy is =  0.9602\n",
            "At step  20  and at epoch =  53  the loss is =  0.010516583919525146  and accuracy is =  0.9672\n",
            "At step  20  and at epoch =  54  the loss is =  0.004411243833601475  and accuracy is =  0.9716\n",
            "At step  20  and at epoch =  55  the loss is =  0.01605048216879368  and accuracy is =  0.9708\n",
            "At step  20  and at epoch =  56  the loss is =  0.005674143321812153  and accuracy is =  0.9756\n",
            "At step  20  and at epoch =  57  the loss is =  0.007288915570825338  and accuracy is =  0.9802\n",
            "At step  20  and at epoch =  58  the loss is =  0.014821144752204418  and accuracy is =  0.9802\n",
            "At step  20  and at epoch =  59  the loss is =  0.00787130557000637  and accuracy is =  0.9814\n",
            "At step  20  and at epoch =  60  the loss is =  0.006972134578973055  and accuracy is =  0.9846\n",
            "At step  20  and at epoch =  61  the loss is =  0.012256784364581108  and accuracy is =  0.9854\n",
            "At step  20  and at epoch =  62  the loss is =  0.01302451454102993  and accuracy is =  0.9862\n",
            "At step  20  and at epoch =  63  the loss is =  0.012758023105561733  and accuracy is =  0.9882\n",
            "At step  20  and at epoch =  64  the loss is =  0.01148535031825304  and accuracy is =  0.988\n",
            "At step  20  and at epoch =  65  the loss is =  0.014661342836916447  and accuracy is =  0.9926\n",
            "At step  20  and at epoch =  66  the loss is =  0.01506501529365778  and accuracy is =  0.991\n",
            "At step  20  and at epoch =  67  the loss is =  0.0014215987175703049  and accuracy is =  0.9904\n",
            "At step  20  and at epoch =  68  the loss is =  0.007149972952902317  and accuracy is =  0.9942\n",
            "At step  20  and at epoch =  69  the loss is =  0.010699911043047905  and accuracy is =  0.9952\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "56\n",
            "Validation Loss: 0.09608262032270432 Validation Accuracy : 0.22766666666666666\n",
            "At step  30  and at epoch =  0  the loss is =  0.09126926213502884  and accuracy is =  0.108\n",
            "At step  30  and at epoch =  1  the loss is =  0.051732663065195084  and accuracy is =  0.175\n",
            "At step  30  and at epoch =  2  the loss is =  0.03231468424201012  and accuracy is =  0.2236\n",
            "At step  30  and at epoch =  3  the loss is =  0.031825609505176544  and accuracy is =  0.3652\n",
            "At step  30  and at epoch =  4  the loss is =  0.03251684457063675  and accuracy is =  0.4758\n",
            "At step  30  and at epoch =  5  the loss is =  0.031243273988366127  and accuracy is =  0.5208\n",
            "At step  30  and at epoch =  6  the loss is =  0.0243039820343256  and accuracy is =  0.554\n",
            "At step  30  and at epoch =  7  the loss is =  0.016516389325261116  and accuracy is =  0.5708\n",
            "At step  30  and at epoch =  8  the loss is =  0.020109297707676888  and accuracy is =  0.5986\n",
            "At step  30  and at epoch =  9  the loss is =  0.02445041574537754  and accuracy is =  0.6222\n",
            "At step  30  and at epoch =  10  the loss is =  0.01824447326362133  and accuracy is =  0.6308\n",
            "At step  30  and at epoch =  11  the loss is =  0.02697143889963627  and accuracy is =  0.6586\n",
            "At step  30  and at epoch =  12  the loss is =  0.012860342860221863  and accuracy is =  0.6738\n",
            "At step  30  and at epoch =  13  the loss is =  0.017641080543398857  and accuracy is =  0.6902\n",
            "At step  30  and at epoch =  14  the loss is =  0.023025188595056534  and accuracy is =  0.702\n",
            "At step  30  and at epoch =  15  the loss is =  0.018991293385624886  and accuracy is =  0.7176\n",
            "At step  30  and at epoch =  16  the loss is =  0.02400948479771614  and accuracy is =  0.73\n",
            "At step  30  and at epoch =  17  the loss is =  0.014535615220665932  and accuracy is =  0.7406\n",
            "At step  30  and at epoch =  18  the loss is =  0.0257981289178133  and accuracy is =  0.7554\n",
            "At step  30  and at epoch =  19  the loss is =  0.021134914830327034  and accuracy is =  0.7688\n",
            "At step  30  and at epoch =  20  the loss is =  0.018500957638025284  and accuracy is =  0.78\n",
            "At step  30  and at epoch =  21  the loss is =  0.01629294827580452  and accuracy is =  0.7872\n",
            "At step  30  and at epoch =  22  the loss is =  0.011175790801644325  and accuracy is =  0.8066\n",
            "At step  30  and at epoch =  23  the loss is =  0.019581597298383713  and accuracy is =  0.821\n",
            "At step  30  and at epoch =  24  the loss is =  0.01806100271642208  and accuracy is =  0.828\n",
            "At step  30  and at epoch =  25  the loss is =  0.024849722161889076  and accuracy is =  0.8416\n",
            "At step  30  and at epoch =  26  the loss is =  0.02527686022222042  and accuracy is =  0.847\n",
            "At step  30  and at epoch =  27  the loss is =  0.011195885948836803  and accuracy is =  0.8606\n",
            "At step  30  and at epoch =  28  the loss is =  0.01597975753247738  and accuracy is =  0.8744\n",
            "At step  30  and at epoch =  29  the loss is =  0.01230538822710514  and accuracy is =  0.8898\n",
            "At step  30  and at epoch =  30  the loss is =  0.011012167669832706  and accuracy is =  0.8996\n",
            "At step  30  and at epoch =  31  the loss is =  0.011109933257102966  and accuracy is =  0.9056\n",
            "At step  30  and at epoch =  32  the loss is =  0.005607250612229109  and accuracy is =  0.911\n",
            "At step  30  and at epoch =  33  the loss is =  0.021235685795545578  and accuracy is =  0.9218\n",
            "At step  30  and at epoch =  34  the loss is =  0.011097622103989124  and accuracy is =  0.9324\n",
            "At step  30  and at epoch =  35  the loss is =  0.012610601261258125  and accuracy is =  0.9404\n",
            "At step  30  and at epoch =  36  the loss is =  0.024420833215117455  and accuracy is =  0.9436\n",
            "At step  30  and at epoch =  37  the loss is =  0.014340379275381565  and accuracy is =  0.9502\n",
            "At step  30  and at epoch =  38  the loss is =  0.01366518810391426  and accuracy is =  0.957\n",
            "At step  30  and at epoch =  39  the loss is =  0.014850088395178318  and accuracy is =  0.9618\n",
            "At step  30  and at epoch =  40  the loss is =  0.019430285319685936  and accuracy is =  0.9684\n",
            "At step  30  and at epoch =  41  the loss is =  0.02706809528172016  and accuracy is =  0.97\n",
            "At step  30  and at epoch =  42  the loss is =  0.007673610467463732  and accuracy is =  0.976\n",
            "At step  30  and at epoch =  43  the loss is =  0.013497979380190372  and accuracy is =  0.9802\n",
            "At step  30  and at epoch =  44  the loss is =  0.015824129804968834  and accuracy is =  0.9802\n",
            "At step  30  and at epoch =  45  the loss is =  0.005800067447125912  and accuracy is =  0.9856\n",
            "At step  30  and at epoch =  46  the loss is =  0.011349019594490528  and accuracy is =  0.9848\n",
            "At step  30  and at epoch =  47  the loss is =  0.0017944377614185214  and accuracy is =  0.9892\n",
            "At step  30  and at epoch =  48  the loss is =  0.010818524286150932  and accuracy is =  0.9902\n",
            "At step  30  and at epoch =  49  the loss is =  0.006819572299718857  and accuracy is =  0.991\n",
            "At step  30  and at epoch =  50  the loss is =  0.024409698322415352  and accuracy is =  0.9914\n",
            "At step  30  and at epoch =  51  the loss is =  0.00389250204898417  and accuracy is =  0.99\n",
            "At step  30  and at epoch =  52  the loss is =  0.01368278730660677  and accuracy is =  0.9944\n",
            "At step  30  and at epoch =  53  the loss is =  0.003637478919699788  and accuracy is =  0.9944\n",
            "At step  30  and at epoch =  54  the loss is =  0.01501614972949028  and accuracy is =  0.9946\n",
            "At step  30  and at epoch =  55  the loss is =  0.011349308304488659  and accuracy is =  0.9956\n",
            "At step  30  and at epoch =  56  the loss is =  0.00798073597252369  and accuracy is =  0.9946\n",
            "At step  30  and at epoch =  57  the loss is =  0.024540258571505547  and accuracy is =  0.995\n",
            "At step  30  and at epoch =  58  the loss is =  0.00536454888060689  and accuracy is =  0.9956\n",
            "At step  30  and at epoch =  59  the loss is =  0.01362098939716816  and accuracy is =  0.9972\n",
            "At step  30  and at epoch =  60  the loss is =  0.004062557592988014  and accuracy is =  0.9968\n",
            "At step  30  and at epoch =  61  the loss is =  0.023101503029465675  and accuracy is =  0.996\n",
            "At step  30  and at epoch =  62  the loss is =  0.009917072020471096  and accuracy is =  0.9902\n",
            "At step  30  and at epoch =  63  the loss is =  0.009652535431087017  and accuracy is =  0.9964\n",
            "At step  30  and at epoch =  64  the loss is =  0.016041016206145287  and accuracy is =  0.995\n",
            "At step  30  and at epoch =  65  the loss is =  0.01627269759774208  and accuracy is =  0.9988\n",
            "At step  30  and at epoch =  66  the loss is =  0.00666469894349575  and accuracy is =  0.9982\n",
            "At step  30  and at epoch =  67  the loss is =  0.005719976965337992  and accuracy is =  0.9976\n",
            "At step  30  and at epoch =  68  the loss is =  0.00933075975626707  and accuracy is =  0.999\n",
            "At step  30  and at epoch =  69  the loss is =  0.002356953453272581  and accuracy is =  0.9982\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "32\n",
            "Validation Loss: 0.08979783207178116 Validation Accuracy : 0.1685\n",
            "At step  40  and at epoch =  0  the loss is =  0.09589963406324387  and accuracy is =  0.1144\n",
            "At step  40  and at epoch =  1  the loss is =  0.056931447237730026  and accuracy is =  0.1754\n",
            "At step  40  and at epoch =  2  the loss is =  0.035299673676490784  and accuracy is =  0.211\n",
            "At step  40  and at epoch =  3  the loss is =  0.025409674271941185  and accuracy is =  0.2818\n",
            "At step  40  and at epoch =  4  the loss is =  0.02843294106423855  and accuracy is =  0.374\n",
            "At step  40  and at epoch =  5  the loss is =  0.02824695035815239  and accuracy is =  0.4088\n",
            "At step  40  and at epoch =  6  the loss is =  0.03195154666900635  and accuracy is =  0.4364\n",
            "At step  40  and at epoch =  7  the loss is =  0.028500927612185478  and accuracy is =  0.4562\n",
            "At step  40  and at epoch =  8  the loss is =  0.027579069137573242  and accuracy is =  0.4792\n",
            "At step  40  and at epoch =  9  the loss is =  0.025376081466674805  and accuracy is =  0.5068\n",
            "At step  40  and at epoch =  10  the loss is =  0.024092460051178932  and accuracy is =  0.523\n",
            "At step  40  and at epoch =  11  the loss is =  0.027923807501792908  and accuracy is =  0.5586\n",
            "At step  40  and at epoch =  12  the loss is =  0.027070824056863785  and accuracy is =  0.5692\n",
            "At step  40  and at epoch =  13  the loss is =  0.023852834478020668  and accuracy is =  0.5976\n",
            "At step  40  and at epoch =  14  the loss is =  0.03259042650461197  and accuracy is =  0.621\n",
            "At step  40  and at epoch =  15  the loss is =  0.02314040996134281  and accuracy is =  0.642\n",
            "At step  40  and at epoch =  16  the loss is =  0.03302842006087303  and accuracy is =  0.6624\n",
            "At step  40  and at epoch =  17  the loss is =  0.021233178675174713  and accuracy is =  0.6834\n",
            "At step  40  and at epoch =  18  the loss is =  0.017675170674920082  and accuracy is =  0.7062\n",
            "At step  40  and at epoch =  19  the loss is =  0.014438554644584656  and accuracy is =  0.734\n",
            "At step  40  and at epoch =  20  the loss is =  0.028483523055911064  and accuracy is =  0.7434\n",
            "At step  40  and at epoch =  21  the loss is =  0.021724151447415352  and accuracy is =  0.7764\n",
            "At step  40  and at epoch =  22  the loss is =  0.014092516154050827  and accuracy is =  0.7888\n",
            "At step  40  and at epoch =  23  the loss is =  0.014115831814706326  and accuracy is =  0.8088\n",
            "At step  40  and at epoch =  24  the loss is =  0.022426938638091087  and accuracy is =  0.8306\n",
            "At step  40  and at epoch =  25  the loss is =  0.018531939014792442  and accuracy is =  0.844\n",
            "At step  40  and at epoch =  26  the loss is =  0.021771498024463654  and accuracy is =  0.8632\n",
            "At step  40  and at epoch =  27  the loss is =  0.007682922296226025  and accuracy is =  0.8838\n",
            "At step  40  and at epoch =  28  the loss is =  0.013952129520475864  and accuracy is =  0.8956\n",
            "At step  40  and at epoch =  29  the loss is =  0.014108254574239254  and accuracy is =  0.9112\n",
            "At step  40  and at epoch =  30  the loss is =  0.03274833783507347  and accuracy is =  0.9236\n",
            "At step  40  and at epoch =  31  the loss is =  0.023348301649093628  and accuracy is =  0.9262\n",
            "At step  40  and at epoch =  32  the loss is =  0.01914067752659321  and accuracy is =  0.9424\n",
            "At step  40  and at epoch =  33  the loss is =  0.01898801699280739  and accuracy is =  0.9444\n",
            "At step  40  and at epoch =  34  the loss is =  0.008595868945121765  and accuracy is =  0.9558\n",
            "At step  40  and at epoch =  35  the loss is =  0.012705497443675995  and accuracy is =  0.9674\n",
            "At step  40  and at epoch =  36  the loss is =  0.0037023015320301056  and accuracy is =  0.9782\n",
            "At step  40  and at epoch =  37  the loss is =  0.02260216511785984  and accuracy is =  0.9838\n",
            "At step  40  and at epoch =  38  the loss is =  0.0045110685750842094  and accuracy is =  0.9838\n",
            "At step  40  and at epoch =  39  the loss is =  0.016853194683790207  and accuracy is =  0.9896\n",
            "At step  40  and at epoch =  40  the loss is =  0.016898054629564285  and accuracy is =  0.9812\n",
            "At step  40  and at epoch =  41  the loss is =  0.019450820982456207  and accuracy is =  0.99\n",
            "At step  40  and at epoch =  42  the loss is =  0.013981852680444717  and accuracy is =  0.9932\n",
            "At step  40  and at epoch =  43  the loss is =  0.030195914208889008  and accuracy is =  0.9898\n",
            "At step  40  and at epoch =  44  the loss is =  0.005790635943412781  and accuracy is =  0.9938\n",
            "At step  40  and at epoch =  45  the loss is =  0.014619476161897182  and accuracy is =  0.996\n",
            "At step  40  and at epoch =  46  the loss is =  0.018538134172558784  and accuracy is =  0.9944\n",
            "At step  40  and at epoch =  47  the loss is =  0.014796338975429535  and accuracy is =  0.9954\n",
            "At step  40  and at epoch =  48  the loss is =  0.00799913052469492  and accuracy is =  0.9968\n",
            "At step  40  and at epoch =  49  the loss is =  0.018195193260908127  and accuracy is =  0.9958\n",
            "At step  40  and at epoch =  50  the loss is =  0.013296682387590408  and accuracy is =  0.9958\n",
            "At step  40  and at epoch =  51  the loss is =  0.012104458175599575  and accuracy is =  0.9958\n",
            "At step  40  and at epoch =  52  the loss is =  0.007998624816536903  and accuracy is =  0.995\n",
            "At step  40  and at epoch =  53  the loss is =  0.0060137175023555756  and accuracy is =  0.9962\n",
            "At step  40  and at epoch =  54  the loss is =  0.0055800932459533215  and accuracy is =  0.9984\n",
            "At step  40  and at epoch =  55  the loss is =  0.0168504286557436  and accuracy is =  0.9974\n",
            "At step  40  and at epoch =  56  the loss is =  0.011684803292155266  and accuracy is =  0.9964\n",
            "At step  40  and at epoch =  57  the loss is =  0.012462860904633999  and accuracy is =  0.9986\n",
            "At step  40  and at epoch =  58  the loss is =  0.014447702094912529  and accuracy is =  0.9976\n",
            "At step  40  and at epoch =  59  the loss is =  0.019726349040865898  and accuracy is =  0.9976\n",
            "At step  40  and at epoch =  60  the loss is =  0.021506359800696373  and accuracy is =  0.9918\n",
            "At step  40  and at epoch =  61  the loss is =  0.003482033032923937  and accuracy is =  0.9952\n",
            "At step  40  and at epoch =  62  the loss is =  0.0070205628871917725  and accuracy is =  0.9994\n",
            "At step  40  and at epoch =  63  the loss is =  0.0029285999480634928  and accuracy is =  0.9994\n",
            "At step  40  and at epoch =  64  the loss is =  0.006009981501847506  and accuracy is =  0.9996\n",
            "At step  40  and at epoch =  65  the loss is =  0.01664227433502674  and accuracy is =  0.999\n",
            "At step  40  and at epoch =  66  the loss is =  0.04366923123598099  and accuracy is =  0.9968\n",
            "At step  40  and at epoch =  67  the loss is =  0.00928034633398056  and accuracy is =  0.9978\n",
            "At step  40  and at epoch =  68  the loss is =  0.00574468495324254  and accuracy is =  0.9976\n",
            "At step  40  and at epoch =  69  the loss is =  0.01516746636480093  and accuracy is =  0.998\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "8\n",
            "Validation Loss: 0.08718280494213104 Validation Accuracy : 0.1016\n",
            "At step  50  and at epoch =  0  the loss is =  0.08762211352586746  and accuracy is =  0.1226\n",
            "At step  50  and at epoch =  1  the loss is =  0.05879786238074303  and accuracy is =  0.1648\n",
            "At step  50  and at epoch =  2  the loss is =  0.034618884325027466  and accuracy is =  0.1914\n",
            "At step  50  and at epoch =  3  the loss is =  0.03316069021821022  and accuracy is =  0.2728\n",
            "At step  50  and at epoch =  4  the loss is =  0.024593617767095566  and accuracy is =  0.4246\n",
            "At step  50  and at epoch =  5  the loss is =  0.026638222858309746  and accuracy is =  0.4512\n",
            "At step  50  and at epoch =  6  the loss is =  0.026932643726468086  and accuracy is =  0.485\n",
            "At step  50  and at epoch =  7  the loss is =  0.02369322068989277  and accuracy is =  0.5098\n",
            "At step  50  and at epoch =  8  the loss is =  0.024173680692911148  and accuracy is =  0.536\n",
            "At step  50  and at epoch =  9  the loss is =  0.02940773032605648  and accuracy is =  0.5562\n",
            "At step  50  and at epoch =  10  the loss is =  0.020234158262610435  and accuracy is =  0.579\n",
            "At step  50  and at epoch =  11  the loss is =  0.021504443138837814  and accuracy is =  0.6044\n",
            "At step  50  and at epoch =  12  the loss is =  0.01878749579191208  and accuracy is =  0.634\n",
            "At step  50  and at epoch =  13  the loss is =  0.019992629066109657  and accuracy is =  0.6502\n",
            "At step  50  and at epoch =  14  the loss is =  0.01779809594154358  and accuracy is =  0.6798\n",
            "At step  50  and at epoch =  15  the loss is =  0.02249782532453537  and accuracy is =  0.7022\n",
            "At step  50  and at epoch =  16  the loss is =  0.01677115261554718  and accuracy is =  0.7226\n",
            "At step  50  and at epoch =  17  the loss is =  0.013803230598568916  and accuracy is =  0.749\n",
            "At step  50  and at epoch =  18  the loss is =  0.016936760395765305  and accuracy is =  0.7802\n",
            "At step  50  and at epoch =  19  the loss is =  0.014956659637391567  and accuracy is =  0.7868\n",
            "At step  50  and at epoch =  20  the loss is =  0.021996604278683662  and accuracy is =  0.8154\n",
            "At step  50  and at epoch =  21  the loss is =  0.018649885430932045  and accuracy is =  0.8428\n",
            "At step  50  and at epoch =  22  the loss is =  0.024127231910824776  and accuracy is =  0.8616\n",
            "At step  50  and at epoch =  23  the loss is =  0.014143502339720726  and accuracy is =  0.8734\n",
            "At step  50  and at epoch =  24  the loss is =  0.018368324264883995  and accuracy is =  0.8866\n",
            "At step  50  and at epoch =  25  the loss is =  0.01765911653637886  and accuracy is =  0.9138\n",
            "At step  50  and at epoch =  26  the loss is =  0.013580026105046272  and accuracy is =  0.9102\n",
            "At step  50  and at epoch =  27  the loss is =  0.03202080726623535  and accuracy is =  0.9212\n",
            "At step  50  and at epoch =  28  the loss is =  0.015462199226021767  and accuracy is =  0.9302\n",
            "At step  50  and at epoch =  29  the loss is =  0.013502733781933784  and accuracy is =  0.9464\n",
            "At step  50  and at epoch =  30  the loss is =  0.00839985627681017  and accuracy is =  0.9552\n",
            "At step  50  and at epoch =  31  the loss is =  0.0119320685043931  and accuracy is =  0.9714\n",
            "At step  50  and at epoch =  32  the loss is =  0.014180921949446201  and accuracy is =  0.9776\n",
            "At step  50  and at epoch =  33  the loss is =  0.010676336474716663  and accuracy is =  0.967\n",
            "At step  50  and at epoch =  34  the loss is =  0.026962824165821075  and accuracy is =  0.9756\n",
            "At step  50  and at epoch =  35  the loss is =  0.007303819991648197  and accuracy is =  0.9838\n",
            "At step  50  and at epoch =  36  the loss is =  0.014944912865757942  and accuracy is =  0.9884\n",
            "At step  50  and at epoch =  37  the loss is =  0.007172859739512205  and accuracy is =  0.9852\n",
            "At step  50  and at epoch =  38  the loss is =  0.013920921832323074  and accuracy is =  0.9922\n",
            "At step  50  and at epoch =  39  the loss is =  0.02509981580078602  and accuracy is =  0.9918\n",
            "At step  50  and at epoch =  40  the loss is =  0.01625971868634224  and accuracy is =  0.9902\n",
            "At step  50  and at epoch =  41  the loss is =  0.007439112290740013  and accuracy is =  0.9946\n",
            "At step  50  and at epoch =  42  the loss is =  0.01654365472495556  and accuracy is =  0.9944\n",
            "At step  50  and at epoch =  43  the loss is =  0.02644704282283783  and accuracy is =  0.989\n",
            "At step  50  and at epoch =  44  the loss is =  0.02954958565533161  and accuracy is =  0.992\n",
            "At step  50  and at epoch =  45  the loss is =  0.014270727522671223  and accuracy is =  0.9928\n",
            "At step  50  and at epoch =  46  the loss is =  0.019196273759007454  and accuracy is =  0.996\n",
            "At step  50  and at epoch =  47  the loss is =  0.008292297832667828  and accuracy is =  0.9864\n",
            "At step  50  and at epoch =  48  the loss is =  0.00787888653576374  and accuracy is =  0.9966\n",
            "At step  50  and at epoch =  49  the loss is =  0.005893266294151545  and accuracy is =  0.9964\n",
            "At step  50  and at epoch =  50  the loss is =  0.006213569547981024  and accuracy is =  0.9982\n",
            "At step  50  and at epoch =  51  the loss is =  0.030971257016062737  and accuracy is =  0.9976\n",
            "At step  50  and at epoch =  52  the loss is =  0.020700618624687195  and accuracy is =  0.9916\n",
            "At step  50  and at epoch =  53  the loss is =  0.01559588871896267  and accuracy is =  0.996\n",
            "At step  50  and at epoch =  54  the loss is =  0.012898607179522514  and accuracy is =  0.9942\n",
            "At step  50  and at epoch =  55  the loss is =  0.011667066253721714  and accuracy is =  0.9948\n",
            "At step  50  and at epoch =  56  the loss is =  0.006419571582227945  and accuracy is =  0.994\n",
            "At step  50  and at epoch =  57  the loss is =  0.005136149935424328  and accuracy is =  0.9974\n",
            "At step  50  and at epoch =  58  the loss is =  0.01109381765127182  and accuracy is =  0.9982\n",
            "At step  50  and at epoch =  59  the loss is =  0.007732081227004528  and accuracy is =  0.9974\n",
            "At step  50  and at epoch =  60  the loss is =  0.02655877359211445  and accuracy is =  0.997\n",
            "At step  50  and at epoch =  61  the loss is =  0.003798931837081909  and accuracy is =  0.9974\n",
            "At step  50  and at epoch =  62  the loss is =  0.0044344025664031506  and accuracy is =  0.9982\n",
            "At step  50  and at epoch =  63  the loss is =  0.009024345315992832  and accuracy is =  0.9984\n",
            "At step  50  and at epoch =  64  the loss is =  0.011101586744189262  and accuracy is =  0.9954\n",
            "At step  50  and at epoch =  65  the loss is =  0.01239757053554058  and accuracy is =  0.9932\n",
            "At step  50  and at epoch =  66  the loss is =  0.038209788501262665  and accuracy is =  0.994\n",
            "At step  50  and at epoch =  67  the loss is =  0.005112271290272474  and accuracy is =  0.9914\n",
            "At step  50  and at epoch =  68  the loss is =  0.005339235067367554  and accuracy is =  0.9988\n",
            "At step  50  and at epoch =  69  the loss is =  0.005647981073707342  and accuracy is =  0.9994\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "112\n",
            "Validation Loss: 0.11012464016675949 Validation Accuracy : 0.09383333333333334\n",
            "At step  60  and at epoch =  0  the loss is =  0.08873265981674194  and accuracy is =  0.1076\n",
            "At step  60  and at epoch =  1  the loss is =  0.05128765106201172  and accuracy is =  0.1936\n",
            "At step  60  and at epoch =  2  the loss is =  0.039566513150930405  and accuracy is =  0.2136\n",
            "At step  60  and at epoch =  3  the loss is =  0.029215315356850624  and accuracy is =  0.273\n",
            "At step  60  and at epoch =  4  the loss is =  0.03015182353556156  and accuracy is =  0.371\n",
            "At step  60  and at epoch =  5  the loss is =  0.02745109051465988  and accuracy is =  0.4336\n",
            "At step  60  and at epoch =  6  the loss is =  0.02458668127655983  and accuracy is =  0.4752\n",
            "At step  60  and at epoch =  7  the loss is =  0.01926024630665779  and accuracy is =  0.5056\n",
            "At step  60  and at epoch =  8  the loss is =  0.024369411170482635  and accuracy is =  0.5364\n",
            "At step  60  and at epoch =  9  the loss is =  0.020017094910144806  and accuracy is =  0.5626\n",
            "At step  60  and at epoch =  10  the loss is =  0.027866840362548828  and accuracy is =  0.577\n",
            "At step  60  and at epoch =  11  the loss is =  0.01606958918273449  and accuracy is =  0.6038\n",
            "At step  60  and at epoch =  12  the loss is =  0.019490156322717667  and accuracy is =  0.6342\n",
            "At step  60  and at epoch =  13  the loss is =  0.016306672245264053  and accuracy is =  0.6562\n",
            "At step  60  and at epoch =  14  the loss is =  0.033574119210243225  and accuracy is =  0.6868\n",
            "At step  60  and at epoch =  15  the loss is =  0.022561954334378242  and accuracy is =  0.706\n",
            "At step  60  and at epoch =  16  the loss is =  0.019085299223661423  and accuracy is =  0.7324\n",
            "At step  60  and at epoch =  17  the loss is =  0.022071046754717827  and accuracy is =  0.76\n",
            "At step  60  and at epoch =  18  the loss is =  0.01694192923605442  and accuracy is =  0.7752\n",
            "At step  60  and at epoch =  19  the loss is =  0.01753086782991886  and accuracy is =  0.802\n",
            "At step  60  and at epoch =  20  the loss is =  0.02177562192082405  and accuracy is =  0.824\n",
            "At step  60  and at epoch =  21  the loss is =  0.01745545119047165  and accuracy is =  0.8444\n",
            "At step  60  and at epoch =  22  the loss is =  0.014715515077114105  and accuracy is =  0.8618\n",
            "At step  60  and at epoch =  23  the loss is =  0.009604869410395622  and accuracy is =  0.885\n",
            "At step  60  and at epoch =  24  the loss is =  0.009677538648247719  and accuracy is =  0.9046\n",
            "At step  60  and at epoch =  25  the loss is =  0.017443299293518066  and accuracy is =  0.9242\n",
            "At step  60  and at epoch =  26  the loss is =  0.022222446277737617  and accuracy is =  0.9374\n",
            "At step  60  and at epoch =  27  the loss is =  0.012793753296136856  and accuracy is =  0.9462\n",
            "At step  60  and at epoch =  28  the loss is =  0.0042179846204817295  and accuracy is =  0.9606\n",
            "At step  60  and at epoch =  29  the loss is =  0.007494321092963219  and accuracy is =  0.9752\n",
            "At step  60  and at epoch =  30  the loss is =  0.021655194461345673  and accuracy is =  0.9792\n",
            "At step  60  and at epoch =  31  the loss is =  0.007233428303152323  and accuracy is =  0.9768\n",
            "At step  60  and at epoch =  32  the loss is =  0.012168253771960735  and accuracy is =  0.9844\n",
            "At step  60  and at epoch =  33  the loss is =  0.014900394715368748  and accuracy is =  0.9882\n",
            "At step  60  and at epoch =  34  the loss is =  0.010960591956973076  and accuracy is =  0.9902\n",
            "At step  60  and at epoch =  35  the loss is =  0.013407452031970024  and accuracy is =  0.9904\n",
            "At step  60  and at epoch =  36  the loss is =  0.01735874079167843  and accuracy is =  0.991\n",
            "At step  60  and at epoch =  37  the loss is =  0.0075006005354225636  and accuracy is =  0.9888\n",
            "At step  60  and at epoch =  38  the loss is =  0.0077827926725149155  and accuracy is =  0.9906\n",
            "At step  60  and at epoch =  39  the loss is =  0.01031866017729044  and accuracy is =  0.9968\n",
            "At step  60  and at epoch =  40  the loss is =  0.02547786943614483  and accuracy is =  0.9946\n",
            "At step  60  and at epoch =  41  the loss is =  0.019977018237113953  and accuracy is =  0.9926\n",
            "At step  60  and at epoch =  42  the loss is =  0.026834439486265182  and accuracy is =  0.9954\n",
            "At step  60  and at epoch =  43  the loss is =  0.007338886149227619  and accuracy is =  0.9972\n",
            "At step  60  and at epoch =  44  the loss is =  0.026219235733151436  and accuracy is =  0.9958\n",
            "At step  60  and at epoch =  45  the loss is =  0.011304323561489582  and accuracy is =  0.9882\n",
            "At step  60  and at epoch =  46  the loss is =  0.011094050481915474  and accuracy is =  0.9946\n",
            "At step  60  and at epoch =  47  the loss is =  0.01769089512526989  and accuracy is =  0.9972\n",
            "At step  60  and at epoch =  48  the loss is =  0.004240585025399923  and accuracy is =  0.9966\n",
            "At step  60  and at epoch =  49  the loss is =  0.005128249991685152  and accuracy is =  0.9998\n",
            "At step  60  and at epoch =  50  the loss is =  0.005145563278347254  and accuracy is =  0.9992\n",
            "At step  60  and at epoch =  51  the loss is =  0.014163129031658173  and accuracy is =  0.999\n",
            "At step  60  and at epoch =  52  the loss is =  0.013009673915803432  and accuracy is =  0.9986\n",
            "At step  60  and at epoch =  53  the loss is =  0.003825664985924959  and accuracy is =  0.9956\n",
            "At step  60  and at epoch =  54  the loss is =  0.003882241202518344  and accuracy is =  0.9988\n",
            "At step  60  and at epoch =  55  the loss is =  0.015253826975822449  and accuracy is =  0.9988\n",
            "At step  60  and at epoch =  56  the loss is =  0.021246641874313354  and accuracy is =  0.997\n",
            "At step  60  and at epoch =  57  the loss is =  0.0044422270730137825  and accuracy is =  0.9958\n",
            "At step  60  and at epoch =  58  the loss is =  0.015337073244154453  and accuracy is =  0.9984\n",
            "At step  60  and at epoch =  59  the loss is =  0.011198380030691624  and accuracy is =  0.9978\n",
            "At step  60  and at epoch =  60  the loss is =  0.012327948585152626  and accuracy is =  0.999\n",
            "At step  60  and at epoch =  61  the loss is =  0.013739615678787231  and accuracy is =  0.9976\n",
            "At step  60  and at epoch =  62  the loss is =  0.003882608376443386  and accuracy is =  0.997\n",
            "At step  60  and at epoch =  63  the loss is =  0.015798263251781464  and accuracy is =  0.999\n",
            "At step  60  and at epoch =  64  the loss is =  0.011034962721168995  and accuracy is =  0.997\n",
            "At step  60  and at epoch =  65  the loss is =  0.012008611112833023  and accuracy is =  0.9972\n",
            "At step  60  and at epoch =  66  the loss is =  0.011868796311318874  and accuracy is =  0.9956\n",
            "At step  60  and at epoch =  67  the loss is =  0.006448870524764061  and accuracy is =  0.9918\n",
            "At step  60  and at epoch =  68  the loss is =  0.0185796320438385  and accuracy is =  0.9982\n",
            "At step  60  and at epoch =  69  the loss is =  0.0011773870792239904  and accuracy is =  0.9956\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "88\n",
            "Validation Loss: 0.1121630147099495 Validation Accuracy : 0.082\n",
            "At step  70  and at epoch =  0  the loss is =  0.0916474312543869  and accuracy is =  0.105\n",
            "At step  70  and at epoch =  1  the loss is =  0.05033274367451668  and accuracy is =  0.203\n",
            "At step  70  and at epoch =  2  the loss is =  0.034127384424209595  and accuracy is =  0.2714\n",
            "At step  70  and at epoch =  3  the loss is =  0.03716840222477913  and accuracy is =  0.344\n",
            "At step  70  and at epoch =  4  the loss is =  0.027088019996881485  and accuracy is =  0.3946\n",
            "At step  70  and at epoch =  5  the loss is =  0.02634018287062645  and accuracy is =  0.436\n",
            "At step  70  and at epoch =  6  the loss is =  0.021623248234391212  and accuracy is =  0.4944\n",
            "At step  70  and at epoch =  7  the loss is =  0.024664772674441338  and accuracy is =  0.5174\n",
            "At step  70  and at epoch =  8  the loss is =  0.02082386426627636  and accuracy is =  0.5502\n",
            "At step  70  and at epoch =  9  the loss is =  0.024676525965332985  and accuracy is =  0.5974\n",
            "At step  70  and at epoch =  10  the loss is =  0.018838943913578987  and accuracy is =  0.6046\n",
            "At step  70  and at epoch =  11  the loss is =  0.02836635522544384  and accuracy is =  0.6394\n",
            "At step  70  and at epoch =  12  the loss is =  0.021453527733683586  and accuracy is =  0.6798\n",
            "At step  70  and at epoch =  13  the loss is =  0.016393311321735382  and accuracy is =  0.7092\n",
            "At step  70  and at epoch =  14  the loss is =  0.019039606675505638  and accuracy is =  0.738\n",
            "At step  70  and at epoch =  15  the loss is =  0.015284404158592224  and accuracy is =  0.7594\n",
            "At step  70  and at epoch =  16  the loss is =  0.021993044763803482  and accuracy is =  0.7966\n",
            "At step  70  and at epoch =  17  the loss is =  0.02696567215025425  and accuracy is =  0.8126\n",
            "At step  70  and at epoch =  18  the loss is =  0.014093314297497272  and accuracy is =  0.8402\n",
            "At step  70  and at epoch =  19  the loss is =  0.010687279514968395  and accuracy is =  0.8724\n",
            "At step  70  and at epoch =  20  the loss is =  0.013305895030498505  and accuracy is =  0.8822\n",
            "At step  70  and at epoch =  21  the loss is =  0.024664882570505142  and accuracy is =  0.903\n",
            "At step  70  and at epoch =  22  the loss is =  0.0191990677267313  and accuracy is =  0.91\n",
            "At step  70  and at epoch =  23  the loss is =  0.012678487226366997  and accuracy is =  0.9226\n",
            "At step  70  and at epoch =  24  the loss is =  0.008063550107181072  and accuracy is =  0.9386\n",
            "At step  70  and at epoch =  25  the loss is =  0.02478059008717537  and accuracy is =  0.9544\n",
            "At step  70  and at epoch =  26  the loss is =  0.011841309256851673  and accuracy is =  0.942\n",
            "At step  70  and at epoch =  27  the loss is =  0.006087216082960367  and accuracy is =  0.9554\n",
            "At step  70  and at epoch =  28  the loss is =  0.009586088359355927  and accuracy is =  0.9728\n",
            "At step  70  and at epoch =  29  the loss is =  0.012649421580135822  and accuracy is =  0.9804\n",
            "At step  70  and at epoch =  30  the loss is =  0.012040323577821255  and accuracy is =  0.9828\n",
            "At step  70  and at epoch =  31  the loss is =  0.014870253391563892  and accuracy is =  0.9832\n",
            "At step  70  and at epoch =  32  the loss is =  0.029778841882944107  and accuracy is =  0.9856\n",
            "At step  70  and at epoch =  33  the loss is =  0.005579391028732061  and accuracy is =  0.987\n",
            "At step  70  and at epoch =  34  the loss is =  0.014872963540256023  and accuracy is =  0.9928\n",
            "At step  70  and at epoch =  35  the loss is =  0.008068053983151913  and accuracy is =  0.9918\n",
            "At step  70  and at epoch =  36  the loss is =  0.006924348883330822  and accuracy is =  0.9948\n",
            "At step  70  and at epoch =  37  the loss is =  0.010266346856951714  and accuracy is =  0.9958\n",
            "At step  70  and at epoch =  38  the loss is =  0.01515219546854496  and accuracy is =  0.9968\n",
            "At step  70  and at epoch =  39  the loss is =  0.014950730837881565  and accuracy is =  0.9952\n",
            "At step  70  and at epoch =  40  the loss is =  0.003974125254899263  and accuracy is =  0.9966\n",
            "At step  70  and at epoch =  41  the loss is =  0.014509398490190506  and accuracy is =  0.9972\n",
            "At step  70  and at epoch =  42  the loss is =  0.015512208454310894  and accuracy is =  0.9956\n",
            "At step  70  and at epoch =  43  the loss is =  0.0095553332939744  and accuracy is =  0.9982\n",
            "At step  70  and at epoch =  44  the loss is =  0.0095613282173872  and accuracy is =  0.9976\n",
            "At step  70  and at epoch =  45  the loss is =  0.003860050579532981  and accuracy is =  0.9948\n",
            "At step  70  and at epoch =  46  the loss is =  0.007180601824074984  and accuracy is =  0.9988\n",
            "At step  70  and at epoch =  47  the loss is =  0.013623571023344994  and accuracy is =  0.9964\n",
            "At step  70  and at epoch =  48  the loss is =  0.005036267917603254  and accuracy is =  0.9864\n",
            "At step  70  and at epoch =  49  the loss is =  0.009663021191954613  and accuracy is =  0.998\n",
            "At step  70  and at epoch =  50  the loss is =  0.010146495886147022  and accuracy is =  0.9968\n",
            "At step  70  and at epoch =  51  the loss is =  0.002917737700045109  and accuracy is =  0.995\n",
            "At step  70  and at epoch =  52  the loss is =  0.02462436631321907  and accuracy is =  0.998\n",
            "At step  70  and at epoch =  53  the loss is =  0.01966649666428566  and accuracy is =  0.9902\n",
            "At step  70  and at epoch =  54  the loss is =  0.005568100139498711  and accuracy is =  0.9932\n",
            "At step  70  and at epoch =  55  the loss is =  0.017088383436203003  and accuracy is =  0.998\n",
            "At step  70  and at epoch =  56  the loss is =  0.018055563792586327  and accuracy is =  0.9928\n",
            "At step  70  and at epoch =  57  the loss is =  0.0036380435340106487  and accuracy is =  0.9954\n",
            "At step  70  and at epoch =  58  the loss is =  0.01333595346659422  and accuracy is =  0.9978\n",
            "At step  70  and at epoch =  59  the loss is =  0.007299633231014013  and accuracy is =  0.9974\n",
            "At step  70  and at epoch =  60  the loss is =  0.011539191007614136  and accuracy is =  0.9974\n",
            "At step  70  and at epoch =  61  the loss is =  0.012756873853504658  and accuracy is =  0.9918\n",
            "At step  70  and at epoch =  62  the loss is =  0.010558728128671646  and accuracy is =  0.9888\n",
            "At step  70  and at epoch =  63  the loss is =  0.019017048180103302  and accuracy is =  0.9974\n",
            "At step  70  and at epoch =  64  the loss is =  0.021580373868346214  and accuracy is =  0.997\n",
            "At step  70  and at epoch =  65  the loss is =  0.0029600670095533133  and accuracy is =  0.9824\n",
            "At step  70  and at epoch =  66  the loss is =  0.010376918129622936  and accuracy is =  0.9974\n",
            "At step  70  and at epoch =  67  the loss is =  0.012959455139935017  and accuracy is =  0.9934\n",
            "At step  70  and at epoch =  68  the loss is =  0.003113787155598402  and accuracy is =  0.9842\n",
            "At step  70  and at epoch =  69  the loss is =  0.023616105318069458  and accuracy is =  0.9958\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "64\n",
            "Validation Loss: 0.10819041728973389 Validation Accuracy : 0.07875\n",
            "At step  80  and at epoch =  0  the loss is =  0.07593443244695663  and accuracy is =  0.1248\n",
            "At step  80  and at epoch =  1  the loss is =  0.045420873910188675  and accuracy is =  0.1794\n",
            "At step  80  and at epoch =  2  the loss is =  0.038063857704401016  and accuracy is =  0.2322\n",
            "At step  80  and at epoch =  3  the loss is =  0.029701199382543564  and accuracy is =  0.2676\n",
            "At step  80  and at epoch =  4  the loss is =  0.0317671112716198  and accuracy is =  0.3172\n",
            "At step  80  and at epoch =  5  the loss is =  0.02859293855726719  and accuracy is =  0.3534\n",
            "At step  80  and at epoch =  6  the loss is =  0.025365309789776802  and accuracy is =  0.3918\n",
            "At step  80  and at epoch =  7  the loss is =  0.02277751825749874  and accuracy is =  0.4264\n",
            "At step  80  and at epoch =  8  the loss is =  0.031471531838178635  and accuracy is =  0.4486\n",
            "At step  80  and at epoch =  9  the loss is =  0.02003617212176323  and accuracy is =  0.4908\n",
            "At step  80  and at epoch =  10  the loss is =  0.018300795927643776  and accuracy is =  0.5166\n",
            "At step  80  and at epoch =  11  the loss is =  0.02841104008257389  and accuracy is =  0.5522\n",
            "At step  80  and at epoch =  12  the loss is =  0.015322783030569553  and accuracy is =  0.5866\n",
            "At step  80  and at epoch =  13  the loss is =  0.02118830569088459  and accuracy is =  0.6188\n",
            "At step  80  and at epoch =  14  the loss is =  0.020429357886314392  and accuracy is =  0.6516\n",
            "At step  80  and at epoch =  15  the loss is =  0.02919897995889187  and accuracy is =  0.693\n",
            "At step  80  and at epoch =  16  the loss is =  0.021643290296196938  and accuracy is =  0.7312\n",
            "At step  80  and at epoch =  17  the loss is =  0.02443687431514263  and accuracy is =  0.7452\n",
            "At step  80  and at epoch =  18  the loss is =  0.025879019871354103  and accuracy is =  0.7958\n",
            "At step  80  and at epoch =  19  the loss is =  0.02337525226175785  and accuracy is =  0.8254\n",
            "At step  80  and at epoch =  20  the loss is =  0.019458046182990074  and accuracy is =  0.8538\n",
            "At step  80  and at epoch =  21  the loss is =  0.01786702871322632  and accuracy is =  0.8864\n",
            "At step  80  and at epoch =  22  the loss is =  0.031636010855436325  and accuracy is =  0.894\n",
            "At step  80  and at epoch =  23  the loss is =  0.017713140696287155  and accuracy is =  0.8974\n",
            "At step  80  and at epoch =  24  the loss is =  0.017825616523623466  and accuracy is =  0.925\n",
            "At step  80  and at epoch =  25  the loss is =  0.019109364598989487  and accuracy is =  0.9434\n",
            "At step  80  and at epoch =  26  the loss is =  0.012186392210423946  and accuracy is =  0.952\n",
            "At step  80  and at epoch =  27  the loss is =  0.011621708981692791  and accuracy is =  0.9652\n",
            "At step  80  and at epoch =  28  the loss is =  0.01592308282852173  and accuracy is =  0.974\n",
            "At step  80  and at epoch =  29  the loss is =  0.025841951370239258  and accuracy is =  0.9752\n",
            "At step  80  and at epoch =  30  the loss is =  0.017277061939239502  and accuracy is =  0.9692\n",
            "At step  80  and at epoch =  31  the loss is =  0.008264175616204739  and accuracy is =  0.9854\n",
            "At step  80  and at epoch =  32  the loss is =  0.019970029592514038  and accuracy is =  0.9914\n",
            "At step  80  and at epoch =  33  the loss is =  0.017011405900120735  and accuracy is =  0.9878\n",
            "At step  80  and at epoch =  34  the loss is =  0.02661602385342121  and accuracy is =  0.9924\n",
            "At step  80  and at epoch =  35  the loss is =  0.009638885967433453  and accuracy is =  0.9892\n",
            "At step  80  and at epoch =  36  the loss is =  0.01255007740110159  and accuracy is =  0.9942\n",
            "At step  80  and at epoch =  37  the loss is =  0.006252926308661699  and accuracy is =  0.9958\n",
            "At step  80  and at epoch =  38  the loss is =  0.020172467455267906  and accuracy is =  0.9962\n",
            "At step  80  and at epoch =  39  the loss is =  0.009396828711032867  and accuracy is =  0.993\n",
            "At step  80  and at epoch =  40  the loss is =  0.018311623483896255  and accuracy is =  0.9926\n",
            "At step  80  and at epoch =  41  the loss is =  0.009904707781970501  and accuracy is =  0.9918\n",
            "At step  80  and at epoch =  42  the loss is =  0.020807413384318352  and accuracy is =  0.9878\n",
            "At step  80  and at epoch =  43  the loss is =  0.017015373334288597  and accuracy is =  0.9902\n",
            "At step  80  and at epoch =  44  the loss is =  0.023426072672009468  and accuracy is =  0.9948\n",
            "At step  80  and at epoch =  45  the loss is =  0.010615489445626736  and accuracy is =  0.9844\n",
            "At step  80  and at epoch =  46  the loss is =  0.0118757588788867  and accuracy is =  0.9934\n",
            "At step  80  and at epoch =  47  the loss is =  0.006895470432937145  and accuracy is =  0.9956\n",
            "At step  80  and at epoch =  48  the loss is =  0.006071486510336399  and accuracy is =  0.9982\n",
            "At step  80  and at epoch =  49  the loss is =  0.021187683567404747  and accuracy is =  0.9974\n",
            "At step  80  and at epoch =  50  the loss is =  0.010124651715159416  and accuracy is =  0.9784\n",
            "At step  80  and at epoch =  51  the loss is =  0.012102465145289898  and accuracy is =  0.9918\n",
            "At step  80  and at epoch =  52  the loss is =  0.01790541410446167  and accuracy is =  0.9846\n",
            "At step  80  and at epoch =  53  the loss is =  0.009392070583999157  and accuracy is =  0.9816\n",
            "At step  80  and at epoch =  54  the loss is =  0.01406471710652113  and accuracy is =  0.9838\n",
            "At step  80  and at epoch =  55  the loss is =  0.021752145141363144  and accuracy is =  0.9816\n",
            "At step  80  and at epoch =  56  the loss is =  0.0036914816591888666  and accuracy is =  0.9902\n",
            "At step  80  and at epoch =  57  the loss is =  0.006489051040261984  and accuracy is =  0.9966\n",
            "At step  80  and at epoch =  58  the loss is =  0.013048171997070312  and accuracy is =  0.9988\n",
            "At step  80  and at epoch =  59  the loss is =  0.00663282023742795  and accuracy is =  0.9922\n",
            "At step  80  and at epoch =  60  the loss is =  0.009560782462358475  and accuracy is =  0.9954\n",
            "At step  80  and at epoch =  61  the loss is =  0.009691360406577587  and accuracy is =  0.9934\n",
            "At step  80  and at epoch =  62  the loss is =  0.003968555014580488  and accuracy is =  0.9902\n",
            "At step  80  and at epoch =  63  the loss is =  0.01393859088420868  and accuracy is =  0.9976\n",
            "At step  80  and at epoch =  64  the loss is =  0.003074235748499632  and accuracy is =  0.9796\n",
            "At step  80  and at epoch =  65  the loss is =  0.00888114608824253  and accuracy is =  0.9982\n",
            "At step  80  and at epoch =  66  the loss is =  0.017228204756975174  and accuracy is =  0.996\n",
            "At step  80  and at epoch =  67  the loss is =  0.0025286993477493525  and accuracy is =  0.9946\n",
            "At step  80  and at epoch =  68  the loss is =  0.014836236834526062  and accuracy is =  0.9976\n",
            "At step  80  and at epoch =  69  the loss is =  0.010550754144787788  and accuracy is =  0.9942\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "40\n",
            "Validation Loss: 0.11191745102405548 Validation Accuracy : 0.05011111111111111\n",
            "At step  90  and at epoch =  0  the loss is =  0.07780889421701431  and accuracy is =  0.1074\n",
            "At step  90  and at epoch =  1  the loss is =  0.048036087304353714  and accuracy is =  0.1634\n",
            "At step  90  and at epoch =  2  the loss is =  0.03493782877922058  and accuracy is =  0.2306\n",
            "At step  90  and at epoch =  3  the loss is =  0.027668194845318794  and accuracy is =  0.2958\n",
            "At step  90  and at epoch =  4  the loss is =  0.02933092974126339  and accuracy is =  0.323\n",
            "At step  90  and at epoch =  5  the loss is =  0.027320126071572304  and accuracy is =  0.3528\n",
            "At step  90  and at epoch =  6  the loss is =  0.02525598742067814  and accuracy is =  0.4026\n",
            "At step  90  and at epoch =  7  the loss is =  0.03023284673690796  and accuracy is =  0.428\n",
            "At step  90  and at epoch =  8  the loss is =  0.030476903542876244  and accuracy is =  0.4564\n",
            "At step  90  and at epoch =  9  the loss is =  0.029868122190237045  and accuracy is =  0.4852\n",
            "At step  90  and at epoch =  10  the loss is =  0.02277698926627636  and accuracy is =  0.516\n",
            "At step  90  and at epoch =  11  the loss is =  0.024570340290665627  and accuracy is =  0.5492\n",
            "At step  90  and at epoch =  12  the loss is =  0.023796167224645615  and accuracy is =  0.5812\n",
            "At step  90  and at epoch =  13  the loss is =  0.02155471220612526  and accuracy is =  0.6224\n",
            "At step  90  and at epoch =  14  the loss is =  0.024977803230285645  and accuracy is =  0.6514\n",
            "At step  90  and at epoch =  15  the loss is =  0.023956215009093285  and accuracy is =  0.704\n",
            "At step  90  and at epoch =  16  the loss is =  0.014691808260977268  and accuracy is =  0.7366\n",
            "At step  90  and at epoch =  17  the loss is =  0.0224568173289299  and accuracy is =  0.7684\n",
            "At step  90  and at epoch =  18  the loss is =  0.02240888588130474  and accuracy is =  0.7928\n",
            "At step  90  and at epoch =  19  the loss is =  0.012498864904046059  and accuracy is =  0.819\n",
            "At step  90  and at epoch =  20  the loss is =  0.024546299129724503  and accuracy is =  0.8486\n",
            "At step  90  and at epoch =  21  the loss is =  0.010598021559417248  and accuracy is =  0.8534\n",
            "At step  90  and at epoch =  22  the loss is =  0.033931951969861984  and accuracy is =  0.8876\n",
            "At step  90  and at epoch =  23  the loss is =  0.011756794527173042  and accuracy is =  0.8962\n",
            "At step  90  and at epoch =  24  the loss is =  0.04273313283920288  and accuracy is =  0.9092\n",
            "At step  90  and at epoch =  25  the loss is =  0.021156789734959602  and accuracy is =  0.9374\n",
            "At step  90  and at epoch =  26  the loss is =  0.01902947388589382  and accuracy is =  0.9406\n",
            "At step  90  and at epoch =  27  the loss is =  0.031404364854097366  and accuracy is =  0.9432\n",
            "At step  90  and at epoch =  28  the loss is =  0.022486010566353798  and accuracy is =  0.9528\n",
            "At step  90  and at epoch =  29  the loss is =  0.010490131564438343  and accuracy is =  0.9576\n",
            "At step  90  and at epoch =  30  the loss is =  0.017406057566404343  and accuracy is =  0.9634\n",
            "At step  90  and at epoch =  31  the loss is =  0.017661623656749725  and accuracy is =  0.963\n",
            "At step  90  and at epoch =  32  the loss is =  0.028398549184203148  and accuracy is =  0.9768\n",
            "At step  90  and at epoch =  33  the loss is =  0.005345209501683712  and accuracy is =  0.9796\n",
            "At step  90  and at epoch =  34  the loss is =  0.011386306025087833  and accuracy is =  0.9896\n",
            "At step  90  and at epoch =  35  the loss is =  0.008899345993995667  and accuracy is =  0.9914\n",
            "At step  90  and at epoch =  36  the loss is =  0.009671670384705067  and accuracy is =  0.9876\n",
            "At step  90  and at epoch =  37  the loss is =  0.01126028411090374  and accuracy is =  0.9886\n",
            "At step  90  and at epoch =  38  the loss is =  0.007625099271535873  and accuracy is =  0.9898\n",
            "At step  90  and at epoch =  39  the loss is =  0.021539418026804924  and accuracy is =  0.989\n",
            "At step  90  and at epoch =  40  the loss is =  0.013966383412480354  and accuracy is =  0.9858\n",
            "At step  90  and at epoch =  41  the loss is =  0.020486196503043175  and accuracy is =  0.9858\n",
            "At step  90  and at epoch =  42  the loss is =  0.0186862051486969  and accuracy is =  0.9662\n",
            "At step  90  and at epoch =  43  the loss is =  0.008821598254144192  and accuracy is =  0.9792\n",
            "At step  90  and at epoch =  44  the loss is =  0.019572313874959946  and accuracy is =  0.9916\n",
            "At step  90  and at epoch =  45  the loss is =  0.011984016746282578  and accuracy is =  0.9862\n",
            "At step  90  and at epoch =  46  the loss is =  0.013202279806137085  and accuracy is =  0.9896\n",
            "At step  90  and at epoch =  47  the loss is =  0.014826378785073757  and accuracy is =  0.9768\n",
            "At step  90  and at epoch =  48  the loss is =  0.0033223505597561598  and accuracy is =  0.9824\n",
            "At step  90  and at epoch =  49  the loss is =  0.016393637284636497  and accuracy is =  0.9954\n",
            "At step  90  and at epoch =  50  the loss is =  0.003842822276055813  and accuracy is =  0.994\n",
            "At step  90  and at epoch =  51  the loss is =  0.029111379757523537  and accuracy is =  0.9924\n",
            "At step  90  and at epoch =  52  the loss is =  0.022530464455485344  and accuracy is =  0.9766\n",
            "At step  90  and at epoch =  53  the loss is =  0.020387738943099976  and accuracy is =  0.9852\n",
            "At step  90  and at epoch =  54  the loss is =  0.00751847866922617  and accuracy is =  0.9824\n",
            "At step  90  and at epoch =  55  the loss is =  0.01798059418797493  and accuracy is =  0.9916\n",
            "At step  90  and at epoch =  56  the loss is =  0.011169174686074257  and accuracy is =  0.9884\n",
            "At step  90  and at epoch =  57  the loss is =  0.008172071538865566  and accuracy is =  0.9914\n",
            "At step  90  and at epoch =  58  the loss is =  0.010547523386776447  and accuracy is =  0.9898\n",
            "At step  90  and at epoch =  59  the loss is =  0.03828771412372589  and accuracy is =  0.989\n",
            "At step  90  and at epoch =  60  the loss is =  0.021014157682657242  and accuracy is =  0.9864\n",
            "At step  90  and at epoch =  61  the loss is =  0.0028169432189315557  and accuracy is =  0.961\n",
            "At step  90  and at epoch =  62  the loss is =  0.013019545935094357  and accuracy is =  0.9954\n",
            "At step  90  and at epoch =  63  the loss is =  0.010957135818898678  and accuracy is =  0.993\n",
            "At step  90  and at epoch =  64  the loss is =  0.019879978150129318  and accuracy is =  0.9956\n",
            "At step  90  and at epoch =  65  the loss is =  0.009099440649151802  and accuracy is =  0.9836\n",
            "At step  90  and at epoch =  66  the loss is =  0.028288692235946655  and accuracy is =  0.9886\n",
            "At step  90  and at epoch =  67  the loss is =  0.007190475706011057  and accuracy is =  0.993\n",
            "At step  90  and at epoch =  68  the loss is =  0.045767731964588165  and accuracy is =  0.994\n",
            "At step  90  and at epoch =  69  the loss is =  0.01803767867386341  and accuracy is =  0.9934\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "16\n",
            "Validation Loss: 0.11343757063150406 Validation Accuracy : 0.0574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pomxiRbnUNq7",
        "colab_type": "code",
        "outputId": "e755df60-99bd-45cc-f390-9102d6f56582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plotTask(pars_tasks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}